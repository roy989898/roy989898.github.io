<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stepping on Terminal</title>
    <link>https://roy989898.github.io/tags/stepping/</link>
    <description>Recent content in stepping on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Apr 2021 11:57:33 +0800</lastBuildDate><atom:link href="https://roy989898.github.io/tags/stepping/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Tutorial 4.7 An End-to-End SGD Example</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.7/</link>
      <pubDate>Wed, 28 Apr 2021 11:57:33 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.7/</guid>
      <description>An End-to-End SGD Example </description>
      <content>&lt;h1 id=&#34;_an-end-to-end-sgd-example_&#34;&gt;&lt;em&gt;An End-to-End SGD Example&lt;/em&gt;&lt;/h1&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.6 Stepping With a Learning Rate</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.6/</link>
      <pubDate>Wed, 28 Apr 2021 11:30:02 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.6/</guid>
      <description>Stepping With a Learning Rate when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder).</description>
      <content>&lt;h1 id=&#34;_stepping-with-a-learning-rate_&#34;&gt;&lt;em&gt;Stepping With a Learning Rate&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder). Once you&amp;rsquo;ve picked a learning rate, you can adjust your parameters using this simple function:
w -= gradient(w) * lr&lt;br&gt;
This is known as &lt;em&gt;stepping&lt;/em&gt; your parameters, using an &lt;em&gt;optimizer step&lt;/em&gt;.
當我們得到梯度時，我們就用它來計算新的參數。 幾乎所有方法都始於將梯度乘以一個稱為學習率（LR）的小數的基本思想。 學習率通常是0.001到0.1之間的數字，儘管可以是任意數。通常，人們僅通過嘗試一些就可以選擇學習率，並在訓練後發現哪種模式可以得到最佳模型（我們將在稍後向您展示一種更好的方法 在這本書中，稱為學習率查找器）。 選擇學習速度後，您可以使用以下簡單功能調整參數：
w -= gradient(w) * lr&lt;br&gt;
使用“優化步”，這稱為“步進”你的參數。&lt;/p&gt;
&lt;p&gt;if your Lr too small,maybe too slow,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/step_small.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;if LR too big,it can actually result in the loss getting worse,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/strp_big1.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/steo_big2.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
