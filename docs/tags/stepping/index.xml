<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>stepping on Terminal</title>
    <link>https://roy989898.github.io/tags/stepping/</link>
    <description>Recent content in stepping on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Apr 2021 11:57:33 +0800</lastBuildDate><atom:link href="https://roy989898.github.io/tags/stepping/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Tutorial 4.7 An End-to-End SGD Example</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.7/</link>
      <pubDate>Wed, 28 Apr 2021 11:57:33 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.7/</guid>
      <description>An End-to-End SGD Example we want to find the smallest value
Some useful function craete a 0-19 torch array
time = torch.arange(0,20).float(); time # tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) create randome number
# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。 num=20 t=torch.randn(num) time_f = torch.arange(0,num).float(); time plt.scatter(time_f,t); t simulate a car speed
# simulate a car speed # torch.randn(20)*3 is some random noise time = torch.</description>
      <content>&lt;h1 id=&#34;_an-end-to-end-sgd-example_&#34;&gt;&lt;em&gt;An End-to-End SGD Example&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;we want to find the smallest value&lt;/p&gt;
&lt;h2 id=&#34;some-useful-function&#34;&gt;Some useful function&lt;/h2&gt;
&lt;p&gt;craete a 0-19 torch array&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create randome number&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。&lt;/span&gt;
num&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;
t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(num)
time_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,num)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time_f,t);
t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rt.PNG&#34; alt=&#34;rt&#34;&gt;&lt;/p&gt;
&lt;p&gt;simulate a car speed&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# simulate a car speed&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.randn(20)*3 is some random noise&lt;/span&gt;
time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
speed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.75&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(time&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9.5&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time,speed);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/car_speed.PNG&#34; alt=&#34;car_speed&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;use-sgd-to-find-the-smallest-value-for-the-loss&#34;&gt;use SGD to find the smallest value for the loss&lt;/h2&gt;
&lt;h3 id=&#34;step-0-gues-the-functions&#34;&gt;Step 0 gues the functions&lt;/h3&gt;
&lt;p&gt;we nedd to find the a,b,c that make the loss is the lowset
(time**2)+(b*time)+c&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(t, params):
    a,b,c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; params
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(t&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (b&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;t) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-01-define-the-meaning-of-best&#34;&gt;Step 0.1 define the meaning of best&lt;/h3&gt;
&lt;p&gt;we use a loss function to define the best, which will return a value based on a prediction and a target, where lower values of the function correspond to &amp;ldquo;better&amp;rdquo; predictions. For continuous data, it&amp;rsquo;s common to use mean squared error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mse&lt;/span&gt;(preds, targets): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ((preds&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;targets)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.6 Stepping With a Learning Rate</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.6/</link>
      <pubDate>Wed, 28 Apr 2021 11:30:02 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.6/</guid>
      <description>Stepping With a Learning Rate when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder).</description>
      <content>&lt;h1 id=&#34;_stepping-with-a-learning-rate_&#34;&gt;&lt;em&gt;Stepping With a Learning Rate&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder). Once you&amp;rsquo;ve picked a learning rate, you can adjust your parameters using this simple function:
w -= gradient(w) * lr&lt;br&gt;
This is known as &lt;em&gt;stepping&lt;/em&gt; your parameters, using an &lt;em&gt;optimizer step&lt;/em&gt;.
當我們得到梯度時，我們就用它來計算新的參數。 幾乎所有方法都始於將梯度乘以一個稱為學習率（LR）的小數的基本思想。 學習率通常是0.001到0.1之間的數字，儘管可以是任意數。通常，人們僅通過嘗試一些就可以選擇學習率，並在訓練後發現哪種模式可以得到最佳模型（我們將在稍後向您展示一種更好的方法 在這本書中，稱為學習率查找器）。 選擇學習速度後，您可以使用以下簡單功能調整參數：
w -= gradient(w) * lr&lt;br&gt;
使用“優化步”，這稱為“步進”你的參數。&lt;/p&gt;
&lt;p&gt;if your Lr too small,maybe too slow,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/step_small.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;if LR too big,it can actually result in the loss getting worse,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/strp_big1.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/steo_big2.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
