<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>ai on Terminal</title>
    <link>https://roy989898.github.io/tags/ai/</link>
    <description>Recent content in ai on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 May 2021 15:03:29 +0800</lastBuildDate><atom:link href="https://roy989898.github.io/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Tutorial 6.1 Other Computer Vision Problems-Multi-Label Classification</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-6.1/</link>
      <pubDate>Sun, 16 May 2021 15:03:29 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-6.1/</guid>
      <description>My Code 
Source Code 
Multi-Label Classification a picture,can &amp;gt; 1 tag,or 0 tag
pandas dataframe tutorial the image that have more than one tag
from fastai.vision.all import * path = untar_data(URLs.PASCAL_2007) # use the path read the csv to the dataframe df = pd.read_csv(path/&amp;#39;train.csv&amp;#39;) df.head() # fname labels is_valid # 0 000005.jpg chair True # 1 000007.jpg car True # 2 000009.jpg horse person True # 3 000012.jpg car False # 4 000016.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1VzYTbBKx-JPfJ1FaLHOhG1Hpf3GNdG5C?usp=sharing&#34;&gt;My Code
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/06_multicat.ipynb&#34;&gt;Source Code
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;multi-label-classification&#34;&gt;Multi-Label Classification&lt;/h1&gt;
&lt;p&gt;a picture,can &amp;gt; 1 tag,or 0 tag&lt;/p&gt;
&lt;h2 id=&#34;pandas-dataframe-tutorial&#34;&gt;pandas dataframe tutorial&lt;/h2&gt;
&lt;p&gt;the image that have more than one tag&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.all &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; untar_data(URLs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;PASCAL_2007)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# use the path read the csv to the dataframe&lt;/span&gt;
df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;read_csv(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train.csv&amp;#39;&lt;/span&gt;)
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;head()

&lt;span style=&#34;color:#75715e&#34;&gt;# fname labels is_valid&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 000005.jpg chair True&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 000007.jpg car True&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 000009.jpg horse person True&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 000012.jpg car False&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 000016.jpg bicycle True&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;dataframe get a column&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;fname&amp;#39;&lt;/span&gt;]

&lt;span style=&#34;color:#75715e&#34;&gt;# 0       000005.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1       000007.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2       000009.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3       000012.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4       000016.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#            ...    &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5006    009954.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5007    009955.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5008    009958.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5009    009959.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5010    009961.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Name: fname, Length: 5011, dtype: object&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;dataframe get a row buy index&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iloc[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]

&lt;span style=&#34;color:#75715e&#34;&gt;# fname       000005.jpg&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# labels           chair&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# is_valid          True&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Name: 0, dtype: object&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;get all row,first column&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iloc[:,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;get first row,all coumn&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;iloc[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,:]

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create dataframe&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;df1&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame()
df1[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
df1

&lt;span style=&#34;color:#75715e&#34;&gt;# a&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 2&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 3&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 4&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Dataframe  operator&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;df1[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;]
df1[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;a&amp;#39;&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;df1[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;b&amp;#39;&lt;/span&gt;]

&lt;span style=&#34;color:#75715e&#34;&gt;# 0    11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1    22&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2    33&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3    44&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# dtype: int64&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 5.4 Image Classification &gt;2 types Improving Our Model</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-5.4/</link>
      <pubDate>Fri, 07 May 2021 14:42:55 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-5.4/</guid>
      <description>My Code 
Source Code 
Improving Our Model we will explain a little bit more about transfer learning and how to fine-tune our pretrained model as best as possible, without breaking the pretrained weights.
The Learning Rate Finder if lr too small, many epochs to train our model,waste time,and every time we do a complete pass through the data, we give our model a chance to memorize it.also remember the validate data</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Rqum2194iz5nXH26PPoBMpKM71wQ4eYI?usp=sharing&#34;&gt;My Code
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb#scrollTo=YOTrrdP7BuWd&#34;&gt;Source Code
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;improving-our-model&#34;&gt;Improving Our Model&lt;/h1&gt;
&lt;p&gt;we will explain a little bit more about transfer learning and how to fine-tune our pretrained model as best as possible, without breaking the pretrained weights.&lt;/p&gt;
&lt;h2 id=&#34;the-learning-rate-finder&#34;&gt;The Learning Rate Finder&lt;/h2&gt;
&lt;p&gt;if lr too small, many epochs to train our model,waste time,and every time we do a complete pass through the data, we give our model a chance to memorize it.also remember the validate data&lt;/p&gt;
&lt;p&gt;set it very high frist,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, base_lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 2.568456 6.223738 0.496617 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 3.971391 2.541565 0.698917 01:12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the way to find the best LR:&lt;br&gt;
simple concept: use a very LR start,train a one mini-batch,&amp;gt; increase the LR by some percentage (e.g., doubling it each time),than repeat,until the loss gets worse, instead of better,This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)&lt;/li&gt;
&lt;li&gt;The last point where the loss was clearly decreasing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;fastai will help you to find this 2 point Both these rules usually give around the same value&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# default start LR is 1e-3=10^-3&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
lr_min,lr_steep &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr_find()

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Minimum/10: 1.00e-02, steepest point: 2.51e-03&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;1e-3 mean 10^-3&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/lrf.PNG&#34; alt=&#34;sgd_LRFstep&#34;&gt;
for the picture,we can seeif LR &amp;gt; 1e-1,the loss increase,but 1-e-1 too high,becasu already leave the loss decrease phase&lt;br&gt;
we use 3e-3 at here(follow the book),we still can use 8.32e-03 and 2.09e-03&lt;/p&gt;
&lt;h2 id=&#34;unfreezing-and-transfer-learning&#34;&gt;Unfreezing and Transfer Learning&lt;/h2&gt;
&lt;p&gt;what is transfer learning??? We saw that the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task.&lt;/p&gt;
&lt;p&gt;Our challenge when fine-tuning is to replace the random weights in our added linear layers with weights that correctly achieve our desired task (classifying pet breeds) without breaking the carefully pretrained weights and the other layers. There is actually a very simple trick to allow this to happen: tell the optimizer to only update the weights in those randomly added final layers. Don&amp;rsquo;t change the weights in the rest of the neural network at all. This is called freezing those pretrained layers.&lt;/p&gt;
&lt;p&gt;進行微調時，我們面臨的挑戰是在不破壞經過精心訓練的砝碼和其他層的情況下，用能夠正確完成我們期望任務（對寵物品種進行分類）的砝碼替換添加的線性層中的隨機砝碼。 實際上，有一個很簡單的技巧可以使這種情況發生：告訴優化器僅更新那些隨機添加的最終層中的權重。 完全不要更改神經網絡其餘部分的權重。 這稱為凍結那些預訓練的層。&lt;/p&gt;
&lt;p&gt;When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:&lt;/p&gt;
&lt;p&gt;Trains the randomly added layers for one epoch, with all other layers frozen.&lt;br&gt;
Unfreezes all of the layers, and trains them all for the number of epochs requested&lt;/p&gt;
&lt;p&gt;try implement&lt;/p&gt;
&lt;p&gt;First of all we will train the randomly added layers for three epochs, using fit_one_cycle
fit_one_cycle is the suggested way to train models without using fine_tune. We&amp;rsquo;ll see why later in the book; in short, what fit_one_cycle does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# here only train the randomly added layers&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3e-3&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 1.149184 0.357759 0.112314 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.516031 0.269226 0.082544 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.307812 0.237481 0.071719 01:07&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# unfreeze the model&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfreeze()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;run lr_find again to find the LR, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn&amp;rsquo;t appropriate any more:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr_find()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/lr2.PNG&#34; alt=&#34;lr2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;important&lt;/strong&gt;!!!!!!we should not use the lr_steep at here,because our model has been trained already. Here we have a somewhat flat area before a sharp increase, and we should take a point well before that sharp increase—for instance, 1e-5. The point with the maximum gradient isn&amp;rsquo;t what we look for here and should be ignored.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#  train all layer&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, lr_max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.245116 0.232571 0.071042 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.244692 0.223327 0.069689 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.214002 0.217773 0.068336 01:13&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.194007 0.214042 0.066306 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.180974 0.212813 0.067659 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.183777 0.215303 0.064953 01:12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The deepest layers of model might not need as high a learning rate as the last ones, so we should probably use different learning rates for those—this is known as using discriminative learning rates.&lt;/p&gt;
&lt;h2 id=&#34;discriminative-learning-rates&#34;&gt;Discriminative Learning Rates&lt;/h2&gt;
&lt;p&gt;each level can use different LR,at low level,we can use the lower LR,because they already trained,they have pretrained weights,useful for nearly any task,no need to change so much,at higher level,the pretrained weights is for   much more complex concepts, like &amp;ldquo;eye&amp;rdquo; and &amp;ldquo;sunset,&amp;rdquo; which might not be useful in your task at all,use a faster lr to train them&lt;br&gt;
main point:&lt;br&gt;
use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers)&lt;/p&gt;
&lt;h3 id=&#34;basic-for-the-slice&#34;&gt;Basic for the slice&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;arr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;list(range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
myslice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
arr[myslice]  
&lt;span style=&#34;color:#75715e&#34;&gt;# [0, 1, 2, 3, 4]&lt;/span&gt;
myslice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
arr[myslice]  
&lt;span style=&#34;color:#75715e&#34;&gt;# [1, 2, 3, 4]&lt;/span&gt;
myslice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
arr[myslice]  
&lt;span style=&#34;color:#75715e&#34;&gt;# [0, 2, 4]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# lr_max=slice(1e-6,1e-4)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# mean lowest LR is 1e-6,the other layers will scale up to 1e-4&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3e-3&lt;/span&gt;)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfreeze()
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;, lr_max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-4&lt;/span&gt;))

&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 1.131566 0.361410 0.111637 01:06&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.544027 0.264487 0.086604 01:06&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.316729 0.248465 0.083221 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.256258 0.242825 0.085250 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.242427 0.238632 0.080514 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.233899 0.233360 0.083221 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.217217 0.217414 0.075778 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.189038 0.217263 0.070365 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.181181 0.207588 0.069012 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 6 0.158933 0.208005 0.070365 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 7 0.148363 0.205170 0.068336 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 8 0.135392 0.203676 0.069012 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 9 0.122220 0.203666 0.065629 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 10 0.130100 0.200204 0.065629 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 11 0.119578 0.205134 0.069689 01:11&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now the fine-tuning is working great!&lt;/p&gt;
&lt;p&gt;we can see the loss chnage&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# plot the loss change&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recorder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_loss()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/p_loss.PNG&#34; alt=&#34;pLoss&#34;&gt;&lt;/p&gt;
&lt;p&gt;the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does not mean that it is getting less accurate, necessarily. Take a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen &lt;strong&gt;metrics&lt;/strong&gt;, &lt;strong&gt;not the loss&lt;/strong&gt;. The loss is just the function we&amp;rsquo;ve given the computer to help us to optimize.&lt;/p&gt;
&lt;h2 id=&#34;number-of-epochs&#34;&gt;Number of Epochs&lt;/h2&gt;
&lt;p&gt;choose the number of epoch that you willing to wait,than watch the above picture, if you see that the metric are still getting better even in your final epochs, then you know that you have not trained for too long.&lt;/p&gt;
&lt;h2 id=&#34;deeper-architectures&#34;&gt;Deeper Architectures&lt;/h2&gt;
&lt;p&gt;a model with more parameters(depper) can model your data more accurately.&lt;/p&gt;
&lt;p&gt;This is why, in practice, architectures tend to come in a small number of variants. For instance, the ResNet architecture that we are using in this chapter comes in variants with 18, 34, 50, 101, and 152 layer, pretrained on ImageNet. A larger (more layers and parameters; sometimes described as the &amp;ldquo;capacity&amp;rdquo; of a model) version of a ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, because it has more parameters to overfit with.&lt;/p&gt;
&lt;p&gt;the other problem is,depper, will use more GPU RAM,an duse more time&lt;/p&gt;
&lt;p&gt;nearly all current NVIDIA GPUs support a special feature called&lt;strong&gt;tensor cores&lt;/strong&gt; that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module).&lt;/p&gt;
&lt;p&gt;You can&amp;rsquo;t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let&amp;rsquo;s try a ResNet-50 now with mixed precision:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.callback.fp16 &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet50, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_fp16()
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, freeze_epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 1.279959 0.309704 0.102842 01:05&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.590101 0.312733 0.101489 01:05&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.447781 0.294772 0.088633 01:05&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.274948 0.280899 0.085250 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.299947 0.331522 0.089310 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.251186 0.292205 0.084574 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.159606 0.241466 0.068336 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.083857 0.210775 0.060893 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.054267 0.210627 0.060893 01:06&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;try small model first,than try big model&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 5.3 Image Classification &gt;2 types Cross-entropy loss 2</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-5.3/</link>
      <pubDate>Fri, 07 May 2021 11:46:30 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-5.3/</guid>
      <description>My Code 
Source Code 
Cross-entropy loss 2 although softmax+ log Likelihood look like very suitable as a loss function.But the problem is we are using probabilities, 1&amp;gt;=p&amp;gt;=0.That mean when the model see 0.99 and 0.999, they are very close,but in another sense, 0.999 is 10 times more confident than 0.99. So, we want to transform our numbers between 0 and 1 to instead be between negative infinity and 0.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Rqum2194iz5nXH26PPoBMpKM71wQ4eYI?usp=sharing&#34;&gt;My Code
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb#scrollTo=YOTrrdP7BuWd&#34;&gt;Source Code
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;cross-entropy-loss-2&#34;&gt;Cross-entropy loss 2&lt;/h1&gt;
&lt;p&gt;although softmax+ log Likelihood look like very suitable as a loss function.But the problem is we are using probabilities, 1&amp;gt;=p&amp;gt;=0.That mean when the model see 0.99 and 0.999, they are very close,but in another sense, 0.999 is 10 times more confident than 0.99. So, we want to transform our numbers between 0 and 1 to instead be between negative infinity and 0.Log!!!!!&lt;/p&gt;
&lt;h2 id=&#34;taking-the-log&#34;&gt;Taking the Log&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plot_function(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log, min&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/log.PNG&#34; alt=&#34;log&#34;&gt;&lt;/p&gt;
&lt;p&gt;log in python&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; b&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;a
a &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; log(y,b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;log(a&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;b) &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; log(a)&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;log(b)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;at default, the pYtorch use e=2.718 as the log basic&lt;/p&gt;
&lt;p&gt;in the Pytorch,nll_loss awsume you get the log of the softmax,so it do not the log.
softmax+log+nll_loss==log_softmax+nll_loss==nn.CrossEntropyLoss()&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# log_softmax -&amp;gt;nll_loss,cross-entropy loss!!!!!&lt;/span&gt;
loss_func &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CrossEntropyLoss()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss_func(acts, targ)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(1.7790)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;same&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# same&lt;/span&gt;
F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cross_entropy(acts, targ)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(1.7790)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# at default,will take all the loss mean&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# reduction=&amp;#39;none&amp;#39; disable&lt;/span&gt;
nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;CrossEntropyLoss(reduction&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;none&amp;#39;&lt;/span&gt;)(acts, targ)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we do some testing to prove &lt;code&gt;softmax+log+nll_loss==log_softmax+nll_loss==nn.CrossEntropyLoss()&lt;/code&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;sm_acts2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;softmax(acts, dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
sm_acts2
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[0.7795, 0.2205],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.8902, 0.1098],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.1517, 0.8483],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.5245, 0.4755],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.9956, 0.0044],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.8464, 0.1536]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log(sm_acts2)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[-2.4908e-01, -1.5119e+00],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.1630e-01, -2.2091e+00],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.8857e+00, -1.6455e-01],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-6.4534e-01, -7.4335e-01],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-4.4367e-03, -5.4201e+00],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.6675e-01, -1.8735e+00]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# test ,equal the above code,softmax + log&lt;/span&gt;
sfm&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log_softmax(acts, dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
sfm
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[-2.4908e-01, -1.5119e+00],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.1630e-01, -2.2091e+00],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.8857e+00, -1.6455e-01],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-6.4534e-01, -7.4335e-01],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-4.4366e-03, -5.4201e+00],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.6675e-01, -1.8735e+00]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nll_loss(sfm, targ, reduction&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;none&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([0.2491, 2.2091, 1.8857, 0.7434, 5.4201, 0.1667])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;model-interpretation&#34;&gt;Model Interpretation&lt;/h1&gt;
&lt;p&gt;use a confusion matrix to see where our model is doing well, and where it&amp;rsquo;s doing badly:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#width 600&lt;/span&gt;
interp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ClassificationInterpretation&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_learner(learn)
interp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_confusion_matrix(figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;), dpi&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;60&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pet_matrix.PNG&#34; alt=&#34;pet_matrix&#34;&gt;&lt;/p&gt;
&lt;p&gt;too diccfcult to read&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# only show the most bad &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# min_val 5 mean the wrong at least is 5&lt;/span&gt;
interp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;most_confused(min_val&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# [(&amp;#39;Ragdoll&amp;#39;, &amp;#39;Birman&amp;#39;, 9),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  (&amp;#39;american_pit_bull_terrier&amp;#39;, &amp;#39;staffordshire_bull_terrier&amp;#39;, 8),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  (&amp;#39;Bengal&amp;#39;, &amp;#39;Egyptian_Mau&amp;#39;, 6)]&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# actual Egyptian_Mau ,but predict Bengal is9&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# search at google, we can foind the they really need to classifer by a humanexpert, so it is ok&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;new we have a good model,how can we make it better?&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 5.2 Image Classification &gt;2 types Cross-entropy loss 1</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-5.2/</link>
      <pubDate>Thu, 06 May 2021 16:55:28 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-5.2/</guid>
      <description>My Code 
Source Code 
Cross-entropy loss fastai will choose the loss based on what kind of data and model you are using. In this case we have image data and a categorical outcome, so fastai will default to using cross-entropy loss.
Cross-entropy loss can use for more than 2 category
Viewing Activations and Labels x,y = dls.one_batch() x.shape # torch.Size([64, 3, 224, 224]) our batch isze is 64,so we can see the list is 64 item.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Rqum2194iz5nXH26PPoBMpKM71wQ4eYI?usp=sharing&#34;&gt;My Code
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb#scrollTo=YOTrrdP7BuWd&#34;&gt;Source Code
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;cross-entropy-loss&#34;&gt;Cross-entropy loss&lt;/h1&gt;
&lt;p&gt;fastai will choose the loss based on what kind of data and model you are using. In this case we have image data and a categorical outcome, so fastai will default to using cross-entropy loss.&lt;/p&gt;
&lt;p&gt;Cross-entropy loss can use for more than 2 category&lt;/p&gt;
&lt;h2 id=&#34;viewing-activations-and-labels&#34;&gt;Viewing Activations and Labels&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;x,y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;one_batch()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([64, 3, 224, 224])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;our batch isze is 64,so we can see the list is 64 item.0-36,37 type&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;y
&lt;span style=&#34;color:#75715e&#34;&gt;# TensorCategory([ 7,  1,  0, 14, 19,  9,  2, 35, 12,  0, 26, 34, 18, 21,  5,  8,  0, 35,  8,  8, 28, 35, 17, 34, 21,  3, 17, 19, 18, 22,  9, 12, 34, 10, 35, 25, 13, 18, 32, 36, 20, 26,  5, 18, 31,  6,  7,  9,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          3,  1,  0, 30,  2,  4, 12, 24, 30,  1, 30, 20, 30, 21,  3, 12], device=&amp;#39;cuda:0&amp;#39;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;see the predict&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;preds,target &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_preds(dl&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[(x,y)])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;target
&lt;span style=&#34;color:#75715e&#34;&gt;# TensorCategory([ 7,  1,  0, 14, 19,  9,  2, 35, 12,  0, 26, 34, 18, 21,  5,  8,  0, 35,  8,  8, 28, 35, 17, 34, 21,  3, 17, 19, 18, 22,  9, 12, 34, 10, 35, 25, 13, 18, 32, 36, 20, 26,  5, 18, 31,  6,  7,  9,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          3,  1,  0, 30,  2,  4, 12, 24, 30,  1, 30, 20, 30, 21,  3, 12])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# preds containe 64 pred, becasue beatch size is 64,probilitiesof 37 type ,because it contain 37 type&lt;/span&gt;
preds&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([64, 37])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# between 0-1,&lt;/span&gt;
preds[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([2.7509e-08, 4.1222e-08, 3.7762e-06, 4.6692e-07, 6.6490e-06, 1.6953e-08, 2.9940e-05, 9.9975e-01, 1.9381e-04, 2.9978e-09, 1.0564e-08, 1.0974e-07, 3.9340e-07, 1.0617e-08, 7.8258e-09, 4.8307e-08,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         2.9032e-07, 8.0013e-09, 2.2539e-08, 5.3139e-07, 1.7915e-08, 1.0556e-07, 3.6633e-06, 5.3050e-06, 1.2096e-07, 6.5162e-08, 4.3347e-09, 9.6756e-08, 5.2215e-06, 2.0169e-07, 1.5412e-07, 8.8911e-07,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         2.2806e-07, 1.2523e-07, 6.1131e-09, 6.0672e-08, 3.3345e-07])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# add them all is 1&lt;/span&gt;
len(preds[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]),preds[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()
&lt;span style=&#34;color:#75715e&#34;&gt;# (37, tensor(1.))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;softmax&#34;&gt;Softmax&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# if we have 6 picture,and 2 type&lt;/span&gt;
acts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn((&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
acts
&lt;span style=&#34;color:#75715e&#34;&gt;# first column is confident of the 3 ,second is the column of the 7&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[-0.9916, -2.2545],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [ 0.1560, -1.9368],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-0.6164,  1.1047],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-2.0798, -2.1778],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [ 1.6429, -3.7728],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.2445, -2.9512]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;acts&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
&lt;span style=&#34;color:#75715e&#34;&gt;# we can not direct use sigmoid,because c1+c2!=1, we hope the probaility of 7 and 3 sum is 1&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can calculate the relative of the 7 and 3&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;acts[:,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# get the first column&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([-0.9916,  0.1560, -0.6164, -2.0798,  1.6429, -1.2445])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# this is first column&lt;/span&gt;
f_c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(acts[:,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;acts[:,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
f_c
&lt;span style=&#34;color:#75715e&#34;&gt;# second column is 1- first column softmax do this thing&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;s_c&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;f_c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;softmax do this thing&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;softmax&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; exp(x) &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt; exp(x)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum(dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, keepdim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;span style=&#34;color:#75715e&#34;&gt;# exp is e**8 ,e is 2.718&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;sm_acts &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;softmax(acts, dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
sm_acts
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[0.7795, 0.2205],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.8902, 0.1098],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.1517, 0.8483],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.5245, 0.4755],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.9956, 0.0044],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.8464, 0.1536]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;softmax is the multi-category equivalent of sigmoid—we have to use it any time we have more than two categories and the probabilities of the categories must add to 1, and we often use it even when there are just two categories, just to make things a bit more consistent.&lt;br&gt;
Taking the exponential ensures all our numbers are positive, and then dividing by the sum ensures we are going to have a bunch of numbers that add up to 1. The exponential also has a nice property: if one of the numbers in our activations x is slightly bigger than the others, the exponential will amplify this (since it grows, well&amp;hellip; exponentially), which means that in the softmax, that number will be closer to 1.&lt;/p&gt;
&lt;p&gt;Intuitively, the softmax function really wants to pick one class among the others, so it&amp;rsquo;s ideal for training a classifier when we know each picture has a definite label. (Note that it may be less ideal during inference, as you might want your model to sometimes tell you it doesn&amp;rsquo;t recognize any of the classes that it has seen during training, and not pick a class because it has a slightly bigger activation score. In this case, it might be better to train a model using multiple binary output columns, each using a sigmoid activation.)&lt;/p&gt;
&lt;p&gt;Softmax is the first part of the cross-entropy loss—the second part is log likelihood.&lt;/p&gt;
&lt;p&gt;取指數可確保我們所有的數字都是正數，然後除以和可確保我們將擁有一堆加起來為1的數字。指數也具有很好的屬性：如果x中的數字之一比其他稍大一些,放大（因為它會以指數形式增長）（這是指數增長），這意味著在softmax中，該數字將接近於1。&lt;/p&gt;
&lt;p&gt;直觀上，softmax函數確實希望從其他類別中選擇一個類別，因此當我們知道每張圖片都有一個確定的標籤時，訓練分類器是理想的選擇。 （請注意，在推理過程中它可能不太理想，因為您可能希望模型有時告訴您，它無法識別訓練中看到的任何課程，並且不選一個課程，因為它的激活分數稍高在這種情況下，最好使用多個二進制輸出列訓練模型，每個輸出列都使用S型激活。）&lt;/p&gt;
&lt;h2 id=&#34;log-likelihood&#34;&gt;Log Likelihood&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# old&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mnist_loss&lt;/span&gt;(inputs, targets):
    inputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inputs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;where(targets&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;inputs, inputs)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# tag&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 is7, 1 is3?????&lt;/span&gt;
targ &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# these are the softmax activations:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# left is 3,rightis 7 probility&lt;/span&gt;
sm_acts

&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[0.7795, 0.2205],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.8902, 0.1098],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.1517, 0.8483],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.5245, 0.4755],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.9956, 0.0044],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0.8464, 0.1536]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# get the taged probility&lt;/span&gt;
idx &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)
sm_acts[idx, targ]
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([0.7795, 0.1098, 0.1517, 0.4755, 0.0044, 0.8464])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# is 3? &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 is3, 1 is 7&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;#hide_input&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; IPython.display &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; HTML
df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(sm_acts, columns&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;3&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;7&amp;#34;&lt;/span&gt;])
df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;targ&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; targ
df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;idx&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; idx
df[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;loss&amp;#39;&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; sm_acts[range(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;), targ]
t &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;style&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;hide_index()
&lt;span style=&#34;color:#75715e&#34;&gt;#To have html code compatible with our script&lt;/span&gt;
html &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; t&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_repr_html_()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;split(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;/style&amp;gt;&amp;#39;&lt;/span&gt;)[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
html &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sub(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;table id=&amp;#34;([^&amp;#34;]+)&amp;#34;\s*&amp;gt;&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;lt;table &amp;gt;&amp;#39;&lt;/span&gt;, html)
display(HTML(html))


&lt;span style=&#34;color:#75715e&#34;&gt;# 3 7 targ idx loss&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.779514 0.220486 0 0 0.779514&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.890204 0.109796 1 1 0.109796&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.151727 0.848273 0 2 0.151727&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.524483 0.475517 1 3 0.475517&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.995573 0.004427 1 4 0.004427&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.846414 0.153586 0 5 0.846414&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the above is log likehold&lt;/p&gt;
&lt;p&gt;Pytorch have a function that do the samething with the sm_acts[],but it recive negative number nll_loss&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# do the same thing of sm_acts[range(n), targ],except it takes the negative, because when applying the log afterward, we will have negative numbers&lt;/span&gt;
F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;nll_loss(sm_acts, targ, reduction&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;none&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([-0.7795, -0.1098, -0.1517, -0.4755, -0.0044, -0.8464])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can see thta the Log Likelihood get the number is bigger when the distance is close,but we want when the distance is close,the loss number is close, we handle this problem later&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 5.1 Image Classification &gt;2 types 1</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-5.1/</link>
      <pubDate>Thu, 06 May 2021 16:15:02 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-5.1/</guid>
      <description>My Code 
Source Code 
Image Classification 1 #hide !pip install -Uqq fastbook import fastbook fastbook.setup_book() #hide from fastbook import * ``````py from fastai.vision.all import * path = untar_data(URLs.PETS) #hide Path.BASE_PATH = path See the image path.ls() (path/&amp;#34;images&amp;#34;).ls() Get the file name fname = (path/&amp;#34;images&amp;#34;).ls()[1] # Path(&amp;#39;images/havanese_158.jpg&amp;#39;) fname.name # havanese_158.jpg use regex to get the file name
re.findall(r&amp;#39;(.+)_\d+.jpg$&amp;#39;, fname.name) # [&amp;#39;havanese&amp;#39;] Prepare the data pets = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&amp;#39;(.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Rqum2194iz5nXH26PPoBMpKM71wQ4eYI?usp=sharing&#34;&gt;My Code
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb#scrollTo=YOTrrdP7BuWd&#34;&gt;Source Code
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;image-classification-1&#34;&gt;Image Classification 1&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#hide&lt;/span&gt;
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;Uqq fastbook
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fastbook
fastbook&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup_book()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#hide&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastbook &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

&lt;span style=&#34;color:#e6db74&#34;&gt;``````&lt;/span&gt;py
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.all &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; untar_data(URLs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;PETS)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#hide&lt;/span&gt;
Path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;BASE_PATH &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; path
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;see-the-image&#34;&gt;See the image&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;images&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;get-the-file-name&#34;&gt;Get the file name&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;fname &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;images&amp;#34;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# Path(&amp;#39;images/havanese_158.jpg&amp;#39;)&lt;/span&gt;
fname&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name
&lt;span style=&#34;color:#75715e&#34;&gt;# havanese_158.jpg&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;use regex to get the file name&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;re&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;findall(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(.+)_\d+.jpg$&amp;#39;&lt;/span&gt;, fname&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name)
&lt;span style=&#34;color:#75715e&#34;&gt;# [&amp;#39;havanese&amp;#39;]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;prepare-the-data&#34;&gt;Prepare the data&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;pets &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataBlock(blocks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (ImageBlock, CategoryBlock),
                 get_items&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_image_files, 
                 splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomSplitter(seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;),
                 get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;using_attr(RegexLabeller(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(.+)_\d+.jpg$&amp;#39;&lt;/span&gt;), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;),
                 item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;460&lt;/span&gt;),
                 batch_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;aug_transforms(size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, min_scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.75&lt;/span&gt;))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;images&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# this is presizing&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  item_tfms=Resize(460),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#                  batch_tfms=aug_transforms(size=224, min_scale=0.75))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we apply the aug_transforms at gpu to a batch of image,but before that,we need to make the image to the smae size,yhan pass the sendor to the GPU,
so we apply Resize(460) to each item at CPU.&lt;br&gt;
first step:&lt;br&gt;
Crop full width or height: This is in item_tfms, so it&amp;rsquo;s applied to each individual image before it is copied to the GPU. It&amp;rsquo;s used to ensure all images are the same size. On the training set, the crop area is chosen randomly. On the validation set, the center square of the image is always chosen.
we crop them to a bigger(460) square.Why bigger? because we wnat to have  have spare margin to allow further augmentation transforms on their inner regions without creating empty zones.if performed after resizing down to the augmented size, various common data augmentation transforms might introduce spurious empty zones, degrade data, or both .e.g. rotating an image by 45 degrees fills corner regions of the new bounds with emptiness, which will not teach the model anything.This transformation works by resizing to a square, using a large crop size. On the training set, the crop area is chosen randomly, and the size of the crop is selected to cover the entire width or height of the image, whichever is smaller.&lt;br&gt;
second step:&lt;br&gt;
Random crop and augment: This is in batch_tfms,
at second step,GPU is used for all data augmentation,with a single interpolation(make the picture more clear) at the end.
On the training set, the random crop and any other augmentations are done first.&lt;br&gt;
我們將gpu上的aug_transforms應用於一批圖像，但是在此之前，我們需要將圖像調整為smae大小，然後將發送方傳遞到GPU，
因此我們將Resize（460）應用於CPU的每個項目。
第一步：
裁剪全寬或全高：這位於item_tfms中，因此在將其複製到GPU之前將其應用於每個單獨的圖像。用於確保所有圖像的尺寸相同。在訓練集上，隨機選擇作物面積。在驗證集上，始終選擇圖像的中心正方形。
我們將它們裁剪到更大的（460）平方。為什麼更大？因為我們擁有足夠的餘量以允許在其內部區域上進行進一步的增強變換而不會創建空白區域，如果在縮小大小到增大大小之後執行此操作，則各種常見的數據增強變換可能會引入虛假的空白區域，降級數據或同時出現這兩種情況。將圖像旋轉45度會以空的方式填充新邊界的角區域，這將不會對模型產生任何影響。此轉換通過使用大作物大小將尺寸調整為正方形來進行。在訓練集上，隨機選擇作物區域，並選擇作物的大小以覆蓋圖像的整個寬度或高度，以較小者為準。
第二步：
隨機裁剪和擴充：這在batch_tfms中，
第二步，將GPU用於所有數據擴充，最後進行一次插值（使畫面更清晰）。
在訓練集上，首先進行隨機裁剪和任何其他擴充。
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/att_00060.png&#34; alt=&#34;tr&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;checking-and-debugging-a-datablock&#34;&gt;Checking and Debugging a DataBlock&lt;/h2&gt;
&lt;p&gt;Checking&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# show some image&lt;/span&gt;
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, ncols&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/check.PNG&#34; alt=&#34;check&#34;&gt;&lt;/p&gt;
&lt;p&gt;we should check all the data,that the tag is correct,we can check by our eyes,or by the &lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-2/#start-create-the-model&#34; title=&#34;model&#34;&gt;model&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Debugging&lt;/p&gt;
&lt;p&gt;error because the image size is different&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#hide_output&lt;/span&gt;
pets1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataBlock(blocks &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (ImageBlock, CategoryBlock),
                 get_items&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_image_files, 
                 splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomSplitter(seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;),
                 get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;using_attr(RegexLabeller(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(.+)_\d+.jpg$&amp;#39;&lt;/span&gt;), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;))
pets1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;images&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;Setting&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;up type transforms pipelines
Collecting items &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;home&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;jhoward&lt;span style=&#34;color:#f92672&#34;&gt;/.&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;oxford&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;iiit&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;pet&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;images
Found &lt;span style=&#34;color:#ae81ff&#34;&gt;7390&lt;/span&gt; items
&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; datasets of sizes &lt;span style=&#34;color:#ae81ff&#34;&gt;5912&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1478&lt;/span&gt;
Setting up Pipeline: PILBase&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create
Setting up Pipeline: partial &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; Categorize

Building one sample
  Pipeline: PILBase&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create
    starting &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;home&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;jhoward&lt;span style=&#34;color:#f92672&#34;&gt;/.&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;oxford&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;iiit&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;pet&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;images&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;american_pit_bull_terrier_31&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;jpg
    applying PILBase&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create gives
      PILImage mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RGB size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;x414
  Pipeline: partial &lt;span style=&#34;color:#f92672&#34;&gt;-&amp;gt;&lt;/span&gt; Categorize
    starting &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;
      &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;home&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;jhoward&lt;span style=&#34;color:#f92672&#34;&gt;/.&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;oxford&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;iiit&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;pet&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;images&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;american_pit_bull_terrier_31&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;jpg
    applying partial gives
      american_pit_bull_terrier
    applying Categorize gives
      TensorCategory(&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;)

Final sample: (PILImage mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RGB size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;x414, TensorCategory(&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;))


Setting up after_item: Pipeline: ToTensor
Setting up before_batch: Pipeline: 
Setting up after_batch: Pipeline: IntToFloatTensor

Building one batch
Applying item_tfms to the first sample:
  Pipeline: ToTensor
    starting &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt;
      (PILImage mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RGB size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;x414, TensorCategory(&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;))
    applying ToTensor gives
      (TensorImage of size &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;x414x500, TensorCategory(&lt;span style=&#34;color:#ae81ff&#34;&gt;13&lt;/span&gt;))

Adding the next &lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; samples

No before_batch transform to apply

Collating items &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; a batch
Error&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt; It&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;s not possible to collate your items in a batch&lt;/span&gt;
Could &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; collate the &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;th members of your tuples because got the following shapes
torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Size([&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;414&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;]),torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Size([&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;375&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;]),torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Size([&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;500&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;281&lt;/span&gt;]),torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Size([&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;203&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;])
&lt;span style=&#34;color:#f92672&#34;&gt;---------------------------------------------------------------------------&lt;/span&gt;
&lt;span style=&#34;color:#a6e22e&#34;&gt;RuntimeError&lt;/span&gt;                              Traceback (most recent call last)
&lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;ipython&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;input&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;11&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;c0a3d421ca2&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;module&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;
      &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;                  splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomSplitter(seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;),
      &lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;                  get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;using_attr(RegexLabeller(&lt;span style=&#34;color:#e6db74&#34;&gt;r&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;(.+)_\d+.jpg$&amp;#39;&lt;/span&gt;), &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;name&amp;#39;&lt;/span&gt;))
&lt;span style=&#34;color:#f92672&#34;&gt;----&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt; pets1&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;summary(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;images&amp;#34;&lt;/span&gt;)

&lt;span style=&#34;color:#f92672&#34;&gt;~/&lt;/span&gt;git&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;block&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;py &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; summary(self, source, bs, show_batch, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;182&lt;/span&gt;         why &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; _find_fail_collate(s)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;183&lt;/span&gt;         &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Make sure all parts of your samples are tensors of the same size&amp;#34;&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; why &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; why)
&lt;span style=&#34;color:#f92672&#34;&gt;--&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;184&lt;/span&gt;         &lt;span style=&#34;color:#66d9ef&#34;&gt;raise&lt;/span&gt; e
    &lt;span style=&#34;color:#ae81ff&#34;&gt;185&lt;/span&gt; 
    &lt;span style=&#34;color:#ae81ff&#34;&gt;186&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; len([f &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; f &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;after_batch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fs &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; f&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;noop&amp;#39;&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;:

&lt;span style=&#34;color:#f92672&#34;&gt;~/&lt;/span&gt;git&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;block&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;py &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; summary(self, source, bs, show_batch, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;176&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;\n&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;Collating items in a batch&amp;#34;&lt;/span&gt;)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;177&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
&lt;span style=&#34;color:#f92672&#34;&gt;--&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;178&lt;/span&gt;         b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create_batch(s)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;179&lt;/span&gt;         b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; retain_types(b, s[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; is_listy(s) &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; s)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;180&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;Exception&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;as&lt;/span&gt; e:

&lt;span style=&#34;color:#f92672&#34;&gt;~/&lt;/span&gt;git&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;load&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;py &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; create_batch(self, b)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;125&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;retain&lt;/span&gt;(self, res, b):  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; retain_types(res, b[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; is_listy(b) &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; b)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;126&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_item&lt;/span&gt;(self, s):  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; next(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;it) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; s &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataset[s]
&lt;span style=&#34;color:#f92672&#34;&gt;--&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;127&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;create_batch&lt;/span&gt;(self, b): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (fa_collate,fa_convert)[self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;prebatched](b)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;do_batch&lt;/span&gt;(self, b): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;retain(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create_batch(self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;before_batch(b)), b)
    &lt;span style=&#34;color:#ae81ff&#34;&gt;129&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;to&lt;/span&gt;(self, device): self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;device &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; device

&lt;span style=&#34;color:#f92672&#34;&gt;~/&lt;/span&gt;git&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;load&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;py &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; fa_collate(t)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;44&lt;/span&gt;     b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
     &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (default_collate(t) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(b, _collate_types)
&lt;span style=&#34;color:#f92672&#34;&gt;---&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;46&lt;/span&gt;             &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; type(t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])([fa_collate(s) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; s &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;t)]) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(b, Sequence)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;47&lt;/span&gt;             &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; default_collate(t))
     &lt;span style=&#34;color:#ae81ff&#34;&gt;48&lt;/span&gt; 

&lt;span style=&#34;color:#f92672&#34;&gt;~/&lt;/span&gt;git&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;load&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;py &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt;listcomp&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;44&lt;/span&gt;     b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
     &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (default_collate(t) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(b, _collate_types)
&lt;span style=&#34;color:#f92672&#34;&gt;---&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;46&lt;/span&gt;             &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; type(t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])([fa_collate(s) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; s &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;t)]) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(b, Sequence)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;47&lt;/span&gt;             &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; default_collate(t))
     &lt;span style=&#34;color:#ae81ff&#34;&gt;48&lt;/span&gt; 

&lt;span style=&#34;color:#f92672&#34;&gt;~/&lt;/span&gt;git&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;fastai&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;load&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;py &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; fa_collate(t)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;43&lt;/span&gt; &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;fa_collate&lt;/span&gt;(t):
     &lt;span style=&#34;color:#ae81ff&#34;&gt;44&lt;/span&gt;     b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;span style=&#34;color:#f92672&#34;&gt;---&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;45&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (default_collate(t) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(b, _collate_types)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;46&lt;/span&gt;             &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; type(t[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])([fa_collate(s) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; s &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; zip(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;t)]) &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; isinstance(b, Sequence)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;47&lt;/span&gt;             &lt;span style=&#34;color:#66d9ef&#34;&gt;else&lt;/span&gt; default_collate(t))

&lt;span style=&#34;color:#f92672&#34;&gt;~/&lt;/span&gt;anaconda3&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;lib&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;python3&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;site&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;packages&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;utils&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;data&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;_utils&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;collate&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;py &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; default_collate(batch)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;53&lt;/span&gt;             storage &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; elem&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;storage()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;_new_shared(numel)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;54&lt;/span&gt;             out &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; elem&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(storage)
&lt;span style=&#34;color:#f92672&#34;&gt;---&amp;gt;&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;55&lt;/span&gt;         &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(batch, &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;, out&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;out)
     &lt;span style=&#34;color:#ae81ff&#34;&gt;56&lt;/span&gt;     &lt;span style=&#34;color:#66d9ef&#34;&gt;elif&lt;/span&gt; elem_type&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__module__ &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;numpy&amp;#39;&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; elem_type&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__ &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;str_&amp;#39;&lt;/span&gt; \
     &lt;span style=&#34;color:#ae81ff&#34;&gt;57&lt;/span&gt;             &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; elem_type&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;__name__ &lt;span style=&#34;color:#f92672&#34;&gt;!=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;string_&amp;#39;&lt;/span&gt;:

&lt;span style=&#34;color:#a6e22e&#34;&gt;RuntimeError&lt;/span&gt;: invalid argument &lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;: Sizes of tensors must match &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dimension &lt;span style=&#34;color:#ae81ff&#34;&gt;0.&lt;/span&gt; Got &lt;span style=&#34;color:#ae81ff&#34;&gt;414&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;and&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;375&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dimension &lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; at &lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;opt&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;conda&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;conda&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;bld&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;pytorch_1579022060824&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;work&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;aten&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;src&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;TH&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;generic&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;THTensor&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cpp:&lt;span style=&#34;color:#ae81ff&#34;&gt;612&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;now we can train it&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.12 Adding a Nonlinearity</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.12/</link>
      <pubDate>Thu, 29 Apr 2021 14:45:20 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.12/</guid>
      <description>My Code Source Code
Adding a Nonlinearity def simple_net(xb): res = xb@w1 + b1 res = res.max(tensor(0.0)) res = res@w2 + b2 return res # init the w and b just like we did in the previous section: w1 = init_params((28*28,30)) b1 = init_params(30) w2 = init_params((30,1)) b2 = init_params(1) why w1 = init_params((28*28,30)) is 30???? That means that the first layer can construct 30 different features, each representing some different mix of pixels.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;adding-a-nonlinearity&#34;&gt;Adding a Nonlinearity&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;simple_net&lt;/span&gt;(xb): 
    res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xb&lt;span style=&#34;color:#a6e22e&#34;&gt;@w1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b1
    res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; res&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;max(tensor(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0&lt;/span&gt;))
    res &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; res&lt;span style=&#34;color:#a6e22e&#34;&gt;@w2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; b2
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; res
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# init the w and b just like we did in the previous section:&lt;/span&gt;
w1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params((&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;))
b1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;)
w2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params((&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
b2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;why w1 = init_params((28*28,30)) is 30???? That means that the first layer can construct 30 different features, each representing some different mix of pixels. You can change that 30 to anything you like, to make the model more or less complex.&lt;/p&gt;
&lt;p&gt;w2 neeed to match w1,so 30 too&lt;/p&gt;
&lt;h2 id=&#34;rectified-linear-unit-整流線性單元relu&#34;&gt;rectified linear unit ,整流線性單元,RelU&lt;/h2&gt;
&lt;p&gt;what is res.max(tensor(0.0))???rectified linear unit ,整流線性單元,RelU,in other words, replace every negative number with a zero. This tiny function is also available in PyTorch as F.relu:&lt;/p&gt;
&lt;p&gt;Why ????? The basic idea is that by using more linear layers,  can have our model do more computation, and therefore model more complex functions. But because when we multiply things together and then add them up multiple times, that could be replaced by multiplying different things together and adding them up just once! That is to say, a series of any number of linear layers in a row can be replaced with a single linear layer with a different set of parameters.&lt;/p&gt;
&lt;p&gt;But if we put a nonlinear function between them, such as max, then this is no longer true. Now each linear layer is actually somewhat decoupled from the other ones, and can do its own useful work. The max function is particularly interesting, because it operates as a simple if statement.&lt;/p&gt;
&lt;p&gt;為什麼 ？？？？？ 基本思想是，通過使用更多的線性層，可以使我們的模型進行更多的計算，從而為更複雜的函數建模。 但是，因為當我們將事物相乘然後多次相加時，可以通過將不同事物相乘並僅相加一次來代替！ 也就是說，可以將一行中任意數量的線性層中的一系列序列替換為具有不同參數集的單個線性層。&lt;/p&gt;
&lt;p&gt;但是，如果我們在它們之間放置一個非線性函數（例如max），則不再適用。 現在，每個線性層實際上都已與其他線性層解耦，並且可以做自己有用的工作。 max函數特別有趣，因為它作為簡單的if語句運行。&lt;/p&gt;
&lt;p&gt;Amazingly enough, it can be mathematically proven that this little function can solve any computable problem to an arbitrarily high level of accuracy, if you can find the right parameters for w1 and w2 and if you make these matrices big enough. For any arbitrarily wiggly function, we can approximate it as a bunch of lines joined together; to make it closer to the wiggly function, we just have to use shorter lines. This is known as the universal approximation theorem. The three lines of code that we have here are known as layers. The first and third are known as linear layers, and the second line of code is known variously as a nonlinearity, or activation function.&lt;/p&gt;
&lt;p&gt;Just like in the previous section, we can replace this code with something a bit simpler, by taking advantage of PyTorch:&lt;/p&gt;
&lt;p&gt;足夠令人驚訝的是，如果可以找到w1和w2的正確參數，並且使這些矩陣足夠大，則可以用數學方式證明此小函數可以以任意高的精度解決任何可計算的問題。 對於任何任意擺動的函數，我們可以將其近似為一束連接在一起的線。 為了使其更接近任意擺動的函數，我們只需要使用較短的線即可。 這被稱為通用近似定理。 我們在這裡擁有的三行代碼稱為層。 第一和第三層被稱為線性層，第二行代碼被不同地稱為非線性或激活函數。&lt;/p&gt;
&lt;p&gt;就像在上一節中一樣，我們可以利用PyTorch將代碼替換為更簡單的代碼：&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# floow the sequence Linear -&amp;gt;ReLU-&amp;gt;Linear&lt;/span&gt;
simple_net &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Sequential(
    nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;),
    nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ReLU(),
    nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;30&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Learner(dls, simple_net, opt_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SGD,
                loss_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mnist_loss, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;batch_accuracy)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# lr  =0.1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# eopch num 40&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(&lt;span style=&#34;color:#ae81ff&#34;&gt;40&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot(L(learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recorder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;values)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;itemgot(&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;));
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/sl.PNG&#34; alt=&#34;sl&#34;&gt;
we can see that, 1.A function that can solve any problem to any level of accuracy (the neural network) given the correct set of parameters
2.A way to find the best set of parameters for any function (stochastic gradient descent)&lt;/p&gt;
&lt;h1 id=&#34;going-deeper&#34;&gt;Going Deeper&lt;/h1&gt;
&lt;p&gt;if this can approximate any function with a single nonlinearity with two linear layers,why we nee to go deeper???? because performance With a deeper model (that is, one with more layers) we do not need to use as many parameters; it turns out that we can use smaller matrices with more layers, and get better results than we would get with larger matrices, and few layers.&lt;/p&gt;
&lt;p&gt;that mean we can train the mode lquicky,smaller memory&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 18 layer,only one epoch,90%!!!&lt;/span&gt;
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ImageDataLoaders&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_folder(path)
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet18, pretrained&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False,
                    loss_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cross_entropy, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;accuracy)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;some-term&#34;&gt;Some term&lt;/h1&gt;
&lt;p&gt;&lt;strong&gt;Activations&lt;/strong&gt;:: Numbers that are calculated (both by linear and nonlinear layers)
&lt;strong&gt;Parameters&lt;/strong&gt;:: Numbers that are randomly initialized, and optimized (that is, the numbers that define the model)&lt;/p&gt;
&lt;p&gt;Our activations and parameters are all contained in tensors. These are simply regularly shaped arrays—for example, a matrix. Matrices have rows and columns; we call these the axes or dimensions. The number of dimensions of a tensor is its rank. There are some special tensors:&lt;/p&gt;
&lt;p&gt;Rank zero: scalar Rank one: vector Rank two: matrix&lt;/p&gt;
&lt;p&gt;A neural network contains a number of layers. Each layer is either linear or nonlinear. We generally alternate between these two kinds of layers in a neural network. Sometimes people refer to both a linear layer and its subsequent nonlinearity together as a single layer. Yes, this is confusing. Sometimes a nonlinearity is referred to as an &lt;strong&gt;activation function&lt;/strong&gt;.&lt;/p&gt;
&lt;h2 id=&#34;deep-learning-vocabulary&#34;&gt;Deep learning vocabulary&lt;/h2&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;ReLU&lt;/td&gt;
&lt;td&gt;Function that returns 0 for negative numbers and doesn&amp;rsquo;t change positive numbers.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Mini-batch&lt;/td&gt;
&lt;td&gt;A small group of inputs and labels gathered together in two arrays. A gradient descent step is updated on this batch (rather than a whole epoch).&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Forward pass&lt;/td&gt;
&lt;td&gt;Applying the model to some input and computing the predictions.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Loss&lt;/td&gt;
&lt;td&gt;A value that represents how well (or badly) our model is doing.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient&lt;/td&gt;
&lt;td&gt;The derivative of the loss with respect to some parameter of the model.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Backward pass&lt;/td&gt;
&lt;td&gt;Computing the gradients of the loss with respect to all model parameters.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Gradient descent&lt;/td&gt;
&lt;td&gt;Taking a step in the directions opposite to the gradients to make the model parameters a little bit better.&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Learning rate&lt;/td&gt;
&lt;td&gt;The size of the step we take when applying SGD to update the parameters of the model.&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.11 Creating an Optimizer</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.11/</link>
      <pubDate>Thu, 29 Apr 2021 11:23:16 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.11/</guid>
      <description>My Code Source Code
Creating an Optimizer we can make the above code more general to use # use nn.Linear to replace the linear1 # it do the same thing with the linear1 and init_params linear_model = nn.Linear(28*28,1) # we can get the paramater, weight,basic w,b = linear_model.parameters() w.shape,b.shape,b # (torch.Size([1, 784]), torch.Size([1]), Parameter containing: # tensor([-0.0180], requires_grad=True)) class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;creating-an-optimizer&#34;&gt;Creating an Optimizer&lt;/h1&gt;
&lt;h2 id=&#34;we-can-make-the-above-code-more-general-to-use&#34;&gt;we can make the above code more general to use&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# use nn.Linear to replace the linear1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# it do the same thing with the linear1 and init_params&lt;/span&gt;
linear_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#  we can get the paramater, weight,basic&lt;/span&gt;
w,b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; linear_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters()
w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,b

&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([1, 784]), torch.Size([1]), Parameter containing:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  tensor([-0.0180], requires_grad=True))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;BasicOptim&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self,params,lr): self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;params,self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(params),lr

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;step&lt;/span&gt;(self, &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;params: p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;zero_grad&lt;/span&gt;(self, &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;params: p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;opt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BasicOptim(linear_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters(), lr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;simplfy the trainb loop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# use it&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train_epoch&lt;/span&gt;(model):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dl:
        calc_grad(xb, yb, model)
        opt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;step()
        opt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_grad()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.10/#put-above-together-to-create-calc_grad-functions&#34; title=&#34;calc_grad&#34;&gt;calc_grad&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train_model&lt;/span&gt;(model, epochs):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(epochs):
        train_epoch(model)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(validate_epoch(model), end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
train_model(linear_model, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# same with above code&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;actually-fastai-already-have-the-same-thing&#34;&gt;Actually ,fastai already have the same thing&lt;/h2&gt;
&lt;p&gt;to replace the BasicOptim&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;linear_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
opt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SGD(linear_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters(), lr)
train_model(linear_model, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;to replace the train train_model&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoaders(dl, valid_dl)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Learner(dls, nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), opt_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SGD,
                loss_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mnist_loss, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;batch_accuracy)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;nn.Linear:how to predict the value&lt;br&gt;
opt_func:howw to change the weight&lt;br&gt;
loss_func:how to calculate the loss&lt;br&gt;
metrics:how to calculate the metrics&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#better-loss-finction&#34; title=&#34;mnist_loss&#34;&gt;mnist_loss&lt;/a&gt;
&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.10/#put-above-together-to-create-calc_grad-functions&#34; title=&#34;batch_accuracy&#34;&gt;batch_accuracy&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;lr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss batch_accuracy time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.636991 0.503566 0.495584 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.553366 0.176069 0.857704 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.202398 0.188561 0.829244 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.088171 0.108241 0.912169 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.046019 0.078468 0.932287 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.029606 0.062658 0.947498 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 6 0.022893 0.052850 0.955839 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 7 0.019928 0.046356 0.962218 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 8 0.018433 0.041814 0.966143 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 9 0.017540 0.038480 0.968106 00:00&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.10 Put it all together</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.10/</link>
      <pubDate>Wed, 28 Apr 2021 16:52:44 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.10/</guid>
      <description>My Code Source Code
Put it all together each epoch is like this
# basic example # for x,y in dl: # pred = model(x) # loss = loss_func(pred, y) # loss.backward() # parameters -= parameters.grad * lr re-initialize our parameters:
weights = init_params((28*28,1)) bias = init_params(1) weights.shape # torch.Size([784, 1]) create DataLoader of train data from Dataset
dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape (torch.Size([784]), tensor([1])) # (torch.Size([784]), tensor([1])) create DataLoader of valid data valid data</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;put-it-all-together&#34;&gt;Put it all together&lt;/h1&gt;
&lt;p&gt;each epoch is like this&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# basic example&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# for x,y in dl:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     pred = model(x)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     loss = loss_func(pred, y)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     loss.backward()&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     parameters -= parameters.grad * lr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;re-initialize our parameters:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
weights &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params((&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
bias &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([784, 1])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create DataLoader of train data  from &lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#prepare-the-pytorch-need-format&#34; title=&#34;Dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(dset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;)
xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; first(dl)
xb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,yb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Size([&lt;span style=&#34;color:#ae81ff&#34;&gt;784&lt;/span&gt;]), tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([784]), tensor([1]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create DataLoader of valid data &lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#prepare-the-valid-data&#34; title=&#34;valid data&#34;&gt;valid data&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;valid_dl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(valid_dset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create a 4 size batch for test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;batch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_x[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
batch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([4, 784])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;alcaulate the predict
&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#predict--multi-image&#34; title=&#34;linear1&#34;&gt;linear1&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; linear1(batch)
preds
&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&amp;#39; tensor([[-4.5725],
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        [ 0.2557],
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        [-5.5496],
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        [ 3.6488]], grad_fn=&amp;lt;AddBackward0&amp;gt;) &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;calculate a loss&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mnist_loss(preds, train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])
loss
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(0.6119, grad_fn=&amp;lt;MeanBackward0&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we can calculate the gradients:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(),bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([784, 1]), tensor(-0.0103), tensor([-0.0712]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;put-above-together-to-create-calc_grad-functions&#34;&gt;put above together to create calc_grad functions&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;calc_grad&lt;/span&gt;(xb, yb, model):
    preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model(xb)
    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mnist_loss(preds, yb)
    loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;calc_grad(batch, train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;], linear1)
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(),bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(-0.0207), tensor([-0.1423]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;run again&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;calc_grad(batch, train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;], linear1)
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(),bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(-0.0310), tensor([-0.2135]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;have probelm!!!!! we expect the grad should be the same ,becasue all the parameter of the calc_grad is same,but not!!!
becasue because the loss.backward add the gradients of loss to any gradients that are currently stored. So, we have to set the current gradients to 0 first:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_()
bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# each epoch function&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# params already use in model&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train_epoch&lt;/span&gt;(model, lr, params):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dl:
        calc_grad(xb, yb, model)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; params:
            p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;lr
            p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That gives us this function to calculate our validation accuracy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;batch_accuracy&lt;/span&gt;(xb, yb):
    preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
    &lt;span style=&#34;color:#75715e&#34;&gt;# after sigmoid 0 becime 0.5 so, &amp;gt;0.5 ,is 1,that mean it is 3&lt;/span&gt;
    correct &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (preds&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; yb
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; correct&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can check it works:
linear1 calculate the prediction&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;batch_accuracy(linear1(batch), train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;check is it work&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;batch_accuracy(linear1(batch), train_y[:4])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;create a valid epoch function to our new weight model&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;validate_epoch&lt;/span&gt;(model):
    accs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [batch_accuracy(model(xb), yb) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; valid_dl]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; round(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(accs)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item(), &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;now-use-the-above-function-to-run-a-epoch&#34;&gt;now use the above function to run a epoch&lt;/h2&gt;
&lt;h3 id=&#34;strat-point&#34;&gt;strat point&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;
params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weights,bias
train_epoch(linear1, lr, params)
validate_epoch(linear1)
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.6268&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;do-more&#34;&gt;do more&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;):
    train_epoch(linear1, lr, params)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(validate_epoch(linear1), end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# 0.7656 0.875 0.9165 0.936 0.9443 0.954 0.9565 0.9575 0.9589 0.9599 0.9619 0.9628 0.9643 0.9662 0.9672 0.9682 0.9692 0.9697 0.9702 0.9702 &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.9 SGD and Mini-Batches</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.9/</link>
      <pubDate>Wed, 28 Apr 2021 16:06:34 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.9/</guid>
      <description>My Code Source Code we already have a SGD loss function,we can go to Step
which is to change or update the weights based on the gradients. This is called an optimization step.
basic Python string.ascii_lowercase string.ascii_lowercase # abcdefghijklmnopqrstuvwxyz basic Pytorch enumerate # L is something liek list # enumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在for 循環當中。 ds = L(enumerate(string.ascii_lowercase)) ds # [(0, &amp;#39;a&amp;#39;),(1, &amp;#39;b&amp;#39;),(2, &amp;#39;c&amp;#39;),(3, &amp;#39;d&amp;#39;),(4, &amp;#39;e&amp;#39;),(5, &amp;#39;f&amp;#39;),(6, &amp;#39;g&amp;#39;),(7, &amp;#39;h&amp;#39;),(8, &amp;#39;i&amp;#39;),(9, &amp;#39;j&amp;#39;)...] optimization step why Mini-Batches we can one item for 1 epoch,but this will be very slow,</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;
we already have a SGD loss function,we can go to &lt;code&gt;Step&lt;/code&gt;&lt;br&gt;
which is to change or update the weights based on the gradients. This is called an optimization step.&lt;/p&gt;
&lt;h1 id=&#34;basic-python&#34;&gt;basic Python&lt;/h1&gt;
&lt;h2 id=&#34;stringascii_lowercase&#34;&gt;string.ascii_lowercase&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_lowercase
&lt;span style=&#34;color:#75715e&#34;&gt;# abcdefghijklmnopqrstuvwxyz&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;basic-pytorch&#34;&gt;basic Pytorch&lt;/h1&gt;
&lt;h2 id=&#34;enumerate&#34;&gt;enumerate&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#  L is something liek list&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# enumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在for 循環當中。&lt;/span&gt;
ds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; L(enumerate(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_lowercase))
ds
&lt;span style=&#34;color:#75715e&#34;&gt;# [(0, &amp;#39;a&amp;#39;),(1, &amp;#39;b&amp;#39;),(2, &amp;#39;c&amp;#39;),(3, &amp;#39;d&amp;#39;),(4, &amp;#39;e&amp;#39;),(5, &amp;#39;f&amp;#39;),(6, &amp;#39;g&amp;#39;),(7, &amp;#39;h&amp;#39;),(8, &amp;#39;i&amp;#39;),(9, &amp;#39;j&amp;#39;)...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;optimization-step&#34;&gt;optimization step&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/sgd_step.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;why-mini-batches&#34;&gt;why Mini-Batches&lt;/h2&gt;
&lt;p&gt;we can one item for 1 epoch,but this will be very slow,&lt;/p&gt;
&lt;h3 id=&#34;1-single-image-size-batch&#34;&gt;1. single image size batch&lt;/h3&gt;
&lt;p&gt;if we ahve 256 picture,we predict 1 picture,tha we calculate the loss for the picture,than use the loss number to calculate the gradient,step the weight,next picture, &lt;code&gt;total 256 epoch&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-4-image-size-batch&#34;&gt;2. 4 image size batch&lt;/h3&gt;
&lt;p&gt;we have 256/4= 64 bitch picture, we predict 4 picture at a time,we calcuate 4 loss for 4 picture,than use a loss number  to calculate 4 gradient number ,step the weight,next batch,&lt;code&gt;total 64 epoch&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So use mini btach more fast!!!!!!!!&lt;/p&gt;
&lt;h2 id=&#34;other-reason-why-mini-batches&#34;&gt;Other reason why Mini-Batches&lt;/h2&gt;
&lt;p&gt;another reason that use mini batch not calculating the gradient on individual data items is that, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it&amp;rsquo;s helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!&lt;/p&gt;
&lt;h2 id=&#34;use-dataloader-to-create-batches&#34;&gt;Use DataLoader to create batches&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;coll &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;)
dl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(coll, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
list(dl)

&lt;span style=&#34;color:#75715e&#34;&gt;# [tensor([ 3, 12,  8, 10,  2]),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  tensor([ 9,  4,  7, 14,  5]),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  tensor([ 1, 13,  0,  6, 11])]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.8 The MNIST Loss Function</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.8/</link>
      <pubDate>Wed, 28 Apr 2021 12:44:41 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.8/</guid>
      <description>My Code Source Code
MNIST Loss Function some basic python zip # zip a=[1,2,3,4] b=[5,6,7,8] list(zip(a,b)) # [(1, 5), (2, 6), (3, 7), (4, 8)] create array [1]*4 # [1, 1, 1, 1] tensor([1]*4 + [0]*3) # tensor([1, 1, 1, 1, 0, 0, 0]) Some basic pytorch functions horizontal tensor to vertical tensors tensor([1]*4 + [0]*3) # tensor([1, 1, 1, 1, 0, 0, 0]) tensor([1]*4 + [0]*3).unsqueeze(1) # tensor([[1], # [1], # [1], # [1], # [0], # [0], # [0]]) torch.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;_mnist-loss-function_&#34;&gt;&lt;em&gt;MNIST Loss Function&lt;/em&gt;&lt;/h1&gt;
&lt;h2 id=&#34;some-basic-python&#34;&gt;some basic python&lt;/h2&gt;
&lt;h3 id=&#34;zip&#34;&gt;zip&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# zip&lt;/span&gt;
a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
list(zip(a,b))
&lt;span style=&#34;color:#75715e&#34;&gt;# [(1, 5), (2, 6), (3, 7), (4, 8)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;create-array&#34;&gt;create array&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# [1, 1, 1, 1]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([1, 1, 1, 1, 0, 0, 0])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;some-basic-pytorch-functions&#34;&gt;Some basic pytorch functions&lt;/h2&gt;
&lt;h3 id=&#34;horizontal-tensor-to-vertical-tensors&#34;&gt;horizontal tensor to vertical tensors&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([1, 1, 1, 1, 0, 0, 0])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;torchcat&#34;&gt;torch.cat&lt;/h3&gt;
&lt;p&gt;connect two tensors together
&lt;a href=&#34;https://blog.csdn.net/qq_39709535/article/details/80803003&#34;&gt;https://blog.csdn.net/qq_39709535/article/details/80803003&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;A&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;#2x3的张量（矩阵）                                     &lt;/span&gt;
A
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1., 1., 1.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1., 1., 1.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1., 1., 1.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1., 1., 1.]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;B&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
B
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[2., 2., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2., 2., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2., 2., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2., 2., 2.]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;C&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat([A,B])
C&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([8, 3])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;tensorview&#34;&gt;Tensor.view&lt;/h3&gt;
&lt;p&gt;PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor.&lt;/p&gt;
&lt;p&gt;把原先tensor中的數據按照行優先的順序排成一個一維的數據（這裡應該是因為要求地址是連續存儲的），然後按照參數組合成其他維度的tensor。比如說是不管你原先的數據是[ [[1,2,3],[4,5,6]]]還是[1,2,3,4,5,6]，因為它們排成一維向量都是6個元素，所以只要view後面的參數一致，得到的結果都是一樣的。比如，
example&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Tensor([[[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;],[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]])
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))


&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [3., 4.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [5., 6.]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;torchrandn&#34;&gt;torch.randn&lt;/h3&gt;
&lt;p&gt;create a list of random numberless&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([ 0.9912,  0.4679, -0.2049, -0.7409,  0.3618,  1.9199, -0.2254, -0.3417])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn((&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[ 0.3040],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-0.6890],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.1267],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-0.2858],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.0935],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [ 1.1351],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [ 0.7592],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-3.5945]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;matrix-multiplication&#34;&gt;matrix multiplication&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;A&lt;span style=&#34;color:#a6e22e&#34;&gt;@B&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/matrix_m.PNG&#34; alt=&#34;rt&#34;&gt;
For instance, row 1, column 2 (the orange dot with a red border) is calculated as a1,1∗b1,2+a1,2∗b2,2&lt;/p&gt;
&lt;h2 id=&#34;mnist-loss-function&#34;&gt;MNIST Loss Function&lt;/h2&gt;
&lt;h3 id=&#34;prepare-the-train-data&#34;&gt;Prepare the train data&lt;/h3&gt;
&lt;h4 id=&#34;connect-the-photo&#34;&gt;connect the photo&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([6131, 28, 28])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;stacked_sevens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([6265, 28, 28])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;train_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat([stacked_threes, stacked_sevens])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;)
train_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([12396, 784])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the above acode,we first connect the stacked_threes(each pixel present by 0-1 number) and
for each picture , orginal is respenct by a 2d tensor,(28&lt;em&gt;28),turn to 1d tensor 784
&lt;code&gt;view(-1, 28*28)&lt;/code&gt; mean 28&lt;/em&gt;28 column,-1 mean not specific the row number,just make it can fit the content,becasue we have
&lt;code&gt;6131+6265=12396&lt;/code&gt;&lt;/p&gt;
&lt;h4 id=&#34;add-the-tag-for-each-photo&#34;&gt;add the tag for each photo&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# assign the tag to each image&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# We need a label for each image. We&amp;#39;ll use `1` for 3s and `0` for 7s:&lt;/span&gt;
train_y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;len(threes) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;len(sevens))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
train_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,train_y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([12396, 784]), torch.Size([12396, 1]))&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# train_X,12396 images,each image total 784 pixels&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#train_y,12396 tag,because eachpicture 1 tag,1 tag inf in each tag&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h4 id=&#34;prepare-the-pytorch-need-format&#34;&gt;prepare the Pytorch need format&lt;/h4&gt;
&lt;p&gt;A Dataset in PyTorch is required to return a tuple of (x,y) when indexed. Python provides a zip function which, when combined with list, provides a simple way to get this functionality:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(zip(train_x,train_y))
x,y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dset[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,y
&lt;span style=&#34;color:#75715e&#34;&gt;# x is  the image,t is the tag&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([784]), tensor([1]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;prepare-the-valid-data&#34;&gt;Prepare the valid data&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;valid_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat([valid_3_tens, valid_7_tens])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;)
valid_y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;len(valid_3_tens) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;len(valid_7_tens))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
valid_dset &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(zip(valid_x,valid_y))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;create-random-init-param&#34;&gt;create random init param&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;init_params&lt;/span&gt;(size, std&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.0&lt;/span&gt;): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(size)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;std)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# weneed a vertical 2d array,show we need (28*28,1),not only 28*28&lt;/span&gt;
weights &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params((&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;span style=&#34;color:#75715e&#34;&gt;# weights&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The function &lt;code&gt;weights*pixels&lt;/code&gt; won&amp;rsquo;t be flexible enough—it is always equal to 0 when the pixels are equal to 0 (i.e., its &lt;em&gt;intercept&lt;/em&gt; is 0). You might remember from high school math that the formula for a line is &lt;code&gt;y=w*x+b&lt;/code&gt;; we still need the &lt;code&gt;b&lt;/code&gt;. We&amp;rsquo;ll initialize it to a random number too:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;bias &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# why??????&lt;/span&gt;
bias
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;predict-a-image&#34;&gt;Predict a image&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;(train_x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;T)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum() &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; bias
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can use a foor loop to calculate all the image pred,but this is slow&lt;br&gt;
so we use matrix multiplication  ,more fast can use GPU
we suggest you take a look at the Intro to Matrix Multiplication &lt;a href=&#34;https://www.youtube.com/watch?v=kT4Mp9EdVqs&amp;amp;ab_channel=KhanAcademy&#34;&gt;https://www.youtube.com/watch?v=kT4Mp9EdVqs&amp;amp;ab_channel=KhanAcademy&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/matrix_m.PNG&#34; alt=&#34;rt&#34;&gt;
For instance, row 1, column 2 (the orange dot with a red border) is calculated as  a1,1∗b1,2+a1,2∗b2,2&lt;/p&gt;
&lt;h3 id=&#34;predict--multi-image&#34;&gt;Predict  multi image&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([784, 1])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;train_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([12396, 784])&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# xb@weights + bias is the formula to predict is the image is 3 or 7,1 is 3,0 is 7&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;linear1&lt;/span&gt;(xb): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; xb&lt;span style=&#34;color:#a6e22e&#34;&gt;@weights&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; bias
preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; linear1(train_x)
preds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;corrects &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (preds&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float() &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; train_y
corrects
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[ True],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [ True],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [ True],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         ...,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [False],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [False],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [False]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;corrects&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item()
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.49080348014831543&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;first-loss-finction&#34;&gt;first loss finction&lt;/h3&gt;
&lt;p&gt;suppose we had three images which we knew were a 3, a 7, and a 3. And suppose our model predicted with high confidence (0.9) that the first was a 3, with slight confidence (0.4) that the second was a 7, and with fair confidence (0.2), but incorrectly, that the last was a 7. This would mean our loss function would receive these values as its inputs:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# 1 is 3,0 is 7&lt;/span&gt;
trgts  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;])
prds   &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.4&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;])

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;C/CUDA speed
具体的意思可以理解为：针对于x而言，如果其中的每个元素都满足condition，就返回x的值；如果不满足condition，就将y对应位置的元素或者y的值(如果y为氮元素tensor的话)替换x的值，&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# low is good&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# low is good&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mnist_loss&lt;/span&gt;(predictions, targets):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;where(targets&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;predictions, predictions)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# example&lt;/span&gt;
torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;where(trgts&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;prds, prds)
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.where(trgts==1, 1-prds, prds)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;mnist_loss(prds,trgts)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(0.4333)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;better-loss-finction&#34;&gt;better loss finction&lt;/h3&gt;
&lt;p&gt;buts this mnist_loss has a problem , it assume the predict is alwasy0-1
we can use sigmoid function,it map all the value between 1 and 0&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exp(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;x))
plot_function(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid, title&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Sigmoid&amp;#39;&lt;/span&gt;, min&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/sig.png&#34; alt=&#34;sig&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mnist_loss&lt;/span&gt;(predictions, targets):
    predictions &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; predictions&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;where(targets&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;predictions, predictions)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;why select sigmoid()? becuase it can keep the mnist_loss function
has a meaningful derivative. It can&amp;rsquo;t have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level
This requirement means that sometimes it does not really reflect exactly what we are trying to achieve, but is rather a compromise between our real goal, and a function that can be optimized using its gradient.&lt;/p&gt;
&lt;p&gt;為什麼選擇sigmoid（）？ 因為它可以保留mnist_loss函數
具有有意義的導數 它不能有較大的扁平部分和較大的跳動，而必須相當平滑。 這就是為什麼我們設計一個損失函數以響應置信度水平的微小變化的原因
此要求意味著有時它不能真正反映出我們要實現的目標，但實際上是我們實際目標與可以使用其梯度進行優化的功能之間的折衷。&lt;/p&gt;
&lt;h2 id=&#34;loss-vs-metrics&#34;&gt;Loss vs Metrics&lt;/h2&gt;
&lt;p&gt;&lt;code&gt;Metrics&lt;/code&gt;, are the numbers that we really care about. These are the values that are printed at the end of each epoch that tell us how our model is really doing.  when judging the performance of a model,we use metrics&lt;/p&gt;
&lt;p&gt;&lt;code&gt;loss&lt;/code&gt;,To drive automated learning, the loss must be a function that has a meaningful derivative. It can&amp;rsquo;t have big flat sections and large jumps, but instead must be reasonably smooth. This is why we designed a loss function that would respond to small changes in confidence level.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.7 An End-to-End SGD Example</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.7/</link>
      <pubDate>Wed, 28 Apr 2021 11:57:33 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.7/</guid>
      <description>My Code Source Code
An End-to-End SGD Example we want to find the smallest value
Some useful function craete a 0-19 torch array
time = torch.arange(0,20).float(); time # tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) create randome number
# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。 num=20 t=torch.randn(num) time_f = torch.arange(0,num).float(); time plt.scatter(time_f,t); t simulate a car speed
# simulate a car speed # torch.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;_an-end-to-end-sgd-example_&#34;&gt;&lt;em&gt;An End-to-End SGD Example&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;we want to find the smallest value&lt;/p&gt;
&lt;h2 id=&#34;some-useful-function&#34;&gt;Some useful function&lt;/h2&gt;
&lt;p&gt;craete a 0-19 torch array&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create randome number&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。&lt;/span&gt;
num&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;
t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(num)
time_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,num)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time_f,t);
t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rt.PNG&#34; alt=&#34;rt&#34;&gt;&lt;/p&gt;
&lt;p&gt;simulate a car speed&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# simulate a car speed&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.randn(20)*3 is some random noise&lt;/span&gt;
time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
speed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.75&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(time&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9.5&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time,speed);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/car_speed.PNG&#34; alt=&#34;car_speed&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;use-sgd-to-find-the-smallest-value-for-the-loss&#34;&gt;use SGD to find the smallest value for the loss&lt;/h2&gt;
&lt;h3 id=&#34;step-0-gues-the-functions&#34;&gt;Step 0 gues the functions&lt;/h3&gt;
&lt;p&gt;we nedd to find the a,b,c that make the loss is the lowset
(time**2)+(b*time)+c&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(t, params):
    a,b,c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; params
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(t&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (b&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;t) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-01-define-the-meaning-of-best&#34;&gt;Step 0.1 define the meaning of best&lt;/h3&gt;
&lt;p&gt;we use a loss function to define the best, which will return a value based on a prediction and a target, where lower values of the function correspond to &amp;ldquo;better&amp;rdquo; predictions. For continuous data, it&amp;rsquo;s common to use mean squared error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mse&lt;/span&gt;(preds, targets): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ((preds&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;targets)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-1-set-the-apramter-as-a-randome-value&#34;&gt;Step 1 set the apramter as a randome value&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;params&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None
params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()
orig_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;clone()
params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-2-calculate-the-predict&#34;&gt;Step 2 calculate the predict&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time, params)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;show_preds&lt;/span&gt;(preds, ax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ax &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None: ax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots()[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time, speed)
    &lt;span style=&#34;color:#75715e&#34;&gt;# to_npconvert tensor to numpy arry&lt;/span&gt;
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time, to_np(preds), color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;)
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_ylim(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)

show_preds(preds)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pred1.PNG&#34; alt=&#34;pred1&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-3-calculate-the-losses&#34;&gt;Step 3 calculate the losses&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse(preds, speed)
loss
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(25.1871, grad_fn=&amp;lt;SqrtBackward&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-4--know-the-gradients&#34;&gt;Step 4  know the gradients&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# the a b c gradients&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([-3.1634, -0.2709, -0.3931])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-5--step-the-weights&#34;&gt;Step 5  Step the weights&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# assign the chnaged parameter to the params&lt;/span&gt;
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; lr &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s see if the loss has improved:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s see if the loss has improved:&lt;/span&gt;
preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time,params)
mse(preds, speed)
show_preds(preds)
&lt;span style=&#34;color:#75715e&#34;&gt;# improve a little bit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/ip.PNG&#34; alt=&#34;pred1&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-6--repeat-it&#34;&gt;step 6 , repeat it&lt;/h3&gt;
&lt;h1 id=&#34;we-use-a-for-loop-to-do-multi-time&#34;&gt;we use a for loop to do multi time&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;apply_step&lt;/span&gt;(params, prn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True):
    preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time, params)
    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse(preds, speed)
    loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
    params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; lr &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data
    params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; prn: &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item())
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; preds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;): apply_step(params)

&lt;span style=&#34;color:#75715e&#34;&gt;# 160.42279052734375&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 160.14772033691406&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.87269592285156&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.59768676757812&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.3227081298828&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.04774475097656&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.7728271484375&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.4979248046875&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.22305297851562&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 157.9481964111328&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;_,axs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;))
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ax &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; axs: show_preds(apply_step(params, False), ax)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tight_layout()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/4p.PNG&#34; alt=&#34;4p&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step7-stop&#34;&gt;Step7 stop&lt;/h3&gt;
&lt;p&gt;we do 10 round ,than stop**&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.6 Stepping With a Learning Rate</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.6/</link>
      <pubDate>Wed, 28 Apr 2021 11:30:02 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.6/</guid>
      <description>My Code Source Code
Stepping With a Learning Rate when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder).</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;_stepping-with-a-learning-rate_&#34;&gt;&lt;em&gt;Stepping With a Learning Rate&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder). Once you&amp;rsquo;ve picked a learning rate, you can adjust your parameters using this simple function:
w -= gradient(w) * lr&lt;br&gt;
This is known as &lt;em&gt;stepping&lt;/em&gt; your parameters, using an &lt;em&gt;optimizer step&lt;/em&gt;.
當我們得到梯度時，我們就用它來計算新的參數。 幾乎所有方法都始於將梯度乘以一個稱為學習率（LR）的小數的基本思想。 學習率通常是0.001到0.1之間的數字，儘管可以是任意數。通常，人們僅通過嘗試一些就可以選擇學習率，並在訓練後發現哪種模式可以得到最佳模型（我們將在稍後向您展示一種更好的方法 在這本書中，稱為學習率查找器）。 選擇學習速度後，您可以使用以下簡單功能調整參數：
w -= gradient(w) * lr&lt;br&gt;
使用“優化步”，這稱為“步進”你的參數。&lt;/p&gt;
&lt;p&gt;if your Lr too small,maybe too slow,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/step_small.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;if LR too big,it can actually result in the loss getting worse,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/strp_big1.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/steo_big2.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.5 Gredient</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.5/</link>
      <pubDate>Wed, 28 Apr 2021 11:15:32 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.5/</guid>
      <description>My Code Source Code
Gradients explain for the Gredient 
calculate for the gradient in program def f(x): return x**2 the function in 4.4
# select a tensor to calculate the grad xt = tensor(3.).requires_grad_() xt yt = f(xt) yt # calculate the gradients yt.backward() # see the grad,answer is 6 xt.grad # tensor(6.) another example xt = tensor([3.,4.,10.]).requires_grad_() def f(x): return (x**2).sum() yt = f(xt) yt.backward() xt.grad </description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;_gradients_&#34;&gt;&lt;em&gt;Gradients&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.khanacademy.org/math/differential-calculus/dc-diff-intro&#34;&gt;explain for the Gredient
&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;calculate-for-the-gradient-in-program&#34;&gt;calculate for the gradient in program&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.4/&#34; title=&#34;the function in 4.4&#34;&gt;the function in 4.4&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# select a tensor to calculate the grad&lt;/span&gt;
xt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor(&lt;span style=&#34;color:#ae81ff&#34;&gt;3.&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()
xt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;yt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(xt)
yt

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# calculate the gradients&lt;/span&gt;
yt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# see the grad,answer is 6&lt;/span&gt;
xt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(6.)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;another-example&#34;&gt;another example&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
xt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;3.&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4.&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10.&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()

yt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(xt)

yt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
xt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.4 Stochastic Gradient Descent 隨機梯度下降 (SGD)</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.4/</link>
      <pubDate>Tue, 27 Apr 2021 19:56:41 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.4/</guid>
      <description>SGD Instead of trying to find the similarity between an image and an &amp;ldquo;ideal image,&amp;rdquo; we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8.</description>
      <content>&lt;h1 id=&#34;_sgd_&#34;&gt;&lt;em&gt;SGD&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Instead of trying to find the similarity between an image and an &amp;ldquo;ideal image,&amp;rdquo; we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category—for instance the probability of being the number 8:&lt;/p&gt;
&lt;p&gt;與其嘗試查找圖像與“理想圖像”之間的相似性，不如查看每個單獨的像素並為每個像素提出一組權重，以使最高的權重與最有可能與之相關的那些像素相關聯。 對於特定類別為黑色。 例如，朝右下角移動的像素不太可能為7激活，因此對於7,像素應該具有較低的權重，但對於8,像素應該很容易被激活，因此對於8,像素應該具有較高的權重.這可以表示為每個可能類別的一個函數和一組權重值，例如，成為數字8的概率：&lt;br&gt;
&lt;code&gt;def pr_eight(x,w): return (x*w).sum()&lt;/code&gt;&lt;br&gt;
x is the image, represented as a vector—in other words, with all of the rows stacked up end to end into a single long line. And we are assuming that the weights are a vector w. If we have this function, then we just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.&lt;/p&gt;
&lt;p&gt;x是表示為矢量的圖像，換句話說，所有行首尾相連地排成一條長線。 並且我們假設權重是向量w。 如果我們具有此功能，那麼我們只需要一些方法來更新權重即可使它們更好一點。 通過這種方法，我們可以重複該步驟多次，使權重越來越好，直到權重達到我們所能達到的程度為止。&lt;/p&gt;
&lt;p&gt;want to find the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not. Searching for the best vector w is a way to search for the best function for recognising 8s.&lt;/p&gt;
&lt;p&gt;想要找到向量w的特定值，該值導致函數的結果對於那些實際上是8s的圖像來說較高，而對於那些不是8s的圖像來說較低。 搜索最佳向量w是搜索識別8s的最佳函數的一種方式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize the weights.初始化權重。&lt;/li&gt;
&lt;li&gt;For each image, use these weights to predict whether it appears to be a 3 or a 7.對於每個圖像，使用這些權重來預測它是3還是7。&lt;/li&gt;
&lt;li&gt;Based on these predictions, calculate how good the model is (its loss).根據這些預測，計算模型的好壞（損失）。&lt;/li&gt;
&lt;li&gt;Calculate the gradient, which measures for each weight, how changing that weight would change the loss.計算坡度，該坡度針對每個權重進行度量，更改該權重將如何改變損耗&lt;/li&gt;
&lt;li&gt;Step (that is, change) all the weights based on that calculation.根據該計算步進（即更改）所有權重。&lt;/li&gt;
&lt;li&gt;Go back to the step 2, and repeat the process.返回到步驟2，並重複該過程。&lt;/li&gt;
&lt;li&gt;Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&amp;rsquo;t want to wait any longer).重複進行直到您決定停止訓練過程為止（例如，因為模型足夠好或者您不想再等待了）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/sgd_step.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are many different ways to do each of these seven steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize:: initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category—but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.
將參數初始化為隨機值。 這聽起來可能令人驚訝。 當然，我們還可以做出其他選擇，例如將其初始化為該類別的像素被激活的次數的百分比-但由於我們已經知道我們有一個例程可以改善這些權重，因此事實證明，只是從隨機權重開始 效果很好。&lt;/li&gt;
&lt;li&gt;Loss:: when testing the effectiveness of any current weight assignment in terms of actual performance. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).
在實際性能方面測試任何當前重量分配的有效性時。 如果模型的性能良好，我們需要一些函數返回一個較小的數字（標準方法是將小的損失視為好，將大損失視為壞，儘管這只是一個慣例）。&lt;/li&gt;
&lt;li&gt;Step:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.
一種簡單的判斷重量是否應該增加還是減少的簡單方法就是嘗試：將重量增加一點，然後看看損失是增加還是減少。 找到正確的方向後，您可以再多一點，少一點地更改該金額，直到找到一個行之有效的金額。 但是，這很慢！ 就像我們將看到的那樣，微積分的神奇之處使我們能夠直接弄清楚改變每個權重的方向和大致幅度，而不必嘗試所有這些小的改變。 做到這一點的方法是通過計算梯度。 這只是性能優化，通過使用較慢的手動過程，我們也將獲得完全相同的結果。&lt;/li&gt;
&lt;li&gt;Stop:: Once we&amp;rsquo;ve decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.
一旦我們確定了訓練模型的時間（在前面的列表中給出了一些建議），我們就會應用該決定。 這就是應用該決定的地方。 對於我們的數字分類器，我們將繼續訓練直到模型的準確性開始變差或用完為止。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;simple-example-of-sgd&#34;&gt;simple example of SGD&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plot_function(f, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x**2&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/x2p.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;The sequence of steps we described earlier starts by picking some random value for a parameter, and calculating the value of the loss:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plot_function(f, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x**2&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;, f(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;), color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/2pr.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;if we increased or decreased our parameter by a little bit—the adjustment. This is simply the slope at a particular point:
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rs1.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve:
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rs2.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;we want to find the lower y/(Loss),lowest is good,we we replay to try different x, to find the lowest y. this method is slow,a better ,is The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.3 Metric</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.3/</link>
      <pubDate>Tue, 27 Apr 2021 18:18:28 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.3/</guid>
      <description>My Code Source Code
Computing Metrics Using Broadcasting Metric a metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is.
we want to calculate our metric over a validation set. This is so that we don&amp;rsquo;t inadvertently overfit—that is, train a model to work well only on our training data</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;_computing-metrics-using-broadcasting_&#34;&gt;&lt;em&gt;Computing Metrics Using Broadcasting&lt;/em&gt;&lt;/h1&gt;
&lt;h4 id=&#34;metric&#34;&gt;Metric&lt;/h4&gt;
&lt;p&gt;a metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is.&lt;br&gt;
we want to calculate our metric over a validation set. This is so that we don&amp;rsquo;t inadvertently overfit—that is, train a model to work well only on our training data&lt;br&gt;
指標是根據我們的模型預測和數據集中的正確標籤計算出的數字，目的是告訴我們我們的模型有多好。&lt;br&gt;
我們要根據驗證集計算指標。 這樣一來，我們就不會無意間過度擬合-也就是說，訓練模型只能在訓練數據上有效地工作&lt;/p&gt;
&lt;p&gt;get the data&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;valid_3_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack([tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) 
                            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;valid&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()])
valid_3_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; valid_3_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
valid_7_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack([tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) 
                            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;valid&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;7&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()])
valid_7_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; valid_7_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
valid_3_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,valid_7_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape

&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;computing-metrics-using-broadcasting&#34;&gt;Computing Metrics Using Broadcasting&lt;/h2&gt;
&lt;p&gt;write a function that canculate the distance&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mnist_distance&lt;/span&gt;(a,b): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (a&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;b)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
mnist_distance(a_3, mean3)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(0.1114)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;for every image ,we do not need to write a loop ,we use Broadcasting&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
valid_3_dist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mnist_distance(valid_3_tens, mean3)
valid_3_dist, valid_3_dist&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor([0.1329, 0.1555, 0.1107,  ..., 0.1359, 0.1526, 0.1126]),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  torch.Size([1010]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;它沒有抱怨形狀不匹配，而是將每個單個圖像的距離作為長度為1,010（我們的驗證集中的3的數量）的向量（即1級張量）返回&lt;/p&gt;
&lt;p&gt;當PyTorch嘗試在不同等級的兩個張量之間執行簡單的減法運算時，它將使用廣播。 也就是說，它將自動擴展具有較小等級的張量，使其具有與具有較大等級的張量相同的大小。 廣播是一項重要功能，可使張量代碼更易於編寫。&lt;/p&gt;
&lt;p&gt;Instead of complaining about shapes not matching, it returned the distance for every single image as a vector (i.e., a rank-1 tensor) of length 1,010 (the number of 3s in our validation set).
PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use broadcasting. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.&lt;/p&gt;
&lt;h4 id=&#34;more-brodcast-example&#34;&gt;More Brodcast example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# brodcast example&lt;/span&gt;
tensor([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;],[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]]) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; tensor(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[2, 3, 4],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2, 3, 4]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;(valid_3_tens&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;mean3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape

&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([1010, 28, 28])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;is_3&#34;&gt;is_3&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_3&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mnist_distance(x,mean3) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; mnist_distance(x,mean7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;is_3(a_3), is_3(a_3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(True), tensor(1.))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;is_3(valid_3_tens)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([True, True, True,  ..., True, True, True])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;accuracy_3s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;      is_3(valid_3_tens)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float() &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
accuracy_7s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; is_3(valid_7_tens)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()

accuracy_3s,accuracy_7s,(accuracy_3s&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;accuracy_7s)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.9168), tensor(0.9854), tensor(0.9511))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;over 90% accuracy on both 3s and 7s!!!!&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.2</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.2/</link>
      <pubDate>Tue, 27 Apr 2021 18:14:43 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.2/</guid>
      <description>My Code Source Code
NumPy Arrays and PyTorch Tensors they almost the same but NumPy Arrays not support GPU
data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) arr # numpy # array([[1, 2, 3], # [4, 5, 6]]) tns # pytorch # tensor([[1, 2, 3], # [4, 5, 6]]) tns[1] # get index 1 # tensor([4, 5, 6]) tns[:,1] # all first axis,index 1 at ssecond axis # tensor([2, 5]) tns[1,1:3] # first axis :1,sendoc axis 1-3(exclude) # tensor([5, 6]) tns+1 # tensor([[2, 3, 4], # [5, 6, 7]]) tns*1.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;_numpy-arrays-and-pytorch-tensors_&#34;&gt;&lt;em&gt;NumPy Arrays and PyTorch Tensors&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;they almost the same but  NumPy Arrays not support GPU&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;],[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
arr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; array (data)
tns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor(data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;arr  &lt;span style=&#34;color:#75715e&#34;&gt;# numpy&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# array([[1, 2, 3],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [4, 5, 6]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tns  &lt;span style=&#34;color:#75715e&#34;&gt;# pytorch&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1, 2, 3],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [4, 5, 6]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tns[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# get index 1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([4, 5, 6])&lt;/span&gt;

tns[:,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# all first axis,index 1 at ssecond axis&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([2, 5])&lt;/span&gt;


tns[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# first axis :1,sendoc axis 1-3(exclude)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([5, 6])&lt;/span&gt;

tns&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[2, 3, 4],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [5, 6, 7]])&lt;/span&gt;

tns&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1.5000, 3.0000, 4.5000],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [6.0000, 7.5000, 9.0000]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.1</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.1/</link>
      <pubDate>Tue, 27 Apr 2021 15:51:50 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.1/</guid>
      <description>&#39; My Code Source Code
Get the number sample !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&amp;#39;image&amp;#39;, cmap=&amp;#39;Greys&amp;#39;) path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() [Path(&amp;lsquo;valid&amp;rsquo;),Path(&amp;lsquo;train&amp;rsquo;),Path(&amp;lsquo;labels.csv&amp;rsquo;)] train group use to train,valid group use to test
(path/&amp;#39;train&amp;#39;).ls() # [Path(&amp;#39;train/7&amp;#39;),Path(&amp;#39;train/3&amp;#39;)] threes = (path/&amp;#39;train&amp;#39;/&amp;#39;3&amp;#39;).ls().sorted() sevens = (path/&amp;#39;train&amp;#39;/&amp;#39;7&amp;#39;).ls().sorted() threes # (#6131) [Path(&amp;#39;train/3/10.png&amp;#39;),Path(&amp;#39;train/3/10000.png&amp;#39;),Path(&amp;#39;train/3/10011.png&amp;#39;),Path(&amp;#39;train/3/10031.png&amp;#39;),Path(&amp;#39;train/3/10034.png&amp;#39;),Path(&amp;#39;train/3/10042.png&amp;#39;),Path(&amp;#39;train/3/10052.png&amp;#39;),Path(&amp;#39;train/3/1007.png&amp;#39;),Path(&amp;#39;train/3/10074.png&amp;#39;),Path(&amp;#39;train/3/10091.png&amp;#39;)...] open a image to see
im3_path = threes[1]im3 = Image.open(im3_path)im3array(im3).shape # a 28 * 28 image # (28, 28) turn the image into a 2 d array show row 4:10(not include),column 4:10(not include)</description>
      <content>&lt;p&gt;&#39;
&lt;a href=&#34;https://colab.research.google.com/drive/1rMfM4H92wklMLDydjnChmJMHoJ3OS6SL?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/04_mnist_basics.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;_get-the-number-sample_&#34;&gt;&lt;em&gt;Get the number sample&lt;/em&gt;&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;Uqq fastbook
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fastbook
fastbook&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup_book()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.all &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastbook &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

matplotlib&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rc(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Greys&amp;#39;&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; untar_data(URLs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MNIST_SAMPLE)
Path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;BASE_PATH &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; path
path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;[Path(&amp;lsquo;valid&amp;rsquo;),Path(&amp;lsquo;train&amp;rsquo;),Path(&amp;lsquo;labels.csv&amp;rsquo;)]
train group use to train,valid group use to test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()
&lt;span style=&#34;color:#75715e&#34;&gt;# [Path(&amp;#39;train/7&amp;#39;),Path(&amp;#39;train/3&amp;#39;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;threes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sorted()
sevens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;7&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sorted()
threes
&lt;span style=&#34;color:#75715e&#34;&gt;# (#6131) [Path(&amp;#39;train/3/10.png&amp;#39;),Path(&amp;#39;train/3/10000.png&amp;#39;),Path(&amp;#39;train/3/10011.png&amp;#39;),Path(&amp;#39;train/3/10031.png&amp;#39;),Path(&amp;#39;train/3/10034.png&amp;#39;),Path(&amp;#39;train/3/10042.png&amp;#39;),Path(&amp;#39;train/3/10052.png&amp;#39;),Path(&amp;#39;train/3/1007.png&amp;#39;),Path(&amp;#39;train/3/10074.png&amp;#39;),Path(&amp;#39;train/3/10091.png&amp;#39;)...]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;open a image to see&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;im3_path = threes[1]
im3 = Image.open(im3_path)
im3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;array(im3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# a 28 * 28 image&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# (28, 28)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;turn the image into a  2 d array
show row 4:10(not include),column 4:10(not include)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;array(im3)[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# array([[  0,   0,   0,   0,   0,   0],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,   0,   0,   0,   0,  29],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,   0,   0,  48, 166, 224],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,  93, 244, 249, 253, 187],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0, 107, 253, 253, 230,  48],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,   3,  20,  20,  15,   0]], dtype=uint8)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;tensor is array use in pytorch&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor(im3)[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;show the head of th 3,the colr is Grey,0,is white&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;im3_t &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor(im3)
df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(im3_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;])
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;style&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_properties(&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;font-size&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;6pt&amp;#39;&lt;/span&gt;})&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;background_gradient(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Greys&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/3_head.PNG&#34; alt=&#34;3_h&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;_first-try-pixel-similarity-baseline_&#34;&gt;&lt;em&gt;First Try: Pixel Similarity Baseline&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;we fins the average pixel value of 3 and 7,so we can define the ideal 3 ,7,then the image compare to the ideal 3 7,to identify is 3 or 7&lt;/p&gt;
&lt;p&gt;因此，這是第一個想法：我們如何找到3s的每個像素的平均像素值，然後對7s進行相同的處理。 這將為我們提供兩個組平均值，定義我們可以稱之為“理想”的3和7。然後，將圖像分類為一個數字或另一個數字，我們將看到圖像與這兩個理想數字中的哪一個最相似。 當然，這似乎總比沒有好，因此它將成為一個良好的基準。&lt;/p&gt;
&lt;h3 id=&#34;what-is-a-baseline&#34;&gt;what is a baseline&lt;/h3&gt;
&lt;p&gt;A simple model which you are confident should perform reasonably well.&lt;br&gt;
您相信一個簡單的模型應該可以表現良好。 它應該很容易實現，也很容易測試，這樣您就可以測試每個改進的想法，並確保它們總是比基線更好。 不從合理的基准開始，很難知道您的超級幻想模型是否真的有用。 創建基準的一種好方法是執行我們在此處所做的工作：考慮一個簡單，易於實現的模型。 另一個好的方法是四處搜尋，以解決與您的問題相似的其他人，然後在您的數據集上下載並運行他們的代碼。 理想情況下，嘗試這兩個！&lt;/p&gt;
&lt;h2 id=&#34;step1&#34;&gt;step1&lt;/h2&gt;
&lt;p&gt;get the average of pixel values for each of our two groups
create a tensor containing all of our 3s stacked together&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(Image.open(o)) turn the image to a 2d array&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# this way for loop more fast&lt;/span&gt;
seven_tensors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sevens]
three_tensors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; threes]
len(three_tensors),len(seven_tensors)
&lt;span style=&#34;color:#75715e&#34;&gt;# (6131, 6265)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# display a image in tensor&lt;/span&gt;
show_image(three_tensors[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]);

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we want ot calculate each pixel avarge strange,so wee need to make a 3d tensor /array rank3 tensor each image to a 2d array,multi image become a 3d array&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rank.jpg&#34; alt=&#34;rank&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# change the whole tensorto the 0-1 floating point&lt;/span&gt;
stacked_sevens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(seven_tensors)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
stacked_threes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(three_tensors)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape &lt;span style=&#34;color:#75715e&#34;&gt;#torch.Size([6131, 28, 28]),6131 images, each 28*28 pixels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor. above shape is [6131, 28, 28],rank is 3 (len(stacked_threes.shape)==3)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# get the rank&lt;/span&gt;
len(stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;span style=&#34;color:#75715e&#34;&gt;# or&lt;/span&gt;
stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ndim  &lt;span style=&#34;color:#75715e&#34;&gt;#3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# calculate the mean of each pixel&lt;/span&gt;
mean3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
show_image(mean3);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/ideal_3.png&#34; alt=&#34;ideal3&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;mean7&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;stacked_sevens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
show_image(mean7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/ideal_7.png&#34; alt=&#34;ideal7&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# get a 3&lt;/span&gt;
a_3&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;stacked_threes[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
show_image(a_3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step2-now-we-compare-the-ideal-3-and-the-real-3&#34;&gt;step2 now we compare the ideal 3 and the real 3&lt;/h2&gt;
&lt;p&gt;How do we compare the a_3 and the mean3??&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;compare each pixel,get the bas,calculate the avh of the absof each pixel,This is called the mean absolute difference or L1 norm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;calculaet the (dif)^2,mean, than square root.This is called the root mean squared error (RMSE) or L2 norm.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# try&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1&lt;/span&gt;
dist_3_abs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;span style=&#34;color:#75715e&#34;&gt;# 2&lt;/span&gt;
dist_3_sqr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean3)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
dist_3_abs,dist_3_sqr

&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.1114), tensor(0.2021))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dist_7_abs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean7)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
dist_7_sqr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean7)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
dist_7_abs,dist_7_sqr
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.1586), tensor(0.3021))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In both cases, the distance between our 3 and the &amp;ldquo;ideal&amp;rdquo; 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case.&lt;/p&gt;
&lt;p&gt;PyTorch already provides both of these as loss functions. You&amp;rsquo;ll find these inside torch.nn.functional, which the PyTorch team recommends importing as F (and is available by default under that name in fastai):&lt;/p&gt;
&lt;p&gt;Here mse stands for mean squared error, and l1 refers to the standard mathematical jargon for mean absolute value (in math it&amp;rsquo;s called the L1 norm).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;l1_loss(a_3&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(),mean7), F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mse_loss(a_3,mean7)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.1586), tensor(0.3021))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 2</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-2/</link>
      <pubDate>Thu, 22 Apr 2021 15:30:42 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-2/</guid>
      <description>My Code Source COde
Build your Bear reconigize model Download image we use the Azure bing image search API
you need to apply the key for free
!pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * from fastai.vision.widgets import * key = &amp;#39;secret_key_from_bing&amp;#39; import requests import matplotlib.pyplot as plt from PIL import Image from io import BytesIO def search_images_bing_min(search_term): search_url = &amp;#34;https://api.bing.microsoft.com/v7.0/images/search&amp;#34; headers = {&amp;#34;Ocp-Apim-Subscription-Key&amp;#34; : key} # search_term = &amp;#34;grizzly bear&amp;#34; params = {&amp;#34;q&amp;#34;: search_term, &amp;#34;license&amp;#34;: &amp;#34;public&amp;#34;, &amp;#34;imageType&amp;#34;: &amp;#34;photo&amp;#34;,&amp;#34;count&amp;#34;:&amp;#39;150&amp;#39;} response = requests.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/10bYGsmx9OZYmk5xMw0kPAKNaD5ZGn9kr?usp=sharing&#34;&gt;My Code&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/02_production.ipynb&#34;&gt;Source COde&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;build-your-bear-reconigize-model&#34;&gt;Build your Bear reconigize model&lt;/h1&gt;
&lt;h2 id=&#34;download-image&#34;&gt;Download image&lt;/h2&gt;
&lt;p&gt;we use the Azure &lt;a href=&#34;https://www.microsoft.com/en-us/bing/apis/bing-image-search-api&#34;&gt;bing image search API&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;you need to apply the key for free&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;Uqq fastbook
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fastbook
fastbook&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup_book()

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastbook &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.widgets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;secret_key_from_bing&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; requests
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; io &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; BytesIO
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;search_images_bing_min&lt;/span&gt;(search_term):
  search_url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://api.bing.microsoft.com/v7.0/images/search&amp;#34;&lt;/span&gt;
  headers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ocp-Apim-Subscription-Key&amp;#34;&lt;/span&gt; : key}
  &lt;span style=&#34;color:#75715e&#34;&gt;# search_term = &amp;#34;grizzly bear&amp;#34;&lt;/span&gt;
  params  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;q&amp;#34;&lt;/span&gt;: search_term, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;license&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;public&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;imageType&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;photo&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;count&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;150&amp;#39;&lt;/span&gt;}
  response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(search_url, headers&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;headers, params&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;params)
  response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;raise_for_status()
  search_results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;json()
  &lt;span style=&#34;color:#75715e&#34;&gt;# ims=[img[&amp;#34;thumbnailUrl&amp;#34;] for img in search_results[&amp;#34;value&amp;#34;]]&lt;/span&gt;
  ims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;search_results[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;]
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ims
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can use the abovce function to down the image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;ims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;search_images_bing_min(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;grizzly bear&amp;#34;&lt;/span&gt;)
len(ims)

&lt;span style=&#34;color:#75715e&#34;&gt;# ims is a string array&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;now we can download the iomage to the google drive&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bear_types &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;grizzly&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;black&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;teddy&amp;#39;&lt;/span&gt;
path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Path(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bearss&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists():
    path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mkdir()
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bear_types:
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(o)
  dest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;o)
  dest&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mkdir(exist_ok&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
  results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; search_images_bing_min(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{o} bear&amp;#39;&lt;/span&gt;)
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# print(results)&lt;/span&gt;
  ims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[img[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;contentUrl&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; img &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; results]
  &lt;span style=&#34;color:#75715e&#34;&gt;# print(ims)&lt;/span&gt;
  download_images(dest, urls&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ims)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;clean the image,remove the fail image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;fns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_image_files(path)
fns
failed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; verify_images(fns)
failed
&lt;span style=&#34;color:#75715e&#34;&gt;# if fail,remove it&lt;/span&gt;
failed&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(Path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unlink);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;intro-to-create-the-model&#34;&gt;intro to create the model&lt;/h2&gt;
&lt;p&gt;datablock is the templat of a dataloader
data loader tell fastai 4 thing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;what is the type of the inf&lt;/li&gt;
&lt;li&gt;how to get the items&lt;/li&gt;
&lt;li&gt;how to tag the items&lt;/li&gt;
&lt;li&gt;How to create the validation set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;blocks=(ImageBlock, CategoryBlock), mean use the image to predict,&lt;/p&gt;
&lt;h1 id=&#34;categoryblock-mean-target-is-the-category&#34;&gt;CategoryBlock mean target is the category&lt;/h1&gt;
&lt;p&gt;get_items=get_image_files ,how to get the image,from files&lt;br&gt;
splitter mean how to get the validation set&lt;br&gt;
get_y mean how to get the Category&lt;br&gt;
parent_label mean use the parent folder as a category tag&lt;br&gt;
need t oresize all the image to same size,Resize(128)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# datablock is the templat of a dataloader&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# data loader tell fastai 4 thing:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1.what is the type of the inf&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2.how to get the items&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3.how to tag the items&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4.How to create the validation set&lt;/span&gt;


&lt;span style=&#34;color:#75715e&#34;&gt;# blocks=(ImageBlock, CategoryBlock), bean use the image to predict,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#CategoryBlock mean target is the category&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# get_items=get_image_files ,how to get the image,from files&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# splitter mean how to get the validation set&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# get_y mean how to get the Category&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# parent_label mean use the parent folder as a category tag&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# need t oresize all the image to same size,Resize(128)&lt;/span&gt;
bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataBlock(
    blocks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(ImageBlock, CategoryBlock), 
    get_items&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_image_files, 
    splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomSplitter(valid_pct&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;),
    get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;parent_label,
    item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create the dataloaders,path is the image path,it validate and train the dataloader&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# create the dataloaders,path is the image path,it validate and train the dataloader&lt;/span&gt;
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;see some item in the dataLoader&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;valid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can see ,at defaukt ,the fastai crop the image to the size 128&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/default.PNG&#34; alt=&#34;default&#34;&gt;&lt;/p&gt;
&lt;p&gt;we also can Squish the image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, ResizeMethod&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Squish))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;valid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/squizh.PNG&#34; alt=&#34;Squish&#34;&gt;&lt;/p&gt;
&lt;p&gt;or pad them&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, ResizeMethod&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Pad, pad_mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;zeros&amp;#39;&lt;/span&gt;))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;valid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pad.PNG&#34; alt=&#34;pad&#34;&gt;&lt;/p&gt;
&lt;p&gt;or randomly choose a part to crop the image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# unique=True mean use the sam picture&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# RandomResizedCrop  Crop  different part of the same picture,we can have more data to train,Data Augmentation&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# min_scale=0.3, select the % of the picture to crop,30%&lt;/span&gt;
bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomResizedCrop(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, min_scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, unique&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/RandomResizedCrop.PNG&#34; alt=&#34;pad&#34;&gt;&lt;/p&gt;
&lt;p&gt;another way to Data Augmentation(資料增強,種通過讓有限的資料產生更多的等價資料來人工擴充套件訓練資料集的技術),not crop,just example, to show  rotation, flipping, perspective warping, brightness changes and contrast changes&lt;br&gt;
batch_tfms apply the aug_transforms to batch ,not only items&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# another way to Data Augmentation,not crop,just example, to show  rotation, flipping, perspective warping, brightness changes and contrast changes&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# production use crop + aug_transforms&lt;/span&gt;
bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;), batch_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;aug_transforms(mult&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, unique&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/aug_t.PNG&#34; alt=&#34;pad&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;start-create-the-model&#34;&gt;Start create the model&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataBlock(
    blocks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(ImageBlock, CategoryBlock), 
    get_items&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_image_files, 
    splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomSplitter(valid_pct&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;),
    get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;parent_label,
    item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomResizedCrop(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, min_scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;),
    batch_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;aug_transforms())

dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)


learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet18, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;interp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ClassificationInterpretation&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_learner(learn)
interp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_confusion_matrix()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/square.PNG&#34; alt=&#34;square&#34;&gt;
this is the result box,,to see how many item is worng predict
we can see some grizzly,put in black&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;interp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_top_losses(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# we can see some grizzly,put in black&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# the number loss,mean the predict is right, but not conficdent,or the answer is wrong,this number will high&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the number loss,mean the predict is right, but not conficdent,or the answer is wrong,this number will high
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pl.PNG&#34; alt=&#34;pi&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ues the fastai GUI to clean the data,remove or re tag&lt;/span&gt;
cleaner &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ImageClassifierCleaner(learn)
cleaner
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/gui.png&#34; alt=&#34;gui&#34;&gt;&lt;/p&gt;
&lt;p&gt;after change the action to the pait(bear colormgroup(valid,train))&lt;br&gt;
run below to move and dlete the items&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete())
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;change())
&lt;span style=&#34;color:#75715e&#34;&gt;# delete the delete marked photo&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete(): cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fns[idx]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unlink()
&lt;span style=&#34;color:#75715e&#34;&gt;# move the photo to the right folder /tag&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx,cat &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;change(): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
    shutil&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;move(str(cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fns[idx]), path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;cat)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt;:
    cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fns[idx]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unlink()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;after this process, we can retrain again,run&lt;/p&gt;
&lt;h2 id=&#34;use-it&#34;&gt;Use it&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;uploader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; widgets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FileUpload()
uploader
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; PILImage&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create(uploader&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
bear_type,_,probs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(img)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bear type: {bear_type}.&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Probability : {probs[1]}&amp;#34;&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Notes 1.2</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-1.2/</link>
      <pubDate>Tue, 20 Apr 2021 16:00:14 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-1.2/</guid>
      <description>MyCode Source Code
Detail explain of the mechaine learning    Term Meaning     Term Meaning   Label The data that we&amp;rsquo;re trying to predict, such as &amp;ldquo;dog&amp;rdquo; or &amp;ldquo;cat&amp;rdquo;   Architecture The template of the model that we&amp;rsquo;re trying to fit; the actual mathematical function that we&amp;rsquo;re passing the input data and parameters to   Model The combination of the architecture with a particular set of parameters   Parameters The values in the model that change what task it can do, and are updated through model training   Fit Update the parameters of the model such that the predictions of the model using the input data match the target labels   Train A synonym for fit   Pretrained model A model that has already been trained, generally using a large dataset, and will be fine-tuned   Fine-tune Update a pretrained model for a different task   Epoch One complete pass through the input data   Loss A measure of how good the model is, chosen to drive training via SGD   Metric A measurement of how good the model is, using the validation set, chosen for human consumption   Validation set A set of data held out from training, used only for measuring how good the model is   Training set The data used for fitting the model; does not include any data from the validation set   Overfitting Training a model in such a way that it remembers specific features of the input data, rather than generalizing well to data not seen during training   CNN Convolutional neural network; a type of neural network that works particularly well for computer vision tasks    </description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wp8oHYx2MJq0T9n93DYVi4533QbsK8UT?usp=sharing&#34;&gt;MyCode&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;detail-explain-of-the-mechaine-learning&#34;&gt;Detail explain of the mechaine learning&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/train_loop.PNG&#34; alt=&#34;tl&#34;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Term&lt;/td&gt;
&lt;td&gt;Meaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Label&lt;/td&gt;
&lt;td&gt;The data that we&amp;rsquo;re trying to predict, such as &amp;ldquo;dog&amp;rdquo; or &amp;ldquo;cat&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Architecture&lt;/td&gt;
&lt;td&gt;The &lt;em&gt;template&lt;/em&gt; of the model that we&amp;rsquo;re trying to fit; the actual mathematical function that we&amp;rsquo;re passing the input data and parameters to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Model&lt;/td&gt;
&lt;td&gt;The combination of the architecture with a particular set of parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Parameters&lt;/td&gt;
&lt;td&gt;The values in the model that change what task it can do, and are updated through model training&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fit&lt;/td&gt;
&lt;td&gt;Update the parameters of the model such that the predictions of the model using the input data match the target labels&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Train&lt;/td&gt;
&lt;td&gt;A synonym for &lt;em&gt;fit&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pretrained model&lt;/td&gt;
&lt;td&gt;A model that has already been trained, generally using a large dataset, and will be fine-tuned&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fine-tune&lt;/td&gt;
&lt;td&gt;Update a pretrained model for a different task&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Epoch&lt;/td&gt;
&lt;td&gt;One complete pass through the input data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Loss&lt;/td&gt;
&lt;td&gt;A measure of how good the model is, chosen to drive training via SGD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Metric&lt;/td&gt;
&lt;td&gt;A measurement of how good the model is, using the validation set, chosen for human consumption&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Validation set&lt;/td&gt;
&lt;td&gt;A set of data held out from training, used only for measuring how good the model is&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training set&lt;/td&gt;
&lt;td&gt;The data used for fitting the model; does not include any data from the validation set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Overfitting&lt;/td&gt;
&lt;td&gt;Training a model in such a way that it &lt;em&gt;remembers&lt;/em&gt; specific features of the input data, rather than generalizing well to data not seen during training&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;Convolutional neural network; a type of neural network that works particularly well for computer vision tasks&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
    </item>
    
    <item>
      <title>Ai Notes 1.1</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-1.1/</link>
      <pubDate>Tue, 20 Apr 2021 14:46:19 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-1.1/</guid>
      <description>MyCode Source Code
Your First Model on fastai framework !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * #id first_training #caption Results from the first training from fastai.vision.all import * # get the cat do images path path = untar_data(URLs.PETS)/&amp;#39;images&amp;#39; def is_cat(x): return x[0].isupper() # because we are stuing the image so use ImageDataLoaders # valid_pct=0.2 meankeep 20 % photo image not use on training,for testing # label_func=is_cat mean get the tag of the imag,to detect # is the photos is a cat or dog,if photo name start by Upper case # ,than tha pohot is a cat # item_tfms=Resize(224) resize the photo to 224 dls = ImageDataLoaders.</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1wp8oHYx2MJq0T9n93DYVi4533QbsK8UT?usp=sharing&#34;&gt;MyCode&lt;/a&gt;
&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/01_intro.ipynb&#34;&gt;Source Code&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;your-first-model-on-fastai-framework&#34;&gt;Your First Model on fastai framework&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;Uqq fastbook
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fastbook
fastbook&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup_book()



&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastbook &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;


&lt;span style=&#34;color:#75715e&#34;&gt;#id first_training&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#caption Results from the first training&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.all &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# get the cat do images path&lt;/span&gt;
path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; untar_data(URLs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;PETS)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;images&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_cat&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isupper()
&lt;span style=&#34;color:#75715e&#34;&gt;# because we are stuing the image so use ImageDataLoaders&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# valid_pct=0.2 meankeep 20 % photo image not use on training,for testing&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# label_func=is_cat mean get the tag of the imag,to detect&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# is the photos is a cat or dog,if photo name start by Upper case&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# ,than tha pohot is a cat&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# item_tfms=Resize(224) resize the photo to 224&lt;/span&gt;
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ImageDataLoaders&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_name_func(
    path, get_image_files(path), valid_pct&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;,
    label_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;is_cat, item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;))

learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)


&lt;span style=&#34;color:#75715e&#34;&gt;# use to create a upload button&lt;/span&gt;

uploader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; widgets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FileUpload()
uploader


&lt;span style=&#34;color:#75715e&#34;&gt;# use the model to detect is your photo is a cat&lt;/span&gt;
img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; PILImage&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create(uploader&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
is_cat,_,probs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(img)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Is this a cat?: {is_cat}.&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Probability it&amp;#39;s a cat: {probs[1].item():.6f}&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Notes 0</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-0/</link>
      <pubDate>Mon, 19 Apr 2021 15:26:46 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-0/</guid>
      <description>一向想學下 Machine laerning,見到有本幾好的書 寫給程式設計師的深度學習：使用fastai和PyTorch，決定一邊睇書，一邊寫有關本書的Notes，希望能夠完整完成.</description>
      <content>&lt;p&gt;一向想學下 Machine laerning,見到有本幾好的書 &lt;a href=&#34;https://www.books.com.tw/products/0010886129?sloc=main&#34;&gt;寫給程式設計師的深度學習：使用fastai和PyTorch&lt;/a&gt;，決定一邊睇書，一邊寫有關本書的Notes，希望能夠完整完成.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
