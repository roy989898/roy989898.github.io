<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>epoch number on Terminal</title>
    <link>https://roy989898.github.io/tags/epoch-number/</link>
    <description>Recent content in epoch number on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 07 May 2021 14:42:55 +0800</lastBuildDate><atom:link href="https://roy989898.github.io/tags/epoch-number/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Tutorial 5.4 Image Classification &gt;2 types Improving Our Model</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-5.4/</link>
      <pubDate>Fri, 07 May 2021 14:42:55 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-5.4/</guid>
      <description>My Code 
Source Code 
Improving Our Model we will explain a little bit more about transfer learning and how to fine-tune our pretrained model as best as possible, without breaking the pretrained weights.
The Learning Rate Finder if lr too small, many epochs to train our model,waste time,and every time we do a complete pass through the data, we give our model a chance to memorize it.also remember the validate data</description>
      <content>&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/drive/1Rqum2194iz5nXH26PPoBMpKM71wQ4eYI?usp=sharing&#34;&gt;My Code
&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://colab.research.google.com/github/fastai/fastbook/blob/master/05_pet_breeds.ipynb#scrollTo=YOTrrdP7BuWd&#34;&gt;Source Code
&lt;/a&gt;&lt;/p&gt;
&lt;h1 id=&#34;improving-our-model&#34;&gt;Improving Our Model&lt;/h1&gt;
&lt;p&gt;we will explain a little bit more about transfer learning and how to fine-tune our pretrained model as best as possible, without breaking the pretrained weights.&lt;/p&gt;
&lt;h2 id=&#34;the-learning-rate-finder&#34;&gt;The Learning Rate Finder&lt;/h2&gt;
&lt;p&gt;if lr too small, many epochs to train our model,waste time,and every time we do a complete pass through the data, we give our model a chance to memorize it.also remember the validate data&lt;/p&gt;
&lt;p&gt;set it very high frist,&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, base_lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 2.568456 6.223738 0.496617 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 3.971391 2.541565 0.698917 01:12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the way to find the best LR:&lt;br&gt;
simple concept: use a very LR start,train a one mini-batch,&amp;gt; increase the LR by some percentage (e.g., doubling it each time),than repeat,until the loss gets worse, instead of better,This is the point where we know we have gone too far. We then select a learning rate a bit lower than this point. Our advice is to pick either:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;One order of magnitude less than where the minimum loss was achieved (i.e., the minimum divided by 10)&lt;/li&gt;
&lt;li&gt;The last point where the loss was clearly decreasing&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;fastai will help you to find this 2 point Both these rules usually give around the same value&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# default start LR is 1e-3=10^-3&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
lr_min,lr_steep &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr_find()

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Minimum/10: {lr_min:.2e}, steepest point: {lr_steep:.2e}&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# Minimum/10: 1.00e-02, steepest point: 2.51e-03&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;code&gt;1e-3 mean 10^-3&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/lrf.PNG&#34; alt=&#34;sgd_LRFstep&#34;&gt;
for the picture,we can seeif LR &amp;gt; 1e-1,the loss increase,but 1-e-1 too high,becasu already leave the loss decrease phase&lt;br&gt;
we use 3e-3 at here(follow the book),we still can use 8.32e-03 and 2.09e-03&lt;/p&gt;
&lt;h2 id=&#34;unfreezing-and-transfer-learning&#34;&gt;Unfreezing and Transfer Learning&lt;/h2&gt;
&lt;p&gt;what is transfer learning??? We saw that the basic idea is that a pretrained model, trained potentially on millions of data points (such as ImageNet), is fine-tuned for some other task.&lt;/p&gt;
&lt;p&gt;Our challenge when fine-tuning is to replace the random weights in our added linear layers with weights that correctly achieve our desired task (classifying pet breeds) without breaking the carefully pretrained weights and the other layers. There is actually a very simple trick to allow this to happen: tell the optimizer to only update the weights in those randomly added final layers. Don&amp;rsquo;t change the weights in the rest of the neural network at all. This is called freezing those pretrained layers.&lt;/p&gt;
&lt;p&gt;進行微調時，我們面臨的挑戰是在不破壞經過精心訓練的砝碼和其他層的情況下，用能夠正確完成我們期望任務（對寵物品種進行分類）的砝碼替換添加的線性層中的隨機砝碼。 實際上，有一個很簡單的技巧可以使這種情況發生：告訴優化器僅更新那些隨機添加的最終層中的權重。 完全不要更改神經網絡其餘部分的權重。 這稱為凍結那些預訓練的層。&lt;/p&gt;
&lt;p&gt;When we create a model from a pretrained network fastai automatically freezes all of the pretrained layers for us. When we call the fine_tune method fastai does two things:&lt;/p&gt;
&lt;p&gt;Trains the randomly added layers for one epoch, with all other layers frozen.&lt;br&gt;
Unfreezes all of the layers, and trains them all for the number of epochs requested&lt;/p&gt;
&lt;p&gt;try implement&lt;/p&gt;
&lt;p&gt;First of all we will train the randomly added layers for three epochs, using fit_one_cycle
fit_one_cycle is the suggested way to train models without using fine_tune. We&amp;rsquo;ll see why later in the book; in short, what fit_one_cycle does is to start training at a low learning rate, gradually increase it for the first section of training, and then gradually decrease it again for the last section of training.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# here only train the randomly added layers&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3e-3&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 1.149184 0.357759 0.112314 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.516031 0.269226 0.082544 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.307812 0.237481 0.071719 01:07&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# unfreeze the model&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfreeze()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;run lr_find again to find the LR, because having more layers to train, and weights that have already been trained for three epochs, means our previously found learning rate isn&amp;rsquo;t appropriate any more:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr_find()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/lr2.PNG&#34; alt=&#34;lr2&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;important&lt;/strong&gt;!!!!!!we should not use the lr_steep at here,because our model has been trained already. Here we have a somewhat flat area before a sharp increase, and we should take a point well before that sharp increase—for instance, 1e-5. The point with the maximum gradient isn&amp;rsquo;t what we look for here and should be ignored.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#  train all layer&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, lr_max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.245116 0.232571 0.071042 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.244692 0.223327 0.069689 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.214002 0.217773 0.068336 01:13&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.194007 0.214042 0.066306 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.180974 0.212813 0.067659 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.183777 0.215303 0.064953 01:12&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;The deepest layers of model might not need as high a learning rate as the last ones, so we should probably use different learning rates for those—this is known as using discriminative learning rates.&lt;/p&gt;
&lt;h2 id=&#34;discriminative-learning-rates&#34;&gt;Discriminative Learning Rates&lt;/h2&gt;
&lt;p&gt;each level can use different LR,at low level,we can use the lower LR,because they already trained,they have pretrained weights,useful for nearly any task,no need to change so much,at higher level,the pretrained weights is for   much more complex concepts, like &amp;ldquo;eye&amp;rdquo; and &amp;ldquo;sunset,&amp;rdquo; which might not be useful in your task at all,use a faster lr to train them&lt;br&gt;
main point:&lt;br&gt;
use a lower learning rate for the early layers of the neural network, and a higher learning rate for the later layers (and especially the randomly added layers)&lt;/p&gt;
&lt;h3 id=&#34;basic-for-the-slice&#34;&gt;Basic for the slice&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;arr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;list(range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;))
myslice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
arr[myslice]  
&lt;span style=&#34;color:#75715e&#34;&gt;# [0, 1, 2, 3, 4]&lt;/span&gt;
myslice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;)
arr[myslice]  
&lt;span style=&#34;color:#75715e&#34;&gt;# [1, 2, 3, 4]&lt;/span&gt;
myslice &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)
arr[myslice]  
&lt;span style=&#34;color:#75715e&#34;&gt;# [0, 2, 4]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# lr_max=slice(1e-6,1e-4)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# mean lowest LR is 1e-6,the other layers will scale up to 1e-4&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;3e-3&lt;/span&gt;)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unfreeze()
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit_one_cycle(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;, lr_max&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;slice(&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1e-4&lt;/span&gt;))

&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 1.131566 0.361410 0.111637 01:06&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.544027 0.264487 0.086604 01:06&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.316729 0.248465 0.083221 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.256258 0.242825 0.085250 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.242427 0.238632 0.080514 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.233899 0.233360 0.083221 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.217217 0.217414 0.075778 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.189038 0.217263 0.070365 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.181181 0.207588 0.069012 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 6 0.158933 0.208005 0.070365 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 7 0.148363 0.205170 0.068336 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 8 0.135392 0.203676 0.069012 01:12&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 9 0.122220 0.203666 0.065629 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 10 0.130100 0.200204 0.065629 01:11&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 11 0.119578 0.205134 0.069689 01:11&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now the fine-tuning is working great!&lt;/p&gt;
&lt;p&gt;we can see the loss chnage&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# plot the loss change&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;recorder&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_loss()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/p_loss.PNG&#34; alt=&#34;pLoss&#34;&gt;&lt;/p&gt;
&lt;p&gt;the training loss keeps getting better and better. But notice that eventually the validation loss improvement slows, and sometimes even gets worse! This is the point at which the model is starting to over fit. In particular, the model is becoming overconfident of its predictions. But this does not mean that it is getting less accurate, necessarily. Take a look at the table of training results per epoch, and you will often see that the accuracy continues improving, even as the validation loss gets worse. In the end what matters is your accuracy, or more generally your chosen &lt;strong&gt;metrics&lt;/strong&gt;, &lt;strong&gt;not the loss&lt;/strong&gt;. The loss is just the function we&amp;rsquo;ve given the computer to help us to optimize.&lt;/p&gt;
&lt;h2 id=&#34;number-of-epochs&#34;&gt;Number of Epochs&lt;/h2&gt;
&lt;p&gt;choose the number of epoch that you willing to wait,than watch the above picture, if you see that the metric are still getting better even in your final epochs, then you know that you have not trained for too long.&lt;/p&gt;
&lt;h2 id=&#34;deeper-architectures&#34;&gt;Deeper Architectures&lt;/h2&gt;
&lt;p&gt;a model with more parameters(depper) can model your data more accurately.&lt;/p&gt;
&lt;p&gt;This is why, in practice, architectures tend to come in a small number of variants. For instance, the ResNet architecture that we are using in this chapter comes in variants with 18, 34, 50, 101, and 152 layer, pretrained on ImageNet. A larger (more layers and parameters; sometimes described as the &amp;ldquo;capacity&amp;rdquo; of a model) version of a ResNet will always be able to give us a better training loss, but it can suffer more from overfitting, because it has more parameters to overfit with.&lt;/p&gt;
&lt;p&gt;the other problem is,depper, will use more GPU RAM,an duse more time&lt;/p&gt;
&lt;p&gt;nearly all current NVIDIA GPUs support a special feature called&lt;strong&gt;tensor cores&lt;/strong&gt; that can dramatically speed up neural network training, by 2-3x. They also require a lot less GPU memory. To enable this feature in fastai, just add to_fp16() after your Learner creation (you also need to import the module).&lt;/p&gt;
&lt;p&gt;You can&amp;rsquo;t really know ahead of time what the best architecture for your particular problem is—you need to try training some. So let&amp;rsquo;s try a ResNet-50 now with mixed precision:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;  &lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.callback.fp16 &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet50, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_fp16()
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;, freeze_epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 1.279959 0.309704 0.102842 01:05&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.590101 0.312733 0.101489 01:05&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.447781 0.294772 0.088633 01:05&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss error_rate time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.274948 0.280899 0.085250 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.299947 0.331522 0.089310 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.251186 0.292205 0.084574 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.159606 0.241466 0.068336 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.083857 0.210775 0.060893 01:07&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.054267 0.210627 0.060893 01:06&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;try small model first,than try big model&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
