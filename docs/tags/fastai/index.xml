<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>fastai on Terminal</title>
    <link>https://roy989898.github.io/tags/fastai/</link>
    <description>Recent content in fastai on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 28 Apr 2021 12:44:41 +0800</lastBuildDate><atom:link href="https://roy989898.github.io/tags/fastai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Tutorial 4.8 The MNIST Loss Function</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.8/</link>
      <pubDate>Wed, 28 Apr 2021 12:44:41 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.8/</guid>
      <description>MNIST Loss Function some basic python zip # zip a=[1,2,3,4] b=[5,6,7,8] list(zip(a,b)) # [(1, 5), (2, 6), (3, 7), (4, 8)] create array [1]*4 # [1, 1, 1, 1] tensor([1]*4 + [0]*3) # tensor([1, 1, 1, 1, 0, 0, 0]) Some basic pytorch functions horizontal tensor to vertical tensors tensor([1]*4 + [0]*3) # tensor([1, 1, 1, 1, 0, 0, 0]) tensor([1]*4 + [0]*3).unsqueeze(1) # tensor([[1], # [1], # [1], # [1], # [0], # [0], # [0]]) torch.</description>
      <content>&lt;h1 id=&#34;_mnist-loss-function_&#34;&gt;&lt;em&gt;MNIST Loss Function&lt;/em&gt;&lt;/h1&gt;
&lt;h2 id=&#34;some-basic-python&#34;&gt;some basic python&lt;/h2&gt;
&lt;h3 id=&#34;zip&#34;&gt;zip&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# zip&lt;/span&gt;
a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
b&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;7&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;]
list(zip(a,b))
&lt;span style=&#34;color:#75715e&#34;&gt;# [(1, 5), (2, 6), (3, 7), (4, 8)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;create-array&#34;&gt;create array&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# [1, 1, 1, 1]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([1, 1, 1, 1, 0, 0, 0])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;some-basic-pytorch-functions&#34;&gt;Some basic pytorch functions&lt;/h2&gt;
&lt;h3 id=&#34;horizontal-tensor-to-vertical-tensors&#34;&gt;horizontal tensor to vertical tensors&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([1, 1, 1, 1, 0, 0, 0])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [0]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;torchcat&#34;&gt;torch.cat&lt;/h3&gt;
&lt;p&gt;connect two tensors together
&lt;a href=&#34;https://blog.csdn.net/qq_39709535/article/details/80803003&#34;&gt;https://blog.csdn.net/qq_39709535/article/details/80803003&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;A&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;) &lt;span style=&#34;color:#75715e&#34;&gt;#2x3的张量（矩阵）                                     &lt;/span&gt;
A
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1., 1., 1.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1., 1., 1.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1., 1., 1.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [1., 1., 1.]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;B&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ones(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
B
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[2., 2., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2., 2., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2., 2., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2., 2., 2.]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;C&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat([A,B])
C&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([8, 3])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;tensorview&#34;&gt;Tensor.view&lt;/h3&gt;
&lt;p&gt;PyTorch allows a tensor to be a View of an existing tensor. View tensor shares the same underlying data with its base tensor.&lt;/p&gt;
&lt;p&gt;把原先tensor中的數據按照行優先的順序排成一個一維的數據（這裡應該是因為要求地址是連續存儲的），然後按照參數組合成其他維度的tensor。比如說是不管你原先的數據是[ [[1,2,3],[4,5,6]]]還是[1,2,3,4,5,6]，因為它們排成一維向量都是6個元素，所以只要view後面的參數一致，得到的結果都是一樣的。比如，
example&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;a&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Tensor([[[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;],[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]])
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(a&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))


&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1., 2.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [3., 4.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [5., 6.]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;mnist-loss-function&#34;&gt;MNIST Loss Function&lt;/h2&gt;
&lt;h2 id=&#34;connect-the-photo&#34;&gt;connect the photo&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([6131, 28, 28])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;stacked_sevens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([6265, 28, 28])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;train_x &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;cat([stacked_threes, stacked_sevens])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;view(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;)
train_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([12396, 784])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the above acode,we first connect the stacked_threes(each pixel present by 0-1 number) and
for each picture , orginal is respenct by a 2d tensor,(28&lt;em&gt;28),turn to 1d tensor 784
&lt;code&gt;view(-1, 28*28)&lt;/code&gt; mean 28&lt;/em&gt;28 column,-1 mean not specific the row number,just make it can fit the content,becasue we have
&lt;code&gt;6131+6265=12396&lt;/code&gt;&lt;/p&gt;
&lt;h2 id=&#34;add-the-tag-for-each-photo&#34;&gt;add the tag for each photo&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# assign the tag to each image&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# We need a label for each image. We&amp;#39;ll use `1` for 3s and `0` for 7s:&lt;/span&gt;
train_y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;len(threes) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; [&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;len(sevens))&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unsqueeze(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
train_x&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,train_y&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([12396, 784]), torch.Size([12396, 1]))&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# train_X,12396 images,each image total 784 pixels&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#train_y,12396 tag,because eachpicture 1 tag,1 tag inf in each tag&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.7 An End-to-End SGD Example</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.7/</link>
      <pubDate>Wed, 28 Apr 2021 11:57:33 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.7/</guid>
      <description>An End-to-End SGD Example we want to find the smallest value
Some useful function craete a 0-19 torch array
time = torch.arange(0,20).float(); time # tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) create randome number
# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。 num=20 t=torch.randn(num) time_f = torch.arange(0,num).float(); time plt.scatter(time_f,t); t simulate a car speed
# simulate a car speed # torch.randn(20)*3 is some random noise time = torch.</description>
      <content>&lt;h1 id=&#34;_an-end-to-end-sgd-example_&#34;&gt;&lt;em&gt;An End-to-End SGD Example&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;we want to find the smallest value&lt;/p&gt;
&lt;h2 id=&#34;some-useful-function&#34;&gt;Some useful function&lt;/h2&gt;
&lt;p&gt;craete a 0-19 torch array&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create randome number&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。&lt;/span&gt;
num&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;
t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(num)
time_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,num)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time_f,t);
t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rt.PNG&#34; alt=&#34;rt&#34;&gt;&lt;/p&gt;
&lt;p&gt;simulate a car speed&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# simulate a car speed&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.randn(20)*3 is some random noise&lt;/span&gt;
time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
speed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.75&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(time&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9.5&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time,speed);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/car_speed.PNG&#34; alt=&#34;car_speed&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;use-sgd-to-find-the-smallest-value-for-the-loss&#34;&gt;use SGD to find the smallest value for the loss&lt;/h2&gt;
&lt;h3 id=&#34;step-0-gues-the-functions&#34;&gt;Step 0 gues the functions&lt;/h3&gt;
&lt;p&gt;we nedd to find the a,b,c that make the loss is the lowset
(time**2)+(b*time)+c&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(t, params):
    a,b,c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; params
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(t&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (b&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;t) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-01-define-the-meaning-of-best&#34;&gt;Step 0.1 define the meaning of best&lt;/h3&gt;
&lt;p&gt;we use a loss function to define the best, which will return a value based on a prediction and a target, where lower values of the function correspond to &amp;ldquo;better&amp;rdquo; predictions. For continuous data, it&amp;rsquo;s common to use mean squared error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mse&lt;/span&gt;(preds, targets): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ((preds&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;targets)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-1-set-the-apramter-as-a-randome-value&#34;&gt;Step 1 set the apramter as a randome value&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;params&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None
params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()
orig_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;clone()
params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-2-calculate-the-predict&#34;&gt;Step 2 calculate the predict&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time, params)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;show_preds&lt;/span&gt;(preds, ax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ax &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None: ax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots()[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time, speed)
    &lt;span style=&#34;color:#75715e&#34;&gt;# to_npconvert tensor to numpy arry&lt;/span&gt;
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time, to_np(preds), color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;)
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_ylim(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)

show_preds(preds)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pred1.PNG&#34; alt=&#34;pred1&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-3-calculate-the-losses&#34;&gt;Step 3 calculate the losses&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse(preds, speed)
loss
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(25.1871, grad_fn=&amp;lt;SqrtBackward&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-4--know-the-gradients&#34;&gt;Step 4  know the gradients&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# the a b c gradients&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([-3.1634, -0.2709, -0.3931])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-5--step-the-weights&#34;&gt;Step 5  Step the weights&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# assign the chnaged parameter to the params&lt;/span&gt;
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; lr &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s see if the loss has improved:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s see if the loss has improved:&lt;/span&gt;
preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time,params)
mse(preds, speed)
show_preds(preds)
&lt;span style=&#34;color:#75715e&#34;&gt;# improve a little bit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/ip.PNG&#34; alt=&#34;pred1&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-6--repeat-it&#34;&gt;step 6 , repeat it&lt;/h3&gt;
&lt;h1 id=&#34;we-use-a-for-loop-to-do-multi-time&#34;&gt;we use a for loop to do multi time&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;apply_step&lt;/span&gt;(params, prn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True):
    preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time, params)
    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse(preds, speed)
    loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
    params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; lr &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data
    params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; prn: &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item())
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; preds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;): apply_step(params)

&lt;span style=&#34;color:#75715e&#34;&gt;# 160.42279052734375&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 160.14772033691406&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.87269592285156&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.59768676757812&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.3227081298828&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.04774475097656&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.7728271484375&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.4979248046875&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.22305297851562&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 157.9481964111328&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;_,axs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;))
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ax &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; axs: show_preds(apply_step(params, False), ax)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tight_layout()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/4p.PNG&#34; alt=&#34;4p&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step7-stop&#34;&gt;Step7 stop&lt;/h3&gt;
&lt;p&gt;we do 10 round ,than stop**&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.6 Stepping With a Learning Rate</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.6/</link>
      <pubDate>Wed, 28 Apr 2021 11:30:02 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.6/</guid>
      <description>Stepping With a Learning Rate when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder).</description>
      <content>&lt;h1 id=&#34;_stepping-with-a-learning-rate_&#34;&gt;&lt;em&gt;Stepping With a Learning Rate&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;when we get the gradient,we cau use it calculate the new paramter . Nearly all approaches start with the basic idea of multiplying the gradient by some small number, called the learning rate (LR). The learning rate is often a number between 0.001 and 0.1, although it could be anything Often, people select a learning rate just by trying a few, and finding which results in the best model after training (we&amp;rsquo;ll show you a better approach later in this book, called the learning rate finder). Once you&amp;rsquo;ve picked a learning rate, you can adjust your parameters using this simple function:
w -= gradient(w) * lr&lt;br&gt;
This is known as &lt;em&gt;stepping&lt;/em&gt; your parameters, using an &lt;em&gt;optimizer step&lt;/em&gt;.
當我們得到梯度時，我們就用它來計算新的參數。 幾乎所有方法都始於將梯度乘以一個稱為學習率（LR）的小數的基本思想。 學習率通常是0.001到0.1之間的數字，儘管可以是任意數。通常，人們僅通過嘗試一些就可以選擇學習率，並在訓練後發現哪種模式可以得到最佳模型（我們將在稍後向您展示一種更好的方法 在這本書中，稱為學習率查找器）。 選擇學習速度後，您可以使用以下簡單功能調整參數：
w -= gradient(w) * lr&lt;br&gt;
使用“優化步”，這稱為“步進”你的參數。&lt;/p&gt;
&lt;p&gt;if your Lr too small,maybe too slow,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/step_small.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;if LR too big,it can actually result in the loss getting worse,&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/strp_big1.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/steo_big2.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.5 Gredient</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.5/</link>
      <pubDate>Wed, 28 Apr 2021 11:15:32 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.5/</guid>
      <description>Gradients explain for the Gredient 
calculate for the gradient in program def f(x): return x**2 the function in 4.4
# select a tensor to calculate the grad xt = tensor(3.).requires_grad_() xt yt = f(xt) yt # calculate the gradients yt.backward() # see the grad,answer is 6 xt.grad # tensor(6.) another example xt = tensor([3.,4.,10.]).requires_grad_() def f(x): return (x**2).sum() yt = f(xt) yt.backward() xt.grad </description>
      <content>&lt;h1 id=&#34;_gradients_&#34;&gt;&lt;em&gt;Gradients&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;&lt;a href=&#34;https://www.khanacademy.org/math/differential-calculus/dc-diff-intro&#34;&gt;explain for the Gredient
&lt;/a&gt;&lt;/p&gt;
&lt;h2 id=&#34;calculate-for-the-gradient-in-program&#34;&gt;calculate for the gradient in program&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.4/&#34; title=&#34;the function in 4.4&#34;&gt;the function in 4.4&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt; &lt;span style=&#34;color:#75715e&#34;&gt;# select a tensor to calculate the grad&lt;/span&gt;
xt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor(&lt;span style=&#34;color:#ae81ff&#34;&gt;3.&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()
xt
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;yt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(xt)
yt

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# calculate the gradients&lt;/span&gt;
yt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# see the grad,answer is 6&lt;/span&gt;
xt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(6.)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;another-example&#34;&gt;another example&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
xt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;3.&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4.&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;10.&lt;/span&gt;])&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sum()

yt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(xt)

yt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
xt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.4 Stochastic Gradient Descent 隨機梯度下降 (SGD)</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.4/</link>
      <pubDate>Tue, 27 Apr 2021 19:56:41 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.4/</guid>
      <description>SGD Instead of trying to find the similarity between an image and an &amp;ldquo;ideal image,&amp;rdquo; we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8.</description>
      <content>&lt;h1 id=&#34;_sgd_&#34;&gt;&lt;em&gt;SGD&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Instead of trying to find the similarity between an image and an &amp;ldquo;ideal image,&amp;rdquo; we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category—for instance the probability of being the number 8:&lt;/p&gt;
&lt;p&gt;與其嘗試查找圖像與“理想圖像”之間的相似性，不如查看每個單獨的像素並為每個像素提出一組權重，以使最高的權重與最有可能與之相關的那些像素相關聯。 對於特定類別為黑色。 例如，朝右下角移動的像素不太可能為7激活，因此對於7,像素應該具有較低的權重，但對於8,像素應該很容易被激活，因此對於8,像素應該具有較高的權重.這可以表示為每個可能類別的一個函數和一組權重值，例如，成為數字8的概率：&lt;br&gt;
&lt;code&gt;def pr_eight(x,w): return (x*w).sum()&lt;/code&gt;&lt;br&gt;
x is the image, represented as a vector—in other words, with all of the rows stacked up end to end into a single long line. And we are assuming that the weights are a vector w. If we have this function, then we just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.&lt;/p&gt;
&lt;p&gt;x是表示為矢量的圖像，換句話說，所有行首尾相連地排成一條長線。 並且我們假設權重是向量w。 如果我們具有此功能，那麼我們只需要一些方法來更新權重即可使它們更好一點。 通過這種方法，我們可以重複該步驟多次，使權重越來越好，直到權重達到我們所能達到的程度為止。&lt;/p&gt;
&lt;p&gt;want to find the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not. Searching for the best vector w is a way to search for the best function for recognising 8s.&lt;/p&gt;
&lt;p&gt;想要找到向量w的特定值，該值導致函數的結果對於那些實際上是8s的圖像來說較高，而對於那些不是8s的圖像來說較低。 搜索最佳向量w是搜索識別8s的最佳函數的一種方式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize the weights.初始化權重。&lt;/li&gt;
&lt;li&gt;For each image, use these weights to predict whether it appears to be a 3 or a 7.對於每個圖像，使用這些權重來預測它是3還是7。&lt;/li&gt;
&lt;li&gt;Based on these predictions, calculate how good the model is (its loss).根據這些預測，計算模型的好壞（損失）。&lt;/li&gt;
&lt;li&gt;Calculate the gradient, which measures for each weight, how changing that weight would change the loss.計算坡度，該坡度針對每個權重進行度量，更改該權重將如何改變損耗&lt;/li&gt;
&lt;li&gt;Step (that is, change) all the weights based on that calculation.根據該計算步進（即更改）所有權重。&lt;/li&gt;
&lt;li&gt;Go back to the step 2, and repeat the process.返回到步驟2，並重複該過程。&lt;/li&gt;
&lt;li&gt;Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&amp;rsquo;t want to wait any longer).重複進行直到您決定停止訓練過程為止（例如，因為模型足夠好或者您不想再等待了）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/sgd_step.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are many different ways to do each of these seven steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize:: initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category—but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.
將參數初始化為隨機值。 這聽起來可能令人驚訝。 當然，我們還可以做出其他選擇，例如將其初始化為該類別的像素被激活的次數的百分比-但由於我們已經知道我們有一個例程可以改善這些權重，因此事實證明，只是從隨機權重開始 效果很好。&lt;/li&gt;
&lt;li&gt;Loss:: when testing the effectiveness of any current weight assignment in terms of actual performance. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).
在實際性能方面測試任何當前重量分配的有效性時。 如果模型的性能良好，我們需要一些函數返回一個較小的數字（標準方法是將小的損失視為好，將大損失視為壞，儘管這只是一個慣例）。&lt;/li&gt;
&lt;li&gt;Step:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.
一種簡單的判斷重量是否應該增加還是減少的簡單方法就是嘗試：將重量增加一點，然後看看損失是增加還是減少。 找到正確的方向後，您可以再多一點，少一點地更改該金額，直到找到一個行之有效的金額。 但是，這很慢！ 就像我們將看到的那樣，微積分的神奇之處使我們能夠直接弄清楚改變每個權重的方向和大致幅度，而不必嘗試所有這些小的改變。 做到這一點的方法是通過計算梯度。 這只是性能優化，通過使用較慢的手動過程，我們也將獲得完全相同的結果。&lt;/li&gt;
&lt;li&gt;Stop:: Once we&amp;rsquo;ve decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.
一旦我們確定了訓練模型的時間（在前面的列表中給出了一些建議），我們就會應用該決定。 這就是應用該決定的地方。 對於我們的數字分類器，我們將繼續訓練直到模型的準確性開始變差或用完為止。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;simple-example-of-sgd&#34;&gt;simple example of SGD&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plot_function(f, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x**2&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/x2p.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;The sequence of steps we described earlier starts by picking some random value for a parameter, and calculating the value of the loss:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plot_function(f, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x**2&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;, f(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;), color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/2pr.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;if we increased or decreased our parameter by a little bit—the adjustment. This is simply the slope at a particular point:
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rs1.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve:
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rs2.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;we want to find the lower y/(Loss),lowest is good,we we replay to try different x, to find the lowest y. this method is slow,a better ,is The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.3 Metric</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.3/</link>
      <pubDate>Tue, 27 Apr 2021 18:18:28 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.3/</guid>
      <description>Computing Metrics Using Broadcasting Metric a metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is.
we want to calculate our metric over a validation set. This is so that we don&amp;rsquo;t inadvertently overfit—that is, train a model to work well only on our training data
指標是根據我們的模型預測和數據集中的正確標籤計算出的數字，目的是告訴我們我們的模型有多好。
我們要根據驗證集計算指標。 這樣一來，我們就不會無意間過度擬合-也就是說，訓練模型只能在訓練數據上有效地工作</description>
      <content>&lt;h1 id=&#34;_computing-metrics-using-broadcasting_&#34;&gt;&lt;em&gt;Computing Metrics Using Broadcasting&lt;/em&gt;&lt;/h1&gt;
&lt;h4 id=&#34;metric&#34;&gt;Metric&lt;/h4&gt;
&lt;p&gt;a metric is a number that is calculated based on the predictions of our model, and the correct labels in our dataset, in order to tell us how good our model is.&lt;br&gt;
we want to calculate our metric over a validation set. This is so that we don&amp;rsquo;t inadvertently overfit—that is, train a model to work well only on our training data&lt;br&gt;
指標是根據我們的模型預測和數據集中的正確標籤計算出的數字，目的是告訴我們我們的模型有多好。&lt;br&gt;
我們要根據驗證集計算指標。 這樣一來，我們就不會無意間過度擬合-也就是說，訓練模型只能在訓練數據上有效地工作&lt;/p&gt;
&lt;p&gt;get the data&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;valid_3_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack([tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) 
                            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;valid&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()])
valid_3_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; valid_3_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
valid_7_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack([tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) 
                            &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;valid&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;7&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()])
valid_7_tens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; valid_7_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
valid_3_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,valid_7_tens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape

&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;computing-metrics-using-broadcasting&#34;&gt;Computing Metrics Using Broadcasting&lt;/h2&gt;
&lt;p&gt;write a function that canculate the distance&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mnist_distance&lt;/span&gt;(a,b): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (a&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;b)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean((&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
mnist_distance(a_3, mean3)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(0.1114)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;for every image ,we do not need to write a loop ,we use Broadcasting&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
valid_3_dist &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mnist_distance(valid_3_tens, mean3)
valid_3_dist, valid_3_dist&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor([0.1329, 0.1555, 0.1107,  ..., 0.1359, 0.1526, 0.1126]),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  torch.Size([1010]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;它沒有抱怨形狀不匹配，而是將每個單個圖像的距離作為長度為1,010（我們的驗證集中的3的數量）的向量（即1級張量）返回&lt;/p&gt;
&lt;p&gt;當PyTorch嘗試在不同等級的兩個張量之間執行簡單的減法運算時，它將使用廣播。 也就是說，它將自動擴展具有較小等級的張量，使其具有與具有較大等級的張量相同的大小。 廣播是一項重要功能，可使張量代碼更易於編寫。&lt;/p&gt;
&lt;p&gt;Instead of complaining about shapes not matching, it returned the distance for every single image as a vector (i.e., a rank-1 tensor) of length 1,010 (the number of 3s in our validation set).
PyTorch, when it tries to perform a simple subtraction operation between two tensors of different ranks, will use broadcasting. That is, it will automatically expand the tensor with the smaller rank to have the same size as the one with the larger rank. Broadcasting is an important capability that makes tensor code much easier to write.&lt;/p&gt;
&lt;h4 id=&#34;more-brodcast-example&#34;&gt;More Brodcast example&lt;/h4&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# brodcast example&lt;/span&gt;
tensor([[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;],[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]]) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; tensor(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[2, 3, 4],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [2, 3, 4]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;(valid_3_tens&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;mean3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape

&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([1010, 28, 28])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;is_3&#34;&gt;is_3&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_3&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; mnist_distance(x,mean3) &lt;span style=&#34;color:#f92672&#34;&gt;&amp;lt;&lt;/span&gt; mnist_distance(x,mean7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;is_3(a_3), is_3(a_3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(True), tensor(1.))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;is_3(valid_3_tens)
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([True, True, True,  ..., True, True, True])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we can calculate the accuracy for each of the 3s and 7s by taking the average of that function for all 3s and its inverse for all 7s:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;accuracy_3s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;      is_3(valid_3_tens)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float() &lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
accuracy_7s &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; is_3(valid_7_tens)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()

accuracy_3s,accuracy_7s,(accuracy_3s&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;accuracy_7s)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.9168), tensor(0.9854), tensor(0.9511))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;over 90% accuracy on both 3s and 7s!!!!&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.2</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.2/</link>
      <pubDate>Tue, 27 Apr 2021 18:14:43 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.2/</guid>
      <description>NumPy Arrays and PyTorch Tensors they almost the same but NumPy Arrays not support GPU
data = [[1,2,3],[4,5,6]] arr = array (data) tns = tensor(data) arr # numpy # array([[1, 2, 3], # [4, 5, 6]]) tns # pytorch # tensor([[1, 2, 3], # [4, 5, 6]]) tns[1] # get index 1 # tensor([4, 5, 6]) tns[:,1] # all first axis,index 1 at ssecond axis # tensor([2, 5]) tns[1,1:3] # first axis :1,sendoc axis 1-3(exclude) # tensor([5, 6]) tns+1 # tensor([[2, 3, 4], # [5, 6, 7]]) tns*1.</description>
      <content>&lt;h1 id=&#34;_numpy-arrays-and-pytorch-tensors_&#34;&gt;&lt;em&gt;NumPy Arrays and PyTorch Tensors&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;they almost the same but  NumPy Arrays not support GPU&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;],[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;]]
arr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; array (data)
tns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor(data)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;arr  &lt;span style=&#34;color:#75715e&#34;&gt;# numpy&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# array([[1, 2, 3],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [4, 5, 6]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tns  &lt;span style=&#34;color:#75715e&#34;&gt;# pytorch&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1, 2, 3],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [4, 5, 6]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tns[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# get index 1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([4, 5, 6])&lt;/span&gt;

tns[:,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# all first axis,index 1 at ssecond axis&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([2, 5])&lt;/span&gt;


tns[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# first axis :1,sendoc axis 1-3(exclude)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([5, 6])&lt;/span&gt;

tns&lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[2, 3, 4],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [5, 6, 7]])&lt;/span&gt;

tns&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[1.5000, 3.0000, 4.5000],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [6.0000, 7.5000, 9.0000]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.1</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.1/</link>
      <pubDate>Tue, 27 Apr 2021 15:51:50 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.1/</guid>
      <description>Get the number sample !pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastai.vision.all import * from fastbook import * matplotlib.rc(&amp;#39;image&amp;#39;, cmap=&amp;#39;Greys&amp;#39;) path = untar_data(URLs.MNIST_SAMPLE) Path.BASE_PATH = path path.ls() [Path(&amp;lsquo;valid&amp;rsquo;),Path(&amp;lsquo;train&amp;rsquo;),Path(&amp;lsquo;labels.csv&amp;rsquo;)] train group use to train,valid group use to test
(path/&amp;#39;train&amp;#39;).ls() # [Path(&amp;#39;train/7&amp;#39;),Path(&amp;#39;train/3&amp;#39;)] threes = (path/&amp;#39;train&amp;#39;/&amp;#39;3&amp;#39;).ls().sorted() sevens = (path/&amp;#39;train&amp;#39;/&amp;#39;7&amp;#39;).ls().sorted() threes # (#6131) [Path(&amp;#39;train/3/10.png&amp;#39;),Path(&amp;#39;train/3/10000.png&amp;#39;),Path(&amp;#39;train/3/10011.png&amp;#39;),Path(&amp;#39;train/3/10031.png&amp;#39;),Path(&amp;#39;train/3/10034.png&amp;#39;),Path(&amp;#39;train/3/10042.png&amp;#39;),Path(&amp;#39;train/3/10052.png&amp;#39;),Path(&amp;#39;train/3/1007.png&amp;#39;),Path(&amp;#39;train/3/10074.png&amp;#39;),Path(&amp;#39;train/3/10091.png&amp;#39;)...] open a image to see
im3_path = threes[1]im3 = Image.open(im3_path)im3array(im3).shape # a 28 * 28 image # (28, 28) turn the image into a 2 d array show row 4:10(not include),column 4:10(not include)</description>
      <content>&lt;h1 id=&#34;_get-the-number-sample_&#34;&gt;&lt;em&gt;Get the number sample&lt;/em&gt;&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;Uqq fastbook
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fastbook
fastbook&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup_book()

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.all &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastbook &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

matplotlib&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;rc(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;image&amp;#39;&lt;/span&gt;, cmap&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Greys&amp;#39;&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; untar_data(URLs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;MNIST_SAMPLE)
Path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;BASE_PATH &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; path
path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;[Path(&amp;lsquo;valid&amp;rsquo;),Path(&amp;lsquo;train&amp;rsquo;),Path(&amp;lsquo;labels.csv&amp;rsquo;)]
train group use to train,valid group use to test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()
&lt;span style=&#34;color:#75715e&#34;&gt;# [Path(&amp;#39;train/7&amp;#39;),Path(&amp;#39;train/3&amp;#39;)]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;threes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;3&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sorted()
sevens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;train&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;7&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sorted()
threes
&lt;span style=&#34;color:#75715e&#34;&gt;# (#6131) [Path(&amp;#39;train/3/10.png&amp;#39;),Path(&amp;#39;train/3/10000.png&amp;#39;),Path(&amp;#39;train/3/10011.png&amp;#39;),Path(&amp;#39;train/3/10031.png&amp;#39;),Path(&amp;#39;train/3/10034.png&amp;#39;),Path(&amp;#39;train/3/10042.png&amp;#39;),Path(&amp;#39;train/3/10052.png&amp;#39;),Path(&amp;#39;train/3/1007.png&amp;#39;),Path(&amp;#39;train/3/10074.png&amp;#39;),Path(&amp;#39;train/3/10091.png&amp;#39;)...]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;open a image to see&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;im3_path = threes[1]
im3 = Image.open(im3_path)
im3
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/3.png&#34; alt=&#34;3&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;array(im3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# a 28 * 28 image&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# (28, 28)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;turn the image into a  2 d array
show row 4:10(not include),column 4:10(not include)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;array(im3)[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# array([[  0,   0,   0,   0,   0,   0],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,   0,   0,   0,   0,  29],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,   0,   0,  48, 166, 224],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,  93, 244, 249, 253, 187],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0, 107, 253, 253, 230,  48],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0,   3,  20,  20,  15,   0]], dtype=uint8)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;tensor is array use in pytorch&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;tensor(im3)[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;]
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;show the head of th 3,the colr is Grey,0,is white&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;im3_t &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; tensor(im3)
df &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; pd&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;DataFrame(im3_t[&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;:&lt;span style=&#34;color:#ae81ff&#34;&gt;22&lt;/span&gt;])
df&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;style&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_properties(&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;{&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;font-size&amp;#39;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;6pt&amp;#39;&lt;/span&gt;})&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;background_gradient(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Greys&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/3_head.PNG&#34; alt=&#34;3_h&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;_first-try-pixel-similarity-baseline_&#34;&gt;&lt;em&gt;First Try: Pixel Similarity Baseline&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;we fins the average pixel value of 3 and 7,so we can define the ideal 3 ,7,then the image compare to the ideal 3 7,to identify is 3 or 7&lt;/p&gt;
&lt;p&gt;因此，這是第一個想法：我們如何找到3s的每個像素的平均像素值，然後對7s進行相同的處理。 這將為我們提供兩個組平均值，定義我們可以稱之為“理想”的3和7。然後，將圖像分類為一個數字或另一個數字，我們將看到圖像與這兩個理想數字中的哪一個最相似。 當然，這似乎總比沒有好，因此它將成為一個良好的基準。&lt;/p&gt;
&lt;h3 id=&#34;what-is-a-baseline&#34;&gt;what is a baseline&lt;/h3&gt;
&lt;p&gt;A simple model which you are confident should perform reasonably well.&lt;br&gt;
您相信一個簡單的模型應該可以表現良好。 它應該很容易實現，也很容易測試，這樣您就可以測試每個改進的想法，並確保它們總是比基線更好。 不從合理的基准開始，很難知道您的超級幻想模型是否真的有用。 創建基準的一種好方法是執行我們在此處所做的工作：考慮一個簡單，易於實現的模型。 另一個好的方法是四處搜尋，以解決與您的問題相似的其他人，然後在您的數據集上下載並運行他們的代碼。 理想情況下，嘗試這兩個！&lt;/p&gt;
&lt;h2 id=&#34;step1&#34;&gt;step1&lt;/h2&gt;
&lt;p&gt;get the average of pixel values for each of our two groups
create a tensor containing all of our 3s stacked together&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(Image.open(o)) turn the image to a 2d array&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# this way for loop more fast&lt;/span&gt;
seven_tensors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; sevens]
three_tensors &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [tensor(Image&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;open(o)) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; threes]
len(three_tensors),len(seven_tensors)
&lt;span style=&#34;color:#75715e&#34;&gt;# (6131, 6265)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# display a image in tensor&lt;/span&gt;
show_image(three_tensors[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]);

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we want ot calculate each pixel avarge strange,so wee need to make a 3d tensor /array rank3 tensor each image to a 2d array,multi image become a 3d array&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rank.jpg&#34; alt=&#34;rank&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# change the whole tensorto the 0-1 floating point&lt;/span&gt;
stacked_sevens &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(seven_tensors)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
stacked_threes &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(three_tensors)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;255&lt;/span&gt;
stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape &lt;span style=&#34;color:#75715e&#34;&gt;#torch.Size([6131, 28, 28]),6131 images, each 28*28 pixels&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor. above shape is [6131, 28, 28],rank is 3 (len(stacked_threes.shape)==3)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# get the rank&lt;/span&gt;
len(stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape)
&lt;span style=&#34;color:#75715e&#34;&gt;# or&lt;/span&gt;
stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ndim  &lt;span style=&#34;color:#75715e&#34;&gt;#3&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# calculate the mean of each pixel&lt;/span&gt;
mean3 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; stacked_threes&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
show_image(mean3);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/ideal_3.png&#34; alt=&#34;ideal3&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;mean7&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;stacked_sevens&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;)
show_image(mean7)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/ideal_7.png&#34; alt=&#34;ideal7&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# get a 3&lt;/span&gt;
a_3&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;stacked_threes[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
show_image(a_3)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;step2-now-we-compare-the-ideal-3-and-the-real-3&#34;&gt;step2 now we compare the ideal 3 and the real 3&lt;/h2&gt;
&lt;p&gt;How do we compare the a_3 and the mean3??&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;compare each pixel,get the bas,calculate the avh of the absof each pixel,This is called the mean absolute difference or L1 norm&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;calculaet the (dif)^2,mean, than square root.This is called the root mean squared error (RMSE) or L2 norm.&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# try&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1&lt;/span&gt;
dist_3_abs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean3)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;span style=&#34;color:#75715e&#34;&gt;# 2&lt;/span&gt;
dist_3_sqr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean3)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
dist_3_abs,dist_3_sqr

&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.1114), tensor(0.2021))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dist_7_abs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean7)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;abs()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
dist_7_sqr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ((a_3 &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt; mean7)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
dist_7_abs,dist_7_sqr
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.1586), tensor(0.3021))&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;In both cases, the distance between our 3 and the &amp;ldquo;ideal&amp;rdquo; 3 is less than the distance to the ideal 7. So our simple model will give the right prediction in this case.&lt;/p&gt;
&lt;p&gt;PyTorch already provides both of these as loss functions. You&amp;rsquo;ll find these inside torch.nn.functional, which the PyTorch team recommends importing as F (and is available by default under that name in fastai):&lt;/p&gt;
&lt;p&gt;Here mse stands for mean squared error, and l1 refers to the standard mathematical jargon for mean absolute value (in math it&amp;rsquo;s called the L1 norm).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;l1_loss(a_3&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(),mean7), F&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mse_loss(a_3,mean7)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(0.1586), tensor(0.3021))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 2</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-2/</link>
      <pubDate>Thu, 22 Apr 2021 15:30:42 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-2/</guid>
      <description>Build your Bear reconigize model Download image we use the Azure bing image search API
you need to apply the key for free
!pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * from fastai.vision.widgets import * key = &amp;#39;secret_key_from_bing&amp;#39; import requests import matplotlib.pyplot as plt from PIL import Image from io import BytesIO def search_images_bing_min(search_term): search_url = &amp;#34;https://api.bing.microsoft.com/v7.0/images/search&amp;#34; headers = {&amp;#34;Ocp-Apim-Subscription-Key&amp;#34; : key} # search_term = &amp;#34;grizzly bear&amp;#34; params = {&amp;#34;q&amp;#34;: search_term, &amp;#34;license&amp;#34;: &amp;#34;public&amp;#34;, &amp;#34;imageType&amp;#34;: &amp;#34;photo&amp;#34;,&amp;#34;count&amp;#34;:&amp;#39;150&amp;#39;} response = requests.</description>
      <content>&lt;h1 id=&#34;build-your-bear-reconigize-model&#34;&gt;Build your Bear reconigize model&lt;/h1&gt;
&lt;h2 id=&#34;download-image&#34;&gt;Download image&lt;/h2&gt;
&lt;p&gt;we use the Azure &lt;a href=&#34;https://www.microsoft.com/en-us/bing/apis/bing-image-search-api&#34;&gt;bing image search API&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;you need to apply the key for free&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;Uqq fastbook
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fastbook
fastbook&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup_book()

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastbook &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.widgets &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;

key &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;secret_key_from_bing&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; requests
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; matplotlib.pyplot &lt;span style=&#34;color:#f92672&#34;&gt;as&lt;/span&gt; plt
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; PIL &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; Image
&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; io &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; BytesIO
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;search_images_bing_min&lt;/span&gt;(search_term):
  search_url &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;https://api.bing.microsoft.com/v7.0/images/search&amp;#34;&lt;/span&gt;
  headers &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Ocp-Apim-Subscription-Key&amp;#34;&lt;/span&gt; : key}
  &lt;span style=&#34;color:#75715e&#34;&gt;# search_term = &amp;#34;grizzly bear&amp;#34;&lt;/span&gt;
  params  &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; {&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;q&amp;#34;&lt;/span&gt;: search_term, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;license&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;public&amp;#34;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;imageType&amp;#34;&lt;/span&gt;: &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;photo&amp;#34;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;count&amp;#34;&lt;/span&gt;:&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;150&amp;#39;&lt;/span&gt;}
  response &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; requests&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get(search_url, headers&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;headers, params&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;params)
  response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;raise_for_status()
  search_results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; response&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;json()
  &lt;span style=&#34;color:#75715e&#34;&gt;# ims=[img[&amp;#34;thumbnailUrl&amp;#34;] for img in search_results[&amp;#34;value&amp;#34;]]&lt;/span&gt;
  ims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;search_results[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;value&amp;#34;&lt;/span&gt;]
  &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ims
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can use the abovce function to down the image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;ims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;search_images_bing_min(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;grizzly bear&amp;#34;&lt;/span&gt;)
len(ims)

&lt;span style=&#34;color:#75715e&#34;&gt;# ims is a string array&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;now we can download the iomage to the google drive&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bear_types &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;grizzly&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;black&amp;#39;&lt;/span&gt;,&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;teddy&amp;#39;&lt;/span&gt;
path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Path(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;bearss&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;not&lt;/span&gt; path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;exists():
    path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mkdir()
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; o &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; bear_types:
  &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(o)
  dest &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;o)
  dest&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mkdir(exist_ok&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
  results &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; search_images_bing_min(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{o} bear&amp;#39;&lt;/span&gt;)
  
  &lt;span style=&#34;color:#75715e&#34;&gt;# print(results)&lt;/span&gt;
  ims&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[img[&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;contentUrl&amp;#34;&lt;/span&gt;] &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; img &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; results]
  &lt;span style=&#34;color:#75715e&#34;&gt;# print(ims)&lt;/span&gt;
  download_images(dest, urls&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;ims)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;clean the image,remove the fail image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;fns &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_image_files(path)
fns
failed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; verify_images(fns)
failed
&lt;span style=&#34;color:#75715e&#34;&gt;# if fail,remove it&lt;/span&gt;
failed&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;map(Path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unlink);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;intro-to-create-the-model&#34;&gt;intro to create the model&lt;/h2&gt;
&lt;p&gt;datablock is the templat of a dataloader
data loader tell fastai 4 thing:&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;what is the type of the inf&lt;/li&gt;
&lt;li&gt;how to get the items&lt;/li&gt;
&lt;li&gt;how to tag the items&lt;/li&gt;
&lt;li&gt;How to create the validation set&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;blocks=(ImageBlock, CategoryBlock), mean use the image to predict,&lt;br&gt;
#CategoryBlock mean target is the category&lt;br&gt;
get_items=get_image_files ,how to get the image,from files&lt;br&gt;
splitter mean how to get the validation set&lt;br&gt;
get_y mean how to get the Category&lt;br&gt;
parent_label mean use the parent folder as a category tag&lt;br&gt;
need t oresize all the image to same size,Resize(128)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# datablock is the templat of a dataloader&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# data loader tell fastai 4 thing:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1.what is the type of the inf&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2.how to get the items&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3.how to tag the items&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4.How to create the validation set&lt;/span&gt;


&lt;span style=&#34;color:#75715e&#34;&gt;# blocks=(ImageBlock, CategoryBlock), bean use the image to predict,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#CategoryBlock mean target is the category&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# get_items=get_image_files ,how to get the image,from files&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# splitter mean how to get the validation set&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# get_y mean how to get the Category&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# parent_label mean use the parent folder as a category tag&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# need t oresize all the image to same size,Resize(128)&lt;/span&gt;
bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataBlock(
    blocks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(ImageBlock, CategoryBlock), 
    get_items&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_image_files, 
    splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomSplitter(valid_pct&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;),
    get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;parent_label,
    item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;))

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create the dataloaders,path is the image path,it validate and train the dataloader&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# create the dataloaders,path is the image path,it validate and train the dataloader&lt;/span&gt;
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;see some item in the dataLoader&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;valid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can see ,at defaukt ,the fastai crop the image to the size 128&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/default.PNG&#34; alt=&#34;default&#34;&gt;&lt;/p&gt;
&lt;p&gt;we also can Squish the image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, ResizeMethod&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Squish))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;valid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/squizh.PNG&#34; alt=&#34;Squish&#34;&gt;&lt;/p&gt;
&lt;p&gt;or pad them&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, ResizeMethod&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Pad, pad_mode&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;zeros&amp;#39;&lt;/span&gt;))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;valid&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pad.PNG&#34; alt=&#34;pad&#34;&gt;&lt;/p&gt;
&lt;p&gt;or randomly choose a part to crop the image&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# unique=True mean use the sam picture&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# RandomResizedCrop  Crop  different part of the same picture,we can have more data to train,Data Augmentation&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# min_scale=0.3, select the % of the picture to crop,30%&lt;/span&gt;
bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomResizedCrop(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;, min_scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.3&lt;/span&gt;))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, unique&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/RandomResizedCrop.PNG&#34; alt=&#34;pad&#34;&gt;&lt;/p&gt;
&lt;p&gt;another way to Data Augmentation(資料增強,種通過讓有限的資料產生更多的等價資料來人工擴充套件訓練資料集的技術),not crop,just example, to show  rotation, flipping, perspective warping, brightness changes and contrast changes&lt;br&gt;
batch_tfms apply the aug_transforms to batch ,not only items&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# another way to Data Augmentation,not crop,just example, to show  rotation, flipping, perspective warping, brightness changes and contrast changes&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# production use crop + aug_transforms&lt;/span&gt;
bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;new(item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;128&lt;/span&gt;), batch_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;aug_transforms(mult&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;))
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;, unique&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/aug_t.PNG&#34; alt=&#34;pad&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;start-create-the-model&#34;&gt;Start create the model&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;bears &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataBlock(
    blocks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(ImageBlock, CategoryBlock), 
    get_items&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_image_files, 
    splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomSplitter(valid_pct&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;),
    get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;parent_label,
    item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;RandomResizedCrop(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;, min_scale&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;),
    batch_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;aug_transforms())

dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; bears&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)


learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet18, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;interp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ClassificationInterpretation&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_learner(learn)
interp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_confusion_matrix()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/square.PNG&#34; alt=&#34;square&#34;&gt;
this is the result box,,to see how many item is worng predict
we can see some grizzly,put in black&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;interp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;plot_top_losses(&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, nrows&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# we can see some grizzly,put in black&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# the number loss,mean the predict is right, but not conficdent,or the answer is wrong,this number will high&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the number loss,mean the predict is right, but not conficdent,or the answer is wrong,this number will high
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pl.PNG&#34; alt=&#34;pi&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# ues the fastai GUI to clean the data,remove or re tag&lt;/span&gt;
cleaner &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ImageClassifierCleaner(learn)
cleaner
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/gui.png&#34; alt=&#34;gui&#34;&gt;&lt;/p&gt;
&lt;p&gt;after change the action to the pait(bear colormgroup(valid,train))&lt;br&gt;
run below to move and dlete the items&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete())
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;change())
&lt;span style=&#34;color:#75715e&#34;&gt;# delete the delete marked photo&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;delete(): cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fns[idx]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unlink()
&lt;span style=&#34;color:#75715e&#34;&gt;# move the photo to the right folder /tag&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; idx,cat &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;change(): 
  &lt;span style=&#34;color:#66d9ef&#34;&gt;try&lt;/span&gt;:
    shutil&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;move(str(cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fns[idx]), path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;cat)
  &lt;span style=&#34;color:#66d9ef&#34;&gt;except&lt;/span&gt;:
    cleaner&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fns[idx]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;unlink()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;after this process, we can retrain again,run&lt;/p&gt;
&lt;h2 id=&#34;use-it&#34;&gt;Use it&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;uploader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; widgets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FileUpload()
uploader
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; PILImage&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create(uploader&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
bear_type,_,probs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(img)

&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;bear type: {bear_type}.&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Probability : {probs[1]}&amp;#34;&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Notes 1.2</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-1.2/</link>
      <pubDate>Tue, 20 Apr 2021 16:00:14 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-1.2/</guid>
      <description>Detail explain of the mechaine learning    Term Meaning     Term Meaning   Label The data that we&amp;rsquo;re trying to predict, such as &amp;ldquo;dog&amp;rdquo; or &amp;ldquo;cat&amp;rdquo;   Architecture The template of the model that we&amp;rsquo;re trying to fit; the actual mathematical function that we&amp;rsquo;re passing the input data and parameters to   Model The combination of the architecture with a particular set of parameters   Parameters The values in the model that change what task it can do, and are updated through model training   Fit Update the parameters of the model such that the predictions of the model using the input data match the target labels   Train A synonym for fit   Pretrained model A model that has already been trained, generally using a large dataset, and will be fine-tuned   Fine-tune Update a pretrained model for a different task   Epoch One complete pass through the input data   Loss A measure of how good the model is, chosen to drive training via SGD   Metric A measurement of how good the model is, using the validation set, chosen for human consumption   Validation set A set of data held out from training, used only for measuring how good the model is   Training set The data used for fitting the model; does not include any data from the validation set   Overfitting Training a model in such a way that it remembers specific features of the input data, rather than generalizing well to data not seen during training   CNN Convolutional neural network; a type of neural network that works particularly well for computer vision tasks    </description>
      <content>&lt;p&gt;Detail explain of the mechaine learning
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/train_loop.PNG&#34; alt=&#34;tl&#34;&gt;&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;Term&lt;/th&gt;
&lt;th&gt;Meaning&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;Term&lt;/td&gt;
&lt;td&gt;Meaning&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Label&lt;/td&gt;
&lt;td&gt;The data that we&amp;rsquo;re trying to predict, such as &amp;ldquo;dog&amp;rdquo; or &amp;ldquo;cat&amp;rdquo;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Architecture&lt;/td&gt;
&lt;td&gt;The &lt;em&gt;template&lt;/em&gt; of the model that we&amp;rsquo;re trying to fit; the actual mathematical function that we&amp;rsquo;re passing the input data and parameters to&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Model&lt;/td&gt;
&lt;td&gt;The combination of the architecture with a particular set of parameters&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Parameters&lt;/td&gt;
&lt;td&gt;The values in the model that change what task it can do, and are updated through model training&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fit&lt;/td&gt;
&lt;td&gt;Update the parameters of the model such that the predictions of the model using the input data match the target labels&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Train&lt;/td&gt;
&lt;td&gt;A synonym for &lt;em&gt;fit&lt;/em&gt;&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Pretrained model&lt;/td&gt;
&lt;td&gt;A model that has already been trained, generally using a large dataset, and will be fine-tuned&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Fine-tune&lt;/td&gt;
&lt;td&gt;Update a pretrained model for a different task&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Epoch&lt;/td&gt;
&lt;td&gt;One complete pass through the input data&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Loss&lt;/td&gt;
&lt;td&gt;A measure of how good the model is, chosen to drive training via SGD&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Metric&lt;/td&gt;
&lt;td&gt;A measurement of how good the model is, using the validation set, chosen for human consumption&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Validation set&lt;/td&gt;
&lt;td&gt;A set of data held out from training, used only for measuring how good the model is&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Training set&lt;/td&gt;
&lt;td&gt;The data used for fitting the model; does not include any data from the validation set&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;Overfitting&lt;/td&gt;
&lt;td&gt;Training a model in such a way that it &lt;em&gt;remembers&lt;/em&gt; specific features of the input data, rather than generalizing well to data not seen during training&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;CNN&lt;/td&gt;
&lt;td&gt;Convolutional neural network; a type of neural network that works particularly well for computer vision tasks&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
</content>
    </item>
    
    <item>
      <title>Ai Notes 1.1</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-1.1/</link>
      <pubDate>Tue, 20 Apr 2021 14:46:19 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-1.1/</guid>
      <description>Your First Model on fastai framework
!pip install -Uqq fastbook import fastbook fastbook.setup_book() from fastbook import * #id first_training #caption Results from the first training from fastai.vision.all import * # get the cat do images path path = untar_data(URLs.PETS)/&amp;#39;images&amp;#39; def is_cat(x): return x[0].isupper() # because we are stuing the image so use ImageDataLoaders # valid_pct=0.2 meankeep 20 % photo image not use on training,for testing # label_func=is_cat mean get the tag of the imag,to detect # is the photos is a cat or dog,if photo name start by Upper case # ,than tha pohot is a cat # item_tfms=Resize(224) resize the photo to 224 dls = ImageDataLoaders.</description>
      <content>&lt;p&gt;Your First Model on fastai framework&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-python&#34; data-lang=&#34;python&#34;&gt;
&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;!&lt;/span&gt;pip install &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;Uqq fastbook
&lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; fastbook
fastbook&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;setup_book()



&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastbook &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;


&lt;span style=&#34;color:#75715e&#34;&gt;#id first_training&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#caption Results from the first training&lt;/span&gt;

&lt;span style=&#34;color:#f92672&#34;&gt;from&lt;/span&gt; fastai.vision.all &lt;span style=&#34;color:#f92672&#34;&gt;import&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# get the cat do images path&lt;/span&gt;
path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; untar_data(URLs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;PETS)&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;images&amp;#39;&lt;/span&gt;

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;is_cat&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;isupper()
&lt;span style=&#34;color:#75715e&#34;&gt;# because we are stuing the image so use ImageDataLoaders&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# valid_pct=0.2 meankeep 20 % photo image not use on training,for testing&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# label_func=is_cat mean get the tag of the imag,to detect&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# is the photos is a cat or dog,if photo name start by Upper case&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# ,than tha pohot is a cat&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# item_tfms=Resize(224) resize the photo to 224&lt;/span&gt;
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ImageDataLoaders&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_name_func(
    path, get_image_files(path), valid_pct&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;, seed&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;42&lt;/span&gt;,
    label_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;is_cat, item_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;Resize(&lt;span style=&#34;color:#ae81ff&#34;&gt;224&lt;/span&gt;))

learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet34, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;error_rate)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)


&lt;span style=&#34;color:#75715e&#34;&gt;# use to create a upload button&lt;/span&gt;

uploader &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; widgets&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;FileUpload()
uploader


&lt;span style=&#34;color:#75715e&#34;&gt;# use the model to detect is your photo is a cat&lt;/span&gt;
img &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; PILImage&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create(uploader&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
is_cat,_,probs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;predict(img)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Is this a cat?: {is_cat}.&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Probability it&amp;#39;s a cat: {probs[1].item():.6f}&amp;#34;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Notes 0</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-0/</link>
      <pubDate>Mon, 19 Apr 2021 15:26:46 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-0/</guid>
      <description>一向想學下 Machine laerning,見到有本幾好的書 寫給程式設計師的深度學習：使用fastai和PyTorch，決定一邊睇書，一邊寫有關本書的Notes，希望能夠完整完成.</description>
      <content>&lt;p&gt;一向想學下 Machine laerning,見到有本幾好的書 &lt;a href=&#34;https://www.books.com.tw/products/0010886129?sloc=main&#34;&gt;寫給程式設計師的深度學習：使用fastai和PyTorch&lt;/a&gt;，決定一邊睇書，一邊寫有關本書的Notes，希望能夠完整完成.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
