<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Cross-Entropy on Terminal</title>
    <link>https://roy989898.github.io/tags/cross-entropy/</link>
    <description>Recent content in Cross-Entropy on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 16 May 2021 18:41:39 +0800</lastBuildDate><atom:link href="https://roy989898.github.io/tags/cross-entropy/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Tutorial 6.4 Other Computer Vision Problems-Multi-Label Binary Cross-Entropy Image and Point</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-6.4/</link>
      <pubDate>Sun, 16 May 2021 18:41:39 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-6.4/</guid>
      <description>Image and Point key point model A key point refers to a specific location represented in an image—in this case, we&amp;rsquo;ll use images of people and we&amp;rsquo;ll be looking for the center of the person&amp;rsquo;s face in each image. That means we&amp;rsquo;ll actually be predicting two values for each image: the row and column of the face center.
Assemble the Data path = untar_data(URLs.BIWI_HEAD_POSE) #hide Path.BASE_PATH = path 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .</description>
      <content>&lt;h1 id=&#34;image-and-point&#34;&gt;Image and Point&lt;/h1&gt;
&lt;h2 id=&#34;key-point-model&#34;&gt;key point model&lt;/h2&gt;
&lt;p&gt;A key point refers to a specific location represented in an image—in this case, we&amp;rsquo;ll use images of people and we&amp;rsquo;ll be looking for the center of the person&amp;rsquo;s face in each image. That means we&amp;rsquo;ll actually be predicting two values for each image: the row and column of the face center.&lt;/p&gt;
&lt;h2 id=&#34;assemble-the-data&#34;&gt;Assemble the Data&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;path &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; untar_data(URLs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;BIWI_HEAD_POSE)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#hide&lt;/span&gt;
Path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;BASE_PATH &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; path
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .obj file for each (we won&amp;rsquo;t need them here)&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;path&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sorted()
&lt;span style=&#34;color:#75715e&#34;&gt;# (#50) [Path(&amp;#39;01&amp;#39;),Path(&amp;#39;01.obj&amp;#39;),Path(&amp;#39;02&amp;#39;),Path(&amp;#39;02.obj&amp;#39;),Path(&amp;#39;03&amp;#39;),Path(&amp;#39;03.obj&amp;#39;),Path(&amp;#39;04&amp;#39;),Path(&amp;#39;04.obj&amp;#39;),Path(&amp;#39;05&amp;#39;),Path(&amp;#39;05.obj&amp;#39;)...]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;01&amp;#39;&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ls()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sorted()
&lt;span style=&#34;color:#75715e&#34;&gt;# (#1000) [Path(&amp;#39;01/depth.cal&amp;#39;),Path(&amp;#39;01/frame_00003_pose.txt&amp;#39;),Path(&amp;#39;01/frame_00003_rgb.jpg&amp;#39;),Path(&amp;#39;01/frame_00004_pose.txt&amp;#39;),Path(&amp;#39;01/frame_00004_rgb.jpg&amp;#39;),Path(&amp;#39;01/frame_00005_pose.txt&amp;#39;),Path(&amp;#39;01/frame_00005_rgb.jpg&amp;#39;),Path(&amp;#39;01/frame_00006_pose.txt&amp;#39;),Path(&amp;#39;01/frame_00006_rgb.jpg&amp;#39;),Path(&amp;#39;01/frame_00007_pose.txt&amp;#39;)...]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;get the image file&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;img_files &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; get_image_files(path)
img_files
&lt;span style=&#34;color:#75715e&#34;&gt;# (#15678) [Path(&amp;#39;03/frame_00650_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00644_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00491_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00207_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00067_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00056_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00025_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00450_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00584_rgb.jpg&amp;#39;),Path(&amp;#39;03/frame_00285_rgb.jpg&amp;#39;)...]&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;img_files[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# Path(&amp;#39;03/frame_00650_rgb.jpg&amp;#39;)&lt;/span&gt;

&lt;span style=&#34;color:#e6db74&#34;&gt;``&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;`&lt;/span&gt; a function that use the image name to get the pose&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;txt

&lt;span style=&#34;color:#e6db74&#34;&gt;``&lt;/span&gt;&lt;span style=&#34;color:#960050;background-color:#1e0010&#34;&gt;`&lt;/span&gt;py
&lt;span style=&#34;color:#75715e&#34;&gt;# a function that use the image name to get the pose.txt&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;img2pose&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; Path(f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;{str(x)[:-7]}pose.txt&amp;#39;&lt;/span&gt;)
img2pose(img_files[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
&lt;span style=&#34;color:#75715e&#34;&gt;# Path(&amp;#39;03/frame_00650_pose.txt&amp;#39;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# see the first image&lt;/span&gt;
im &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; PILImage&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;create(img_files[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# (480, 640)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;im&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;to_thumb(&lt;span style=&#34;color:#ae81ff&#34;&gt;160&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/py.PNG&#34; alt=&#34;py&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;get-the-head-point&#34;&gt;get the head point&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;cal &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;genfromtxt(path&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;01&amp;#39;&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;rgb.cal&amp;#39;&lt;/span&gt;, skip_footer&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;)
cal

&lt;span style=&#34;color:#75715e&#34;&gt;# array([[517.679,   0.   , 320.   ],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0.   , 517.679, 240.5  ],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#        [  0.   ,   0.   ,   1.   ]])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# the function to get the head point&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;get_ctr&lt;/span&gt;(f):
    ctr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; np&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;genfromtxt(img2pose(f), skip_header&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)
    c1 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ctr[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; cal[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;ctr[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; cal[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
    c2 &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; ctr[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; cal[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;/&lt;/span&gt;ctr[&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;] &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; cal[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;][&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; tensor([c1,c2])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;get_ctr(img_files[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;])
tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;447.6672&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;277.1215&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we have 2 problem at here&lt;/p&gt;
&lt;p&gt;One important point to note is that we should not just use a random splitter. The reason for this is that the same people appear in multiple images in this dataset, but we want to ensure that our model can generalize to people that it hasn&amp;rsquo;t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person&amp;rsquo;s images.&lt;/p&gt;
&lt;p&gt;The only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images:&lt;/p&gt;
&lt;p&gt;splitter=FuncSplitter(lambda o: o.parent.name==&amp;lsquo;13&amp;rsquo;), mean we want ot create validation set containing just that person&amp;rsquo;s images.that contain in the document 13&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;biwi &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataBlock(
    blocks&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(ImageBlock, PointBlock),
    get_items&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_image_files,
    get_y&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;get_ctr,
    splitter&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;FuncSplitter(&lt;span style=&#34;color:#66d9ef&#34;&gt;lambda&lt;/span&gt; o: o&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parent&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;name&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;13&amp;#39;&lt;/span&gt;),
    batch_tfms&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;[&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;aug_transforms(size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;240&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;320&lt;/span&gt;)), 
                Normalize&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;from_stats(&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;imagenet_stats)]
)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# check is the data ok&lt;/span&gt;
dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; biwi&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;dataloaders(path)
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;show_batch(max_n&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9&lt;/span&gt;, figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;8&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;6&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/train_9.PNG&#34; alt=&#34;train_9&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# check the size&lt;/span&gt;
xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;one_batch()
xb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,yb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;the location of the point
yb[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]
&lt;span style=&#34;color:#75715e&#34;&gt;# TensorPoint([[-0.2325,  0.1644]], device=&amp;#39;cuda:0&amp;#39;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;train-a-model&#34;&gt;train a Model&lt;/h2&gt;
&lt;p&gt;we used y_range to tell fastai the range of our targets? We&amp;rsquo;ll do the same here (coordinates in fastai and PyTorch are always rescaled between -1 and +1):&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# why a y_range???&lt;/span&gt;

learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet18, y_range&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# the y_range function&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;sigmoid_range&lt;/span&gt;(x, lo, hi): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid(x) &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; (hi&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;lo) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; lo
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;the loss function in the learner&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# default use MSELoss&lt;/span&gt;
dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;loss_func
&lt;span style=&#34;color:#75715e&#34;&gt;# FlattenedLoss of MSELoss()&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;pytorch 的nn.MSELoss()損失函數 &lt;a href=&#34;https://blog.csdn.net/weixin_38145317/article/details/103735784&#34;&gt;https://blog.csdn.net/weixin_38145317/article/details/103735784&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;This makes sense, since when coordinates are used as the dependent variable, most of the time we&amp;rsquo;re likely to be trying to predict something as close as possible; that&amp;rsquo;s basically what MSELoss (mean squared error loss) does. If you want to use a different loss function, you can pass it to cnn_learner using the loss_func parameter.&lt;/p&gt;
&lt;h2 id=&#34;metrics&#34;&gt;metrics&lt;/h2&gt;
&lt;p&gt;Note also that we didn&amp;rsquo;t specify any metrics. That&amp;rsquo;s because the MSE is already a useful metric for this task (although it&amp;rsquo;s probably more interpretable after we take the square root).&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;min,steep&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr_find()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/lrf6.PNG&#34; alt=&#34;lrf6&#34;&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;min,steep
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-2&lt;/span&gt;  &lt;span style=&#34;color:#75715e&#34;&gt;# book is 2e-2&lt;/span&gt;
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, lr)
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.048711 0.018842 01:48&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.008920 0.010456 01:51&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.002904 0.000215 01:51&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.001400 0.000146 01:51&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Generally when we run this we get a loss of around 0.0001, which corresponds to an average coordinate prediction error of:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;math&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt(&lt;span style=&#34;color:#ae81ff&#34;&gt;0.0001&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;learn.show_results(ds_idx=1, nrows=3, figsize=(6,8))&lt;/p&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pre_r6.PNG&#34; alt=&#34;pre_r6&#34;&gt;&lt;/p&gt;
&lt;h1 id=&#34;conclusion&#34;&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;fastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want:&lt;/p&gt;
&lt;p&gt;nn.CrossEntropyLoss for single-label classification nn.BCEWithLogitsLoss for multi-label classification nn.MSELoss for regression&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 6.3 Other Computer Vision Problems-Multi-Label Binary Cross-Entropy</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-6.3/</link>
      <pubDate>Sun, 16 May 2021 16:59:46 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-6.3/</guid>
      <description>some python basic partial
def say_hello(name, say_what=&amp;#34;Hello&amp;#34;): return f&amp;#34;{say_what} {name}.&amp;#34; say_hello(&amp;#39;Jeremy&amp;#39;),say_hello(&amp;#39;Jeremy&amp;#39;, &amp;#39;Ahoy!&amp;#39;) # (&amp;#39;Hello Jeremy.&amp;#39;, &amp;#39;Ahoy! Jeremy.&amp;#39;) f = partial(say_hello, say_what=&amp;#34;Bonjour&amp;#34;) f(&amp;#34;Jeremy&amp;#34;),f(&amp;#34;Sylvain&amp;#34;) # (&amp;#39;Bonjour Jeremy.&amp;#39;, &amp;#39;Bonjour Sylvain.&amp;#39;) Binary Cross-Entropy a Learner object contains four main things: the model, a DataLoaders object, an Optimizer, and the loss function to use. we use resnet models (teach later),we know howto build SGD optimizer(lesson 4) and the dataloader,so we look focus on the loss function.</description>
      <content>&lt;h1 id=&#34;some-python-basic&#34;&gt;some python basic&lt;/h1&gt;
&lt;p&gt;partial&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;say_hello&lt;/span&gt;(name, say_what&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Hello&amp;#34;&lt;/span&gt;): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; f&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;{say_what} {name}.&amp;#34;&lt;/span&gt;
say_hello(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Jeremy&amp;#39;&lt;/span&gt;),say_hello(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Jeremy&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;Ahoy!&amp;#39;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# (&amp;#39;Hello Jeremy.&amp;#39;, &amp;#39;Ahoy! Jeremy.&amp;#39;)&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partial(say_hello, say_what&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Bonjour&amp;#34;&lt;/span&gt;)
f(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Jeremy&amp;#34;&lt;/span&gt;),f(&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Sylvain&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# (&amp;#39;Bonjour Jeremy.&amp;#39;, &amp;#39;Bonjour Sylvain.&amp;#39;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;binary-cross-entropy&#34;&gt;Binary Cross-Entropy&lt;/h1&gt;
&lt;p&gt;a Learner object contains four main things: the model, a DataLoaders object, an Optimizer, and the loss function to use.
we use resnet models (teach later),we know howto build SGD optimizer(lesson 4) and the dataloader,so we look focus on the &lt;em&gt;loss function&lt;/em&gt;.&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet18)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;seeone batch&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;x,y &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; to_cpu(dls&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;train&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;one_batch())
x[&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;]&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape

&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([3, 128, 128])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# pass the independernt vairable to the model,to gte the activs &lt;/span&gt;
activs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;model(x)
activs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([64, 20])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;why is this shape???torch.Size([64, 20]), because the match size is 64,and we have 20 categories,the activs, is for each image,the probability of each of 20 categories&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;activs
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([[ 0.7476, -1.1988,  4.5421,  ...,  0.7063, -1.3358, -0.3715],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-0.9919, -0.4608, -0.4424,  ..., -1.4165, -2.9962,  0.5873],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [ 2.1179, -0.0294,  0.7001,  ...,  2.2310,  1.1888, -0.0595],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         ...,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-0.3535,  3.0212,  0.4811,  ...,  1.8732,  1.2486, -3.3234],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.4724, -2.8740, -1.2860,  ..., -2.7895, -1.8632, -0.1557],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#         [-1.6487,  1.5647,  1.0682,  ..., -0.6979, -1.5629, -1.7217]], grad_fn=&amp;lt;MmBackward&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we can see that the number still not between 0 and 1,but we can use the the loss function learn in lesson 4(mist_loss,because have sigmoid) and add log&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;binary_cross_entropy&lt;/span&gt;(inputs, targets):
    inputs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inputs&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;where(targets&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;, &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;inputs, inputs)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;log()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;why we do not use the nll_loss or softmax thta lear in lesson 5????becuase it use for one image one tag,but ther is one imagfe maybe &amp;gt;1 tag or 0 tag&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;softmax&lt;/strong&gt;, as we saw, requires that all predictions sum to 1, and tends to push one activation to be much larger than the others (due to the use of exp); however, we may well have multiple objects that we&amp;rsquo;re confident appear in an image, so restricting the maximum sum of activations to 1 is not a good idea. By the same reasoning, we may want the sum to be less than 1, if we don&amp;rsquo;t think any of the categories appear in an image.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;nll_loss&lt;/strong&gt;, as we saw, returns the value of just one activation: the single activation corresponding with the single label for an item. This doesn&amp;rsquo;t make sense when we have multiple labels.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;pytorch already provide binary_cross_entropy&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss_func &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;BCEWithLogitsLoss()
loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; loss_func(activs, y)
loss

&lt;span style=&#34;color:#75715e&#34;&gt;# TensorMultiCategory(1.0342, grad_fn=&amp;lt;AliasBackward&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;However ,we do not need to require fastai use this loss function,!!! Becasue if fastai dataloaders know have multi categories ta a image, default use nn.BCEWithLogitsLoss&lt;/p&gt;
&lt;p&gt;we need to change the metric too,compare to the lesson 5&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# orginal one&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;accuracy&lt;/span&gt;(inp, targ, axis&lt;span style=&#34;color:#f92672&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Compute accuracy with `targ` when `pred` is bs * n_classes&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#75715e&#34;&gt;# select the mots hight valu one.but know we have multi category for a image&lt;/span&gt;
    pred &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;argmax(dim&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;axis)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; (pred &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; targ)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# suitable one&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# we need to set a value:thresh,to decide which is 1,whis is 0&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;accuracy_multi&lt;/span&gt;(inp, targ, thresh&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;, sigmoid&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True):
    &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#34;Compute accuracy when `inp` and `targ` are the same size.&amp;#34;&lt;/span&gt;
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; sigmoid: inp &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; inp&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ((inp&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;thresh)&lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt;targ&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;bool())&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;now ,use the new metric start to train&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# start to train&lt;/span&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; cnn_learner(dls, resnet50, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;partial(accuracy_multi, thresh&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.2&lt;/span&gt;))
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fine_tune(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;, base_lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3e-3&lt;/span&gt;, freeze_epochs&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)

&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss accuracy_multi time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.942215 0.698972 0.239303 00:26&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.824776 0.551198 0.290996 00:26&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.607759 0.198789 0.827131 00:26&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.361537 0.125557 0.943287 00:26&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss accuracy_multi time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.134416 0.125471 0.934343 00:27&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.118428 0.105183 0.949880 00:27&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.097109 0.102836 0.950040 00:27&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;after train we can chnage the metrics with different value of thresh If you pick a threshold that&amp;rsquo;s too low, you&amp;rsquo;ll often be failing to select correctly labeled objects&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;metrics &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partial(accuracy_multi, thresh&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.1&lt;/span&gt;)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;validate()
&lt;span style=&#34;color:#75715e&#34;&gt;# validation loss and metrics&lt;/span&gt;

&lt;span style=&#34;color:#75715e&#34;&gt;# [0.10283613950014114,0.9265138506889343]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;you&amp;rsquo;ll only be selecting the objects for which your model is very confident with a high thresh:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;metrics &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; partial(accuracy_multi, thresh&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.99&lt;/span&gt;)
learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;validate()
&lt;span style=&#34;color:#75715e&#34;&gt;# [0.10283613950014114,0.9433467388153076]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;we calculate one pred to test different thresh value&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# we can getpre and &lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# the get_preds get all the valid data ,to calculate their pred,and the target&lt;/span&gt;
preds,targs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;get_preds()

&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor([[1.3728e-03, 3.1368e-03, 4.9623e-04,  ..., 4.6060e-01, 1.1935e-03, 9.1202e-02],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [3.3482e-04, 1.2069e-02, 1.0969e-03,  ..., 1.7647e-02, 1.6421e-03, 9.6689e-04],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [3.8831e-03, 1.3268e-02, 4.4939e-03,  ..., 1.1680e-02, 1.0579e-03, 2.7399e-03],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          ...,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [3.9797e-03, 5.1892e-03, 7.0612e-04,  ..., 3.0286e-03, 1.7749e-03, 7.1625e-03],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [8.4477e-03, 7.8008e-03, 1.7175e-03,  ..., 2.0243e-03, 2.5596e-02, 1.8781e-03],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [6.2252e-04, 9.4245e-01, 4.3180e-03,  ..., 8.6979e-03, 8.4691e-04, 5.2906e-03]]),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  TensorMultiCategory([[0., 0., 0.,  ..., 0., 0., 0.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [0., 0., 0.,  ..., 0., 0., 0.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [0., 0., 0.,  ..., 0., 0., 0.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          ...,&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [0., 0., 0.,  ..., 0., 0., 0.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [0., 0., 0.,  ..., 0., 0., 0.],&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#          [0., 1., 0.,  ..., 0., 0., 0.]]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# at default ,get_preds applies the output activation function (sigmoid, in this case) for us, so we&amp;#39;ll need to tell accuracy_multi to not apply it:&lt;/span&gt;
accuracy_multi(preds, targs, thresh&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.9&lt;/span&gt;, sigmoid&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;False)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;use this way to find the best thresh value&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;xs = torch.linspace(0.05,0.95,29)
accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs]
plt.plot(xs,accs);
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/graph_tresh.PNG&#34; alt=&#34;graph_tresh&#34;&gt;&lt;/p&gt;
&lt;p&gt;In this case, we&amp;rsquo;re using the validation set to pick a hyperparameter (the threshold), which is the purpose of the validation set. Sometimes students have expressed their concern that we might be overfitting to the validation set, since we&amp;rsquo;re trying lots of values to see which is the best. However, as you see in the plot, changing the threshold in this case results in a smooth curve, so we&amp;rsquo;re clearly not picking some inappropriate outlier. This is a good example of where you have to be careful of the difference between theory (don&amp;rsquo;t try lots of hyperparameter values or you might overfit the validation set) versus practice (if the relationship is smooth, then it&amp;rsquo;s fine to do this).&lt;/p&gt;
&lt;p&gt;在這種情況下，我們使用驗證集來選擇一個超參數（閾值），這是驗證集的目的。 有時，學生表達了他們對我們可能過度適合驗證集的擔憂，因為我們正在嘗試大量的值以查看哪種值最好。 但是，正如您在圖中所看到的，在這種情況下，更改閾值會產生平滑的曲線，因此我們顯然不會選擇一些不合適的離群值。 這是一個很好的例子，說明您必須注意理論（不要嘗試過多的超參數值，否則可能會過度擬合驗證集）與實踐之間的差異（如果關係是平滑的，則可以這樣做） 。&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
