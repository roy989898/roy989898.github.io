<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>SGD on Terminal</title>
    <link>https://roy989898.github.io/tags/sgd/</link>
    <description>Recent content in SGD on Terminal</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 29 Apr 2021 11:23:16 +0800</lastBuildDate><atom:link href="https://roy989898.github.io/tags/sgd/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Ai Tutorial 4.11 Creating an Optimizer</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.11/</link>
      <pubDate>Thu, 29 Apr 2021 11:23:16 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.11/</guid>
      <description>Creating an Optimizer we can make the above code more general to use # use nn.Linear to replace the linear1 # it do the same thing with the linear1 and init_params linear_model = nn.Linear(28*28,1) # we can get the paramater, weight,basic w,b = linear_model.parameters() w.shape,b.shape,b # (torch.Size([1, 784]), torch.Size([1]), Parameter containing: # tensor([-0.0180], requires_grad=True)) class BasicOptim: def __init__(self,params,lr): self.params,self.lr = list(params),lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.</description>
      <content>&lt;h1 id=&#34;creating-an-optimizer&#34;&gt;Creating an Optimizer&lt;/h1&gt;
&lt;h2 id=&#34;we-can-make-the-above-code-more-general-to-use&#34;&gt;we can make the above code more general to use&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# use nn.Linear to replace the linear1&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# it do the same thing with the linear1 and init_params&lt;/span&gt;
linear_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#  we can get the paramater, weight,basic&lt;/span&gt;
w,b &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; linear_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters()
w&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,b&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,b

&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([1, 784]), torch.Size([1]), Parameter containing:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  tensor([-0.0180], requires_grad=True))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;class&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;BasicOptim&lt;/span&gt;:
    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; __init__(self,params,lr): self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;params,self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; list(params),lr

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;step&lt;/span&gt;(self, &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;params: p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;lr

    &lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;zero_grad&lt;/span&gt;(self, &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;args, &lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;kwargs):
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; self&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;params: p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;opt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; BasicOptim(linear_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters(), lr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;simplfy the trainb loop&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# use it&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train_epoch&lt;/span&gt;(model):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dl:
        calc_grad(xb, yb, model)
        opt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;step()
        opt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_grad()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.10/#put-above-together-to-create-calc_grad-functions&#34; title=&#34;calc_grad&#34;&gt;calc_grad&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train_model&lt;/span&gt;(model, epochs):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(epochs):
        train_epoch(model)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(validate_epoch(model), end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
train_model(linear_model, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)
&lt;span style=&#34;color:#75715e&#34;&gt;# same with above code&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;actually-fastai-already-have-the-same-thing&#34;&gt;Actually ,fastai already have the same thing&lt;/h2&gt;
&lt;p&gt;to replace the BasicOptim&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;linear_model &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
opt &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; SGD(linear_model&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;parameters(), lr)
train_model(linear_model, &lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;to replace the train train_model&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dls &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoaders(dl, valid_dl)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
learn &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; Learner(dls, nn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Linear(&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;), opt_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;SGD,
                loss_func&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;mnist_loss, metrics&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;batch_accuracy)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;nn.Linear:how to predict the value&lt;br&gt;
opt_func:howw to change the weight&lt;br&gt;
loss_func:how to calculate the loss&lt;br&gt;
metrics:how to calculate the metrics&lt;/p&gt;
&lt;p&gt;&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#better-loss-finction&#34; title=&#34;mnist_loss&#34;&gt;mnist_loss&lt;/a&gt;
&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.10/#put-above-together-to-create-calc_grad-functions&#34; title=&#34;batch_accuracy&#34;&gt;batch_accuracy&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;learn&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;fit(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;, lr&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;lr)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# epoch train_loss valid_loss batch_accuracy time&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 0 0.636991 0.503566 0.495584 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 1 0.553366 0.176069 0.857704 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 2 0.202398 0.188561 0.829244 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 3 0.088171 0.108241 0.912169 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 4 0.046019 0.078468 0.932287 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 5 0.029606 0.062658 0.947498 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 6 0.022893 0.052850 0.955839 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 7 0.019928 0.046356 0.962218 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 8 0.018433 0.041814 0.966143 00:00&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 9 0.017540 0.038480 0.968106 00:00&lt;/span&gt;

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.10 Put it all together</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.10/</link>
      <pubDate>Wed, 28 Apr 2021 16:52:44 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.10/</guid>
      <description>Put it all together each epoch is like this
# basic example # for x,y in dl: # pred = model(x) # loss = loss_func(pred, y) # loss.backward() # parameters -= parameters.grad * lr re-initialize our parameters:
weights = init_params((28*28,1)) bias = init_params(1) weights.shape # torch.Size([784, 1]) create DataLoader of train data from Dataset
dl = DataLoader(dset, batch_size=256) xb,yb = first(dl) xb.shape,yb.shape (torch.Size([784]), tensor([1])) # (torch.Size([784]), tensor([1])) create DataLoader of valid data valid data</description>
      <content>&lt;h1 id=&#34;put-it-all-together&#34;&gt;Put it all together&lt;/h1&gt;
&lt;p&gt;each epoch is like this&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# basic example&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# for x,y in dl:&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     pred = model(x)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     loss = loss_func(pred, y)&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     loss.backward()&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#     parameters -= parameters.grad * lr&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;re-initialize our parameters:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
weights &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params((&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;28&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;))
bias &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; init_params(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;)
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([784, 1])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create DataLoader of train data  from &lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#prepare-the-pytorch-need-format&#34; title=&#34;Dataset&#34;&gt;Dataset&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;dl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(dset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;)
xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; first(dl)
xb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,yb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;Size([&lt;span style=&#34;color:#ae81ff&#34;&gt;784&lt;/span&gt;]), tensor([&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]))
&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([784]), tensor([1]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create DataLoader of valid data &lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#prepare-the-valid-data&#34; title=&#34;valid data&#34;&gt;valid data&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;valid_dl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(valid_dset, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;256&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create a 4 size batch for test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;batch &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; train_x[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;]
batch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.Size([4, 784])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;alcaulate the predict
&lt;a href=&#34;https://roy989898.github.io/posts/ai-tutorial-4.8/#predict--multi-image&#34; title=&#34;linear1&#34;&gt;linear1&lt;/a&gt;&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; linear1(batch)
preds
&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;&amp;#39;&amp;#39; tensor([[-4.5725],
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        [ 0.2557],
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        [-5.5496],
&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;        [ 3.6488]], grad_fn=&amp;lt;AddBackward0&amp;gt;) &amp;#39;&amp;#39;&amp;#39;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;calculate a loss&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mnist_loss(preds, train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])
loss
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(0.6119, grad_fn=&amp;lt;MeanBackward0&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Now we can calculate the gradients:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;shape,weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(),bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# (torch.Size([784, 1]), tensor(-0.0103), tensor([-0.0712]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;put-above-together-to-create-calc_grad-functions&#34;&gt;put above together to create calc_grad functions&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;calc_grad&lt;/span&gt;(xb, yb, model):
    preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; model(xb)
    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mnist_loss(preds, yb)
    loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;test&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;calc_grad(batch, train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;], linear1)
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(),bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(-0.0207), tensor([-0.1423]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;run again&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;calc_grad(batch, train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;], linear1)
weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean(),bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# (tensor(-0.0310), tensor([-0.2135]))&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;have probelm!!!!! we expect the grad should be the same ,becasue all the parameter of the calc_grad is same,but not!!!
becasue because the loss.backward add the gradients of loss to any gradients that are currently stored. So, we have to set the current gradients to 0 first:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;weights&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_()
bias&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# each epoch function&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# params already use in model&lt;/span&gt;
&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;train_epoch&lt;/span&gt;(model, lr, params):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; dl:
        calc_grad(xb, yb, model)
        &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; p &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; params:
            p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;lr
            p&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;zero_()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;That gives us this function to calculate our validation accuracy:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;batch_accuracy&lt;/span&gt;(xb, yb):
    preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; xb&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sigmoid()
    &lt;span style=&#34;color:#75715e&#34;&gt;# after sigmoid 0 becime 0.5 so, &amp;gt;0.5 ,is 1,that mean it is 3&lt;/span&gt;
    correct &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; (preds&lt;span style=&#34;color:#f92672&#34;&gt;&amp;gt;&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;0.5&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;==&lt;/span&gt; yb
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; correct&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;We can check it works:
linear1 calculate the prediction&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;batch_accuracy(linear1(batch), train_y[:&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;])
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;check is it work&lt;/p&gt;
&lt;pre&gt;&lt;code&gt;batch_accuracy(linear1(batch), train_y[:4])
&lt;/code&gt;&lt;/pre&gt;&lt;p&gt;create a valid epoch function to our new weight model&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;validate_epoch&lt;/span&gt;(model):
    accs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; [batch_accuracy(model(xb), yb) &lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; xb,yb &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; valid_dl]
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; round(torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;stack(accs)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item(), &lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h2 id=&#34;now-use-the-above-function-to-run-a-epoch&#34;&gt;now use the above function to run a epoch&lt;/h2&gt;
&lt;h3 id=&#34;strat-point&#34;&gt;strat point&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1.&lt;/span&gt;
params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; weights,bias
train_epoch(linear1, lr, params)
validate_epoch(linear1)
&lt;span style=&#34;color:#75715e&#34;&gt;# 0.6268&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;do-more&#34;&gt;do more&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;):
    train_epoch(linear1, lr, params)
    &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(validate_epoch(linear1), end&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39; &amp;#39;&lt;/span&gt;)
    &lt;span style=&#34;color:#75715e&#34;&gt;# 0.7656 0.875 0.9165 0.936 0.9443 0.954 0.9565 0.9575 0.9589 0.9599 0.9619 0.9628 0.9643 0.9662 0.9672 0.9682 0.9692 0.9697 0.9702 0.9702 &lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.9 SGD and Mini-Batches</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.9/</link>
      <pubDate>Wed, 28 Apr 2021 16:06:34 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.9/</guid>
      <description>we already have a SGD loss function,we can go to Step
which is to change or update the weights based on the gradients. This is called an optimization step.
basic Python string.ascii_lowercase string.ascii_lowercase # abcdefghijklmnopqrstuvwxyz basic Pytorch enumerate # L is something liek list # enumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在for 循環當中。 ds = L(enumerate(string.ascii_lowercase)) ds # [(0, &amp;#39;a&amp;#39;),(1, &amp;#39;b&amp;#39;),(2, &amp;#39;c&amp;#39;),(3, &amp;#39;d&amp;#39;),(4, &amp;#39;e&amp;#39;),(5, &amp;#39;f&amp;#39;),(6, &amp;#39;g&amp;#39;),(7, &amp;#39;h&amp;#39;),(8, &amp;#39;i&amp;#39;),(9, &amp;#39;j&amp;#39;)...] optimization step why Mini-Batches we can one item for 1 epoch,but this will be very slow,</description>
      <content>&lt;p&gt;we already have a SGD loss function,we can go to &lt;code&gt;Step&lt;/code&gt;&lt;br&gt;
which is to change or update the weights based on the gradients. This is called an optimization step.&lt;/p&gt;
&lt;h1 id=&#34;basic-python&#34;&gt;basic Python&lt;/h1&gt;
&lt;h2 id=&#34;stringascii_lowercase&#34;&gt;string.ascii_lowercase&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_lowercase
&lt;span style=&#34;color:#75715e&#34;&gt;# abcdefghijklmnopqrstuvwxyz&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;basic-pytorch&#34;&gt;basic Pytorch&lt;/h1&gt;
&lt;h2 id=&#34;enumerate&#34;&gt;enumerate&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;#  L is something liek list&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# enumerate() 函數用於將一個可遍歷的數據對象(如列表、元組或字符串)組合為一個索引序列，同時列出數據和數據下標，一般用在for 循環當中。&lt;/span&gt;
ds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; L(enumerate(string&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;ascii_lowercase))
ds
&lt;span style=&#34;color:#75715e&#34;&gt;# [(0, &amp;#39;a&amp;#39;),(1, &amp;#39;b&amp;#39;),(2, &amp;#39;c&amp;#39;),(3, &amp;#39;d&amp;#39;),(4, &amp;#39;e&amp;#39;),(5, &amp;#39;f&amp;#39;),(6, &amp;#39;g&amp;#39;),(7, &amp;#39;h&amp;#39;),(8, &amp;#39;i&amp;#39;),(9, &amp;#39;j&amp;#39;)...]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h1 id=&#34;optimization-step&#34;&gt;optimization step&lt;/h1&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/sgd_step.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;why-mini-batches&#34;&gt;why Mini-Batches&lt;/h2&gt;
&lt;p&gt;we can one item for 1 epoch,but this will be very slow,&lt;/p&gt;
&lt;h3 id=&#34;1-single-image-size-batch&#34;&gt;1. single image size batch&lt;/h3&gt;
&lt;p&gt;if we ahve 256 picture,we predict 1 picture,tha we calculate the loss for the picture,than use the loss number to calculate the gradient,step the weight,next picture, &lt;code&gt;total 256 epoch&lt;/code&gt;&lt;/p&gt;
&lt;h3 id=&#34;2-4-image-size-batch&#34;&gt;2. 4 image size batch&lt;/h3&gt;
&lt;p&gt;we have 256/4= 64 bitch picture, we predict 4 picture at a time,we calcuate 4 loss for 4 picture,than use a loss number  to calculate 4 gradient number ,step the weight,next batch,&lt;code&gt;total 64 epoch&lt;/code&gt;&lt;/p&gt;
&lt;p&gt;So use mini btach more fast!!!!!!!!&lt;/p&gt;
&lt;h2 id=&#34;other-reason-why-mini-batches&#34;&gt;Other reason why Mini-Batches&lt;/h2&gt;
&lt;p&gt;another reason that use mini batch not calculating the gradient on individual data items is that, we nearly always do our training on an accelerator such as a GPU. These accelerators only perform well if they have lots of work to do at a time, so it&amp;rsquo;s helpful if we can give them lots of data items to work on. Using mini-batches is one of the best ways to do this. However, if you give them too much data to work on at once, they run out of memory—making GPUs happy is also tricky!&lt;/p&gt;
&lt;h2 id=&#34;use-dataloader-to-create-batches&#34;&gt;Use DataLoader to create batches&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;coll &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;15&lt;/span&gt;)
dl &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; DataLoader(coll, batch_size&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;5&lt;/span&gt;, shuffle&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True)
list(dl)

&lt;span style=&#34;color:#75715e&#34;&gt;# [tensor([ 3, 12,  8, 10,  2]),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  tensor([ 9,  4,  7, 14,  5]),&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;#  tensor([ 1, 13,  0,  6, 11])]&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.7 An End-to-End SGD Example</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.7/</link>
      <pubDate>Wed, 28 Apr 2021 11:57:33 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.7/</guid>
      <description>An End-to-End SGD Example we want to find the smallest value
Some useful function craete a 0-19 torch array
time = torch.arange(0,20).float(); time # tensor([ 0., 1., 2., 3., 4., 5., 6., 7., 8., 9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.]) create randome number
# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。 num=20 t=torch.randn(num) time_f = torch.arange(0,num).float(); time plt.scatter(time_f,t); t simulate a car speed
# simulate a car speed # torch.randn(20)*3 is some random noise time = torch.</description>
      <content>&lt;h1 id=&#34;_an-end-to-end-sgd-example_&#34;&gt;&lt;em&gt;An End-to-End SGD Example&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;we want to find the smallest value&lt;/p&gt;
&lt;h2 id=&#34;some-useful-function&#34;&gt;Some useful function&lt;/h2&gt;
&lt;p&gt;craete a 0-19 torch array&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([ 0.,  1.,  2.,  3.,  4.,  5.,  6.,  7.,  8.,  9., 10., 11., 12., 13., 14., 15., 16., 17., 18., 19.])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;create randome number&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 返回一個張量，包含了從標準正態分佈（均值為0，方差為1，即高斯白噪聲）中抽取的一組隨機數。張量的形狀由參數sizes定義。&lt;/span&gt;
num&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;
t&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(num)
time_f &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,num)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time_f,t);
t
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rt.PNG&#34; alt=&#34;rt&#34;&gt;&lt;/p&gt;
&lt;p&gt;simulate a car speed&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#75715e&#34;&gt;# simulate a car speed&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# torch.randn(20)*3 is some random noise&lt;/span&gt;
time &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;arange(&lt;span style=&#34;color:#ae81ff&#34;&gt;0&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;float(); time
speed &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;20&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;0.75&lt;/span&gt;&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(time&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;9.5&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time,speed);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/car_speed.PNG&#34; alt=&#34;car_speed&#34;&gt;&lt;/p&gt;
&lt;h2 id=&#34;use-sgd-to-find-the-smallest-value-for-the-loss&#34;&gt;use SGD to find the smallest value for the loss&lt;/h2&gt;
&lt;h3 id=&#34;step-0-gues-the-functions&#34;&gt;Step 0 gues the functions&lt;/h3&gt;
&lt;p&gt;we nedd to find the a,b,c that make the loss is the lowset
(time**2)+(b*time)+c&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(t, params):
    a,b,c &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; params
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; a&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;(t&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; (b&lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt;t) &lt;span style=&#34;color:#f92672&#34;&gt;+&lt;/span&gt; c
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-01-define-the-meaning-of-best&#34;&gt;Step 0.1 define the meaning of best&lt;/h3&gt;
&lt;p&gt;we use a loss function to define the best, which will return a value based on a prediction and a target, where lower values of the function correspond to &amp;ldquo;better&amp;rdquo; predictions. For continuous data, it&amp;rsquo;s common to use mean squared error:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;mse&lt;/span&gt;(preds, targets): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; ((preds&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;targets)&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;mean()&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;sqrt()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-1-set-the-apramter-as-a-randome-value&#34;&gt;Step 1 set the apramter as a randome value&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;params&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None
params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; torch&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;randn(&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;)&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;requires_grad_()
orig_params &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;clone()
params
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-2-calculate-the-predict&#34;&gt;Step 2 calculate the predict&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time, params)

&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;show_preds&lt;/span&gt;(preds, ax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;None):
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; ax &lt;span style=&#34;color:#f92672&#34;&gt;is&lt;/span&gt; None: ax&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots()[&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;]
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time, speed)
    &lt;span style=&#34;color:#75715e&#34;&gt;# to_npconvert tensor to numpy arry&lt;/span&gt;
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(time, to_np(preds), color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;)
    ax&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;set_ylim(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;300&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;100&lt;/span&gt;)

show_preds(preds)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/pred1.PNG&#34; alt=&#34;pred1&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-3-calculate-the-losses&#34;&gt;Step 3 calculate the losses&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse(preds, speed)
loss
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor(25.1871, grad_fn=&amp;lt;SqrtBackward&amp;gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-4--know-the-gradients&#34;&gt;Step 4  know the gradients&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
&lt;span style=&#34;color:#75715e&#34;&gt;# the a b c gradients&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# tensor([-3.1634, -0.2709, -0.3931])&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;h3 id=&#34;step-5--step-the-weights&#34;&gt;Step 5  Step the weights&lt;/h3&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;lr &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#ae81ff&#34;&gt;1e-5&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# assign the chnaged parameter to the params&lt;/span&gt;
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; lr &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad
params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;Let&amp;rsquo;s see if the loss has improved:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# Let&amp;#39;s see if the loss has improved:&lt;/span&gt;
preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time,params)
mse(preds, speed)
show_preds(preds)
&lt;span style=&#34;color:#75715e&#34;&gt;# improve a little bit&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/ip.PNG&#34; alt=&#34;pred1&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step-6--repeat-it&#34;&gt;step 6 , repeat it&lt;/h3&gt;
&lt;h1 id=&#34;we-use-a-for-loop-to-do-multi-time&#34;&gt;we use a for loop to do multi time&lt;/h1&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;apply_step&lt;/span&gt;(params, prn&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;True):
    preds &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; f(time, params)
    loss &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; mse(preds, speed)
    loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;backward()
    params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data &lt;span style=&#34;color:#f92672&#34;&gt;-=&lt;/span&gt; lr &lt;span style=&#34;color:#f92672&#34;&gt;*&lt;/span&gt; params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;data
    params&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;grad &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; None
    &lt;span style=&#34;color:#66d9ef&#34;&gt;if&lt;/span&gt; prn: &lt;span style=&#34;color:#66d9ef&#34;&gt;print&lt;/span&gt;(loss&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;item())
    &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; preds
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; i &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; range(&lt;span style=&#34;color:#ae81ff&#34;&gt;10&lt;/span&gt;): apply_step(params)

&lt;span style=&#34;color:#75715e&#34;&gt;# 160.42279052734375&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 160.14772033691406&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.87269592285156&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.59768676757812&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.3227081298828&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 159.04774475097656&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.7728271484375&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.4979248046875&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 158.22305297851562&lt;/span&gt;
&lt;span style=&#34;color:#75715e&#34;&gt;# 157.9481964111328&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;_,axs &lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt; plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;subplots(&lt;span style=&#34;color:#ae81ff&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;4&lt;/span&gt;,figsize&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#ae81ff&#34;&gt;12&lt;/span&gt;,&lt;span style=&#34;color:#ae81ff&#34;&gt;3&lt;/span&gt;))
&lt;span style=&#34;color:#66d9ef&#34;&gt;for&lt;/span&gt; ax &lt;span style=&#34;color:#f92672&#34;&gt;in&lt;/span&gt; axs: show_preds(apply_step(params, False), ax)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;tight_layout()
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/4p.PNG&#34; alt=&#34;4p&#34;&gt;&lt;/p&gt;
&lt;h3 id=&#34;step7-stop&#34;&gt;Step7 stop&lt;/h3&gt;
&lt;p&gt;we do 10 round ,than stop**&lt;/p&gt;
</content>
    </item>
    
    <item>
      <title>Ai Tutorial 4.4 Stochastic Gradient Descent 隨機梯度下降 (SGD)</title>
      <link>https://roy989898.github.io/posts/ai-tutorial-4.4/</link>
      <pubDate>Tue, 27 Apr 2021 19:56:41 +0800</pubDate>
      
      <guid>https://roy989898.github.io/posts/ai-tutorial-4.4/</guid>
      <description>SGD Instead of trying to find the similarity between an image and an &amp;ldquo;ideal image,&amp;rdquo; we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8.</description>
      <content>&lt;h1 id=&#34;_sgd_&#34;&gt;&lt;em&gt;SGD&lt;/em&gt;&lt;/h1&gt;
&lt;p&gt;Instead of trying to find the similarity between an image and an &amp;ldquo;ideal image,&amp;rdquo; we could instead look at each individual pixel and come up with a set of weights for each one, such that the highest weights are associated with those pixels most likely to be black for a particular category. For instance, pixels toward the bottom right are not very likely to be activated for a 7, so they should have a low weight for a 7, but they are likely to be activated for an 8, so they should have a high weight for an 8. This can be represented as a function and set of weight values for each possible category—for instance the probability of being the number 8:&lt;/p&gt;
&lt;p&gt;與其嘗試查找圖像與“理想圖像”之間的相似性，不如查看每個單獨的像素並為每個像素提出一組權重，以使最高的權重與最有可能與之相關的那些像素相關聯。 對於特定類別為黑色。 例如，朝右下角移動的像素不太可能為7激活，因此對於7,像素應該具有較低的權重，但對於8,像素應該很容易被激活，因此對於8,像素應該具有較高的權重.這可以表示為每個可能類別的一個函數和一組權重值，例如，成為數字8的概率：&lt;br&gt;
&lt;code&gt;def pr_eight(x,w): return (x*w).sum()&lt;/code&gt;&lt;br&gt;
x is the image, represented as a vector—in other words, with all of the rows stacked up end to end into a single long line. And we are assuming that the weights are a vector w. If we have this function, then we just need some way to update the weights to make them a little bit better. With such an approach, we can repeat that step a number of times, making the weights better and better, until they are as good as we can make them.&lt;/p&gt;
&lt;p&gt;x是表示為矢量的圖像，換句話說，所有行首尾相連地排成一條長線。 並且我們假設權重是向量w。 如果我們具有此功能，那麼我們只需要一些方法來更新權重即可使它們更好一點。 通過這種方法，我們可以重複該步驟多次，使權重越來越好，直到權重達到我們所能達到的程度為止。&lt;/p&gt;
&lt;p&gt;want to find the specific values for the vector w that causes the result of our function to be high for those images that are actually 8s, and low for those images that are not. Searching for the best vector w is a way to search for the best function for recognising 8s.&lt;/p&gt;
&lt;p&gt;想要找到向量w的特定值，該值導致函數的結果對於那些實際上是8s的圖像來說較高，而對於那些不是8s的圖像來說較低。 搜索最佳向量w是搜索識別8s的最佳函數的一種方式。&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;Initialize the weights.初始化權重。&lt;/li&gt;
&lt;li&gt;For each image, use these weights to predict whether it appears to be a 3 or a 7.對於每個圖像，使用這些權重來預測它是3還是7。&lt;/li&gt;
&lt;li&gt;Based on these predictions, calculate how good the model is (its loss).根據這些預測，計算模型的好壞（損失）。&lt;/li&gt;
&lt;li&gt;Calculate the gradient, which measures for each weight, how changing that weight would change the loss.計算坡度，該坡度針對每個權重進行度量，更改該權重將如何改變損耗&lt;/li&gt;
&lt;li&gt;Step (that is, change) all the weights based on that calculation.根據該計算步進（即更改）所有權重。&lt;/li&gt;
&lt;li&gt;Go back to the step 2, and repeat the process.返回到步驟2，並重複該過程。&lt;/li&gt;
&lt;li&gt;Iterate until you decide to stop the training process (for instance, because the model is good enough or you don&amp;rsquo;t want to wait any longer).重複進行直到您決定停止訓練過程為止（例如，因為模型足夠好或者您不想再等待了）。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/sgd_step.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;There are many different ways to do each of these seven steps&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Initialize:: initialize the parameters to random values. This may sound surprising. There are certainly other choices we could make, such as initializing them to the percentage of times that pixel is activated for that category—but since we already know that we have a routine to improve these weights, it turns out that just starting with random weights works perfectly well.
將參數初始化為隨機值。 這聽起來可能令人驚訝。 當然，我們還可以做出其他選擇，例如將其初始化為該類別的像素被激活的次數的百分比-但由於我們已經知道我們有一個例程可以改善這些權重，因此事實證明，只是從隨機權重開始 效果很好。&lt;/li&gt;
&lt;li&gt;Loss:: when testing the effectiveness of any current weight assignment in terms of actual performance. We need some function that will return a number that is small if the performance of the model is good (the standard approach is to treat a small loss as good, and a large loss as bad, although this is just a convention).
在實際性能方面測試任何當前重量分配的有效性時。 如果模型的性能良好，我們需要一些函數返回一個較小的數字（標準方法是將小的損失視為好，將大損失視為壞，儘管這只是一個慣例）。&lt;/li&gt;
&lt;li&gt;Step:: A simple way to figure out whether a weight should be increased a bit, or decreased a bit, would be just to try it: increase the weight by a small amount, and see if the loss goes up or down. Once you find the correct direction, you could then change that amount by a bit more, and a bit less, until you find an amount that works well. However, this is slow! As we will see, the magic of calculus allows us to directly figure out in which direction, and by roughly how much, to change each weight, without having to try all these small changes. The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.
一種簡單的判斷重量是否應該增加還是減少的簡單方法就是嘗試：將重量增加一點，然後看看損失是增加還是減少。 找到正確的方向後，您可以再多一點，少一點地更改該金額，直到找到一個行之有效的金額。 但是，這很慢！ 就像我們將看到的那樣，微積分的神奇之處使我們能夠直接弄清楚改變每個權重的方向和大致幅度，而不必嘗試所有這些小的改變。 做到這一點的方法是通過計算梯度。 這只是性能優化，通過使用較慢的手動過程，我們也將獲得完全相同的結果。&lt;/li&gt;
&lt;li&gt;Stop:: Once we&amp;rsquo;ve decided how many epochs to train the model for (a few suggestions for this were given in the earlier list), we apply that decision. This is where that decision is applied. For our digit classifier, we would keep training until the accuracy of the model started getting worse, or we ran out of time.
一旦我們確定了訓練模型的時間（在前面的列表中給出了一些建議），我們就會應用該決定。 這就是應用該決定的地方。 對於我們的數字分類器，我們將繼續訓練直到模型的準確性開始變差或用完為止。&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;simple-example-of-sgd&#34;&gt;simple example of SGD&lt;/h2&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;&lt;span style=&#34;color:#66d9ef&#34;&gt;def&lt;/span&gt; &lt;span style=&#34;color:#a6e22e&#34;&gt;f&lt;/span&gt;(x): &lt;span style=&#34;color:#66d9ef&#34;&gt;return&lt;/span&gt; x&lt;span style=&#34;color:#f92672&#34;&gt;**&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;2&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plot_function(f, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x**2&amp;#39;&lt;/span&gt;)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/x2p.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;The sequence of steps we described earlier starts by picking some random value for a parameter, and calculating the value of the loss:&lt;/p&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre style=&#34;color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-py&#34; data-lang=&#34;py&#34;&gt;plot_function(f, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x&amp;#39;&lt;/span&gt;, &lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;x**2&amp;#39;&lt;/span&gt;)
plt&lt;span style=&#34;color:#f92672&#34;&gt;.&lt;/span&gt;scatter(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;, f(&lt;span style=&#34;color:#f92672&#34;&gt;-&lt;/span&gt;&lt;span style=&#34;color:#ae81ff&#34;&gt;1.5&lt;/span&gt;), color&lt;span style=&#34;color:#f92672&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#e6db74&#34;&gt;&amp;#39;red&amp;#39;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;p&gt;&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/2pr.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;if we increased or decreased our parameter by a little bit—the adjustment. This is simply the slope at a particular point:
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rs1.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;We can change our weight by a little in the direction of the slope, calculate our loss and adjustment again, and repeat this a few times. Eventually, we will get to the lowest point on our curve:
&lt;img src=&#34;https://roy989898.github.io/img/ai_t/t1/rs2.PNG&#34; alt=&#34;sgd_step&#34;&gt;&lt;/p&gt;
&lt;p&gt;we want to find the lower y/(Loss),lowest is good,we we replay to try different x, to find the lowest y. this method is slow,a better ,is The way to do this is by calculating gradients. This is just a performance optimization, we would get exactly the same results by using the slower manual process as well.&lt;/p&gt;
</content>
    </item>
    
  </channel>
</rss>
