<!DOCTYPE html>
<html lang="en">
<head>
  
    <title>Ai Tutorial 6.4  Other Computer Vision Problems-Multi-Label Binary Cross-Entropy Image and Point :: Terminal</title>
  
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="description" content="Image and Point key point model A key point refers to a specific location represented in an image—in this case, we&amp;rsquo;ll use images of people and we&amp;rsquo;ll be looking for the center of the person&amp;rsquo;s face in each image. That means we&amp;rsquo;ll actually be predicting two values for each image: the row and column of the face center.
Assemble the Data path = untar_data(URLs.BIWI_HEAD_POSE) #hide Path.BASE_PATH = path 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding ." />
<meta name="keywords" content=", " />
<meta name="robots" content="noodp" />
<link rel="canonical" href="https://roy989898.github.io/posts/ai-tutorial-6.4/" />




<link rel="stylesheet" href="https://roy989898.github.io/assets/style.css">

  <link rel="stylesheet" href="https://roy989898.github.io/assets/pink.css">






<link rel="apple-touch-icon" href="https://roy989898.github.io/img/apple-touch-icon-192x192.png">

  <link rel="shortcut icon" href="https://roy989898.github.io/img/favicon/pink.png">



<meta name="twitter:card" content="summary" />

  
    <meta name="twitter:site" content="" />
  
    <meta name="twitter:creator" content="" />



<meta property="og:locale" content="en" />
<meta property="og:type" content="article" />
<meta property="og:title" content="Ai Tutorial 6.4  Other Computer Vision Problems-Multi-Label Binary Cross-Entropy Image and Point">
<meta property="og:description" content="Image and Point key point model A key point refers to a specific location represented in an image—in this case, we&amp;rsquo;ll use images of people and we&amp;rsquo;ll be looking for the center of the person&amp;rsquo;s face in each image. That means we&amp;rsquo;ll actually be predicting two values for each image: the row and column of the face center.
Assemble the Data path = untar_data(URLs.BIWI_HEAD_POSE) #hide Path.BASE_PATH = path 24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding ." />
<meta property="og:url" content="https://roy989898.github.io/posts/ai-tutorial-6.4/" />
<meta property="og:site_name" content="Terminal" />

  <meta property="og:image" content="https://roy989898.github.io/">

<meta property="og:image:width" content="2048">
<meta property="og:image:height" content="1024">


  <meta property="article:published_time" content="2021-05-16 18:41:39 &#43;0800 CST" />












</head>
<body class="pink">


<div class="container headings--one-size">

  <header class="header">
  <div class="header__inner">
    <div class="header__logo">
      <a href="/">
  <div class="logo">
    Pom&#39;s Programmer Blog
  </div>
</a>

    </div>
    
      <div class="menu-trigger">menu</div>
    
  </div>
  
    <nav class="menu">
  <ul class="menu__inner menu__inner--desktop">
    
      
        
          <li><a href="/about">About</a></li>
        
      
        
          <li><a href="/showcase">Showcase</a></li>
        
      
      
    

    
  </ul>

  <ul class="menu__inner menu__inner--mobile">
    
      
        <li><a href="/about">About</a></li>
      
    
      
        <li><a href="/showcase">Showcase</a></li>
      
    
    
  </ul>
</nav>

  
</header>


  <div class="content">
    
<div class="post">
  <h1 class="post-title">
    <a href="https://roy989898.github.io/posts/ai-tutorial-6.4/">Ai Tutorial 6.4  Other Computer Vision Problems-Multi-Label Binary Cross-Entropy Image and Point</a></h1>
  <div class="post-meta">
    
      <span class="post-date">
        2021-05-16 
      </span>
    
    
  </div>

  
  <span class="post-tags">
    
    #<a href="https://roy989898.github.io/tags/ai/">ai</a>&nbsp;
    
    #<a href="https://roy989898.github.io/tags/fastai/">fastai</a>&nbsp;
    
    #<a href="https://roy989898.github.io/tags/pytorch/">pytorch</a>&nbsp;
    
    #<a href="https://roy989898.github.io/tags/%E5%AF%AB%E7%B5%A6%E7%A8%8B%E5%BC%8F%E8%A8%AD%E8%A8%88%E5%B8%AB%E7%9A%84%E6%B7%B1%E5%BA%A6%E5%AD%B8%E7%BF%92%E4%BD%BF%E7%94%A8fastai%E5%92%8Cpytorch/">寫給程式設計師的深度學習：使用fastai和PyTorch</a>&nbsp;
    
    #<a href="https://roy989898.github.io/tags/multi-label-classification/">Multi-Label Classification</a>&nbsp;
    
    #<a href="https://roy989898.github.io/tags/cross-entropy/">Cross-Entropy</a>&nbsp;
    
  </span>
  

  

  

  <div class="post-content"><div>
        <h1 id="image-and-point">Image and Point<a href="#image-and-point" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<h2 id="key-point-model">key point model<a href="#key-point-model" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>A key point refers to a specific location represented in an image—in this case, we&rsquo;ll use images of people and we&rsquo;ll be looking for the center of the person&rsquo;s face in each image. That means we&rsquo;ll actually be predicting two values for each image: the row and column of the face center.</p>
<h2 id="assemble-the-data">Assemble the Data<a href="#assemble-the-data" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">path <span style="color:#f92672">=</span> untar_data(URLs<span style="color:#f92672">.</span>BIWI_HEAD_POSE)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e">#hide</span>
Path<span style="color:#f92672">.</span>BASE_PATH <span style="color:#f92672">=</span> path
</code></pre></div><p>24 directories numbered from 01 to 24 (they correspond to the different people photographed), and a corresponding .obj file for each (we won&rsquo;t need them here)</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">path<span style="color:#f92672">.</span>ls()<span style="color:#f92672">.</span>sorted()
<span style="color:#75715e"># (#50) [Path(&#39;01&#39;),Path(&#39;01.obj&#39;),Path(&#39;02&#39;),Path(&#39;02.obj&#39;),Path(&#39;03&#39;),Path(&#39;03.obj&#39;),Path(&#39;04&#39;),Path(&#39;04.obj&#39;),Path(&#39;05&#39;),Path(&#39;05.obj&#39;)...]</span>

</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">(path<span style="color:#f92672">/</span><span style="color:#e6db74">&#39;01&#39;</span>)<span style="color:#f92672">.</span>ls()<span style="color:#f92672">.</span>sorted()
<span style="color:#75715e"># (#1000) [Path(&#39;01/depth.cal&#39;),Path(&#39;01/frame_00003_pose.txt&#39;),Path(&#39;01/frame_00003_rgb.jpg&#39;),Path(&#39;01/frame_00004_pose.txt&#39;),Path(&#39;01/frame_00004_rgb.jpg&#39;),Path(&#39;01/frame_00005_pose.txt&#39;),Path(&#39;01/frame_00005_rgb.jpg&#39;),Path(&#39;01/frame_00006_pose.txt&#39;),Path(&#39;01/frame_00006_rgb.jpg&#39;),Path(&#39;01/frame_00007_pose.txt&#39;)...]</span>

</code></pre></div><p>get the image file</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">img_files <span style="color:#f92672">=</span> get_image_files(path)
img_files
<span style="color:#75715e"># (#15678) [Path(&#39;03/frame_00650_rgb.jpg&#39;),Path(&#39;03/frame_00644_rgb.jpg&#39;),Path(&#39;03/frame_00491_rgb.jpg&#39;),Path(&#39;03/frame_00207_rgb.jpg&#39;),Path(&#39;03/frame_00067_rgb.jpg&#39;),Path(&#39;03/frame_00056_rgb.jpg&#39;),Path(&#39;03/frame_00025_rgb.jpg&#39;),Path(&#39;03/frame_00450_rgb.jpg&#39;),Path(&#39;03/frame_00584_rgb.jpg&#39;),Path(&#39;03/frame_00285_rgb.jpg&#39;)...]</span>

</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">img_files[<span style="color:#ae81ff">0</span>]
<span style="color:#75715e"># Path(&#39;03/frame_00650_rgb.jpg&#39;)</span>

<span style="color:#e6db74">``</span><span style="color:#960050;background-color:#1e0010">`</span> a function that use the image name to get the pose<span style="color:#f92672">.</span>txt

<span style="color:#e6db74">``</span><span style="color:#960050;background-color:#1e0010">`</span>py
<span style="color:#75715e"># a function that use the image name to get the pose.txt</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">img2pose</span>(x): <span style="color:#66d9ef">return</span> Path(f<span style="color:#e6db74">&#39;{str(x)[:-7]}pose.txt&#39;</span>)
img2pose(img_files[<span style="color:#ae81ff">0</span>])
<span style="color:#75715e"># Path(&#39;03/frame_00650_pose.txt&#39;)</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># see the first image</span>
im <span style="color:#f92672">=</span> PILImage<span style="color:#f92672">.</span>create(img_files[<span style="color:#ae81ff">0</span>])
im<span style="color:#f92672">.</span>shape
<span style="color:#75715e"># (480, 640)</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">im<span style="color:#f92672">.</span>to_thumb(<span style="color:#ae81ff">160</span>)
</code></pre></div><p><img src="/img/ai_t/t1/py.PNG" alt="py"></p>
<h2 id="get-the-head-point">get the head point<a href="#get-the-head-point" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">cal <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>genfromtxt(path<span style="color:#f92672">/</span><span style="color:#e6db74">&#39;01&#39;</span><span style="color:#f92672">/</span><span style="color:#e6db74">&#39;rgb.cal&#39;</span>, skip_footer<span style="color:#f92672">=</span><span style="color:#ae81ff">6</span>)
cal

<span style="color:#75715e"># array([[517.679,   0.   , 320.   ],</span>
<span style="color:#75715e">#        [  0.   , 517.679, 240.5  ],</span>
<span style="color:#75715e">#        [  0.   ,   0.   ,   1.   ]])</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># the function to get the head point</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">get_ctr</span>(f):
    ctr <span style="color:#f92672">=</span> np<span style="color:#f92672">.</span>genfromtxt(img2pose(f), skip_header<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>)
    c1 <span style="color:#f92672">=</span> ctr[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> cal[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">0</span>]<span style="color:#f92672">/</span>ctr[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">+</span> cal[<span style="color:#ae81ff">0</span>][<span style="color:#ae81ff">2</span>]
    c2 <span style="color:#f92672">=</span> ctr[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> cal[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>]<span style="color:#f92672">/</span>ctr[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">+</span> cal[<span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">2</span>]
    <span style="color:#66d9ef">return</span> tensor([c1,c2])
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">get_ctr(img_files[<span style="color:#ae81ff">0</span>])
tensor([<span style="color:#ae81ff">447.6672</span>, <span style="color:#ae81ff">277.1215</span>])
</code></pre></div><p>we have 2 problem at here</p>
<p>One important point to note is that we should not just use a random splitter. The reason for this is that the same people appear in multiple images in this dataset, but we want to ensure that our model can generalize to people that it hasn&rsquo;t seen yet. Each folder in the dataset contains the images for one person. Therefore, we can create a splitter function that returns true for just one person, resulting in a validation set containing just that person&rsquo;s images.</p>
<p>The only other difference from the previous data block examples is that the second block is a PointBlock. This is necessary so that fastai knows that the labels represent coordinates; that way, it knows that when doing data augmentation, it should do the same augmentation to these coordinates as it does to the images:</p>
<p>splitter=FuncSplitter(lambda o: o.parent.name==&lsquo;13&rsquo;), mean we want ot create validation set containing just that person&rsquo;s images.that contain in the document 13</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">biwi <span style="color:#f92672">=</span> DataBlock(
    blocks<span style="color:#f92672">=</span>(ImageBlock, PointBlock),
    get_items<span style="color:#f92672">=</span>get_image_files,
    get_y<span style="color:#f92672">=</span>get_ctr,
    splitter<span style="color:#f92672">=</span>FuncSplitter(<span style="color:#66d9ef">lambda</span> o: o<span style="color:#f92672">.</span>parent<span style="color:#f92672">.</span>name<span style="color:#f92672">==</span><span style="color:#e6db74">&#39;13&#39;</span>),
    batch_tfms<span style="color:#f92672">=</span>[<span style="color:#f92672">*</span>aug_transforms(size<span style="color:#f92672">=</span>(<span style="color:#ae81ff">240</span>,<span style="color:#ae81ff">320</span>)), 
                Normalize<span style="color:#f92672">.</span>from_stats(<span style="color:#f92672">*</span>imagenet_stats)]
)
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># check is the data ok</span>
dls <span style="color:#f92672">=</span> biwi<span style="color:#f92672">.</span>dataloaders(path)
dls<span style="color:#f92672">.</span>show_batch(max_n<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>, figsize<span style="color:#f92672">=</span>(<span style="color:#ae81ff">8</span>,<span style="color:#ae81ff">6</span>))
</code></pre></div><p><img src="/img/ai_t/t1/train_9.PNG" alt="train_9"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># check the size</span>
xb,yb <span style="color:#f92672">=</span> dls<span style="color:#f92672">.</span>one_batch()
xb<span style="color:#f92672">.</span>shape,yb<span style="color:#f92672">.</span>shape
<span style="color:#75715e"># (torch.Size([64, 3, 240, 320]), torch.Size([64, 1, 2]))</span>
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">the location of the point
yb[<span style="color:#ae81ff">0</span>]
<span style="color:#75715e"># TensorPoint([[-0.2325,  0.1644]], device=&#39;cuda:0&#39;)</span>
</code></pre></div><h2 id="train-a-model">train a Model<a href="#train-a-model" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>we used y_range to tell fastai the range of our targets? We&rsquo;ll do the same here (coordinates in fastai and PyTorch are always rescaled between -1 and +1):</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># why a y_range???</span>

learn <span style="color:#f92672">=</span> cnn_learner(dls, resnet18, y_range<span style="color:#f92672">=</span>(<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>,<span style="color:#ae81ff">1</span>))
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># the y_range function</span>
<span style="color:#66d9ef">def</span> <span style="color:#a6e22e">sigmoid_range</span>(x, lo, hi): <span style="color:#66d9ef">return</span> torch<span style="color:#f92672">.</span>sigmoid(x) <span style="color:#f92672">*</span> (hi<span style="color:#f92672">-</span>lo) <span style="color:#f92672">+</span> lo
</code></pre></div><p>the loss function in the learner</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py"><span style="color:#75715e"># default use MSELoss</span>
dls<span style="color:#f92672">.</span>loss_func
<span style="color:#75715e"># FlattenedLoss of MSELoss()</span>
</code></pre></div><p>pytorch 的nn.MSELoss()損失函數 <a href="https://blog.csdn.net/weixin_38145317/article/details/103735784">https://blog.csdn.net/weixin_38145317/article/details/103735784</a></p>
<p>This makes sense, since when coordinates are used as the dependent variable, most of the time we&rsquo;re likely to be trying to predict something as close as possible; that&rsquo;s basically what MSELoss (mean squared error loss) does. If you want to use a different loss function, you can pass it to cnn_learner using the loss_func parameter.</p>
<h2 id="metrics">metrics<a href="#metrics" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h2>
<p>Note also that we didn&rsquo;t specify any metrics. That&rsquo;s because the MSE is already a useful metric for this task (although it&rsquo;s probably more interpretable after we take the square root).</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">min,steep<span style="color:#f92672">=</span>learn<span style="color:#f92672">.</span>lr_find()
</code></pre></div><p><img src="/img/ai_t/t1/lrf6.PNG" alt="lrf6"></p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">min,steep
</code></pre></div><div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">lr <span style="color:#f92672">=</span> <span style="color:#ae81ff">1e-2</span>  <span style="color:#75715e"># book is 2e-2</span>
learn<span style="color:#f92672">.</span>fine_tune(<span style="color:#ae81ff">3</span>, lr)
<span style="color:#75715e"># epoch train_loss valid_loss time</span>
<span style="color:#75715e"># 0 0.048711 0.018842 01:48</span>
<span style="color:#75715e"># epoch train_loss valid_loss time</span>
<span style="color:#75715e"># 0 0.008920 0.010456 01:51</span>
<span style="color:#75715e"># 1 0.002904 0.000215 01:51</span>
<span style="color:#75715e"># 2 0.001400 0.000146 01:51</span>
</code></pre></div><p>Generally when we run this we get a loss of around 0.0001, which corresponds to an average coordinate prediction error of:</p>
<div class="highlight"><pre style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-py" data-lang="py">math<span style="color:#f92672">.</span>sqrt(<span style="color:#ae81ff">0.0001</span>)
</code></pre></div><p>learn.show_results(ds_idx=1, nrows=3, figsize=(6,8))</p>
<p><img src="/img/ai_t/t1/pre_r6.PNG" alt="pre_r6"></p>
<h1 id="conclusion">Conclusion<a href="#conclusion" class="hanchor" ariaLabel="Anchor">&#8983;</a> </h1>
<p>fastai will automatically try to pick the right one from the data you built, but if you are using pure PyTorch to build your DataLoaders, make sure you think hard when you have to decide on your choice of loss function, and remember that you most probably want:</p>
<p>nn.CrossEntropyLoss for single-label classification nn.BCEWithLogitsLoss for multi-label classification nn.MSELoss for regression</p>

      </div></div>

  
  
<div class="pagination">
    <div class="pagination__title">
        <span class="pagination__title-h">Read other posts</span>
        <hr />
    </div>
    <div class="pagination__buttons">
        
        
        <span class="button next">
            <a href="https://roy989898.github.io/posts/ai-tutorial-6.3/">
                <span class="button__text">Ai Tutorial 6.3 Other Computer Vision Problems-Multi-Label Binary Cross-Entropy</span>
                <span class="button__icon">→</span>
            </a>
        </span>
        
    </div>
</div>

  

  

</div>

  </div>

  
    <footer class="footer">
  <div class="footer__inner">
    
      <div class="copyright">
        <span>© 2021 Powered by <a href="http://gohugo.io">Hugo</a></span>
    
        <span>:: Theme made by <a href="https://twitter.com/panr">panr</a></span>
      </div>
  </div>
</footer>

<script src="https://roy989898.github.io/assets/main.js"></script>
<script src="https://roy989898.github.io/assets/prism.js"></script>







  
</div>

</body>
</html>
